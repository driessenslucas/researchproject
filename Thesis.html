<!DOCTYPE html>
<html>
<head>
<title>Thesis.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="exploring-the-feasibility-of-sim2real-transfer-in-reinforcement-learning">Exploring the Feasibility of Sim2Real Transfer in Reinforcement Learning</h1>
<h2 id="author-information">Author Information</h2>
<p><strong>Name:</strong> Lucas Driessens<br>
<strong>Institution:</strong> Howest University of Applied Sciences<br>
<strong>Course:</strong> Thesis<br>
<strong>Date:</strong> 2024-19-02<br>
<strong>Github Repository:</strong> <a href="https://github.com/driessenslucas/researchproject">https://github.com/driessenslucas/researchproject</a></p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#exploring-the-feasibility-of-sim2real-transfer-in-reinforcement-learning">Exploring the Feasibility of Sim2Real Transfer in Reinforcement Learning</a>
<ul>
<li><a href="#author-information">Author Information</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#description">Description</a></li>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#glossary-of-terms">Glossary of Terms</a></li>
<li><a href="#list-of-abbreviations">List of Abbreviations</a></li>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#background-on-reinforcement-learning-rl">Background on Reinforcement Learning (RL)</a></li>
<li><a href="#real-world-applications-of-rl">Real-World Applications of RL</a></li>
<li><a href="#purpose-and-significance-of-the-study">Purpose and Significance of the Study</a></li>
<li><a href="#overview-of-the-research-questions">Overview of the Research Questions</a></li>
</ul>
</li>
<li><a href="#main-research-question">Main Research Question</a>
<ul>
<li><a href="#sub-research-questions">Sub Research Questions</a></li>
</ul>
</li>
<li><a href="#methodology">Methodology</a>
<ul>
<li><a href="#virtual-environment-design">Virtual Environment Design</a>
<ul>
<li><a href="#rcmazeenv-environment">RCMazeEnv Environment</a></li>
<li><a href="#agent-design-double-deep-q-network-double-dqn">Agent Design: Double Deep Q-Network (Double DQN)</a></li>
</ul>
</li>
<li><a href="#experimental-setup">Experimental Setup</a>
<ul>
<li><a href="#training-process-of-the-double-dqn-agent">Training Process of the Double DQN Agent</a>
<ul>
<li><a href="#model-architecture">Model Architecture</a></li>
</ul>
</li>
<li><a href="#training-parameters">Training Parameters</a>
<ul>
<li><a href="#training-procedure">Training Procedure</a></li>
</ul>
</li>
<li><a href="#reinforcement-learning-techniques-overview">Reinforcement Learning Techniques Overview</a>
<ul>
<li><a href="#1-deep-q-network-dqn">1. Deep Q-Network (DQN)</a></li>
<li><a href="#2-double-deep-q-network-ddqn">2. Double Deep Q-Network (DDQN)</a></li>
<li><a href="#3-proximal-policy-optimization-ppo">3. Proximal Policy Optimization (PPO)</a></li>
</ul>
</li>
<li><a href="#hardware-setup-and-assembly">Hardware Setup and Assembly</a>
<ul>
<li><a href="#introduction-to-hardware-components">Introduction to Hardware Components</a></li>
<li><a href="#components-list">Components List</a></li>
<li><a href="#wiring-guide">Wiring Guide</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#challenges-and-solutions-in-implementing-rl-techniques-and-virtual-environments">Challenges and Solutions in Implementing RL Techniques and Virtual Environments</a>
<ul>
<li><a href="#challenge-1-selection-of-an-appropriate-virtual-environment">Challenge 1: Selection of an Appropriate Virtual Environment</a></li>
<li><a href="#challenge-2-choosing-the-optimal-reinforcement-learning-technique">Challenge 2: Choosing the Optimal Reinforcement Learning Technique</a></li>
<li><a href="#challenge-3-sim2real-transfer---addressing-movement-discrepancies">Challenge 3: Sim2Real Transfer - Addressing Movement Discrepancies</a></li>
<li><a href="#challenge-4-alignment-issue-and-motor-encoder-implementation">Challenge 4: alignment Issue and Motor Encoder Implementation</a></li>
<li><a href="#challenge-5-ensuring-consistent-and-effective-training">Challenge 5: Ensuring Consistent and Effective Training</a></li>
<li><a href="#challenge-6-accurate-sensor-data-normalization-for-sim2real-transfer">Challenge 6: Accurate Sensor Data Normalization for Sim2Real Transfer</a></li>
<li><a href="#challenge-7-integration-of-failsafe-mechanisms">Challenge 7: Integration of Failsafe Mechanisms</a></li>
<li><a href="#challenge-8-training-environment-and-technique-efficacy">Challenge 8: Training Environment and Technique Efficacy</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#supplementary-materials-video-demonstrations">Supplementary Materials: Video Demonstrations</a>
<ul>
<li><a href="#introduction-1">Introduction</a></li>
<li><a href="#video-1-mpu6050-90-degree-turn">Video 1: mpu6050 90 degree turn</a></li>
<li><a href="#video-2-mpu6050-to-align-forward-movement">Video 2: mpu6050 to align forward movement</a></li>
<li><a href="#video-4-new-rc-car-with-encoder-and-more-powerful-motor">video 4: New RC-car with encoder and more powerful motor</a></li>
<li><a href="#video-5-encoder-implementation-original-rc-car">video 5: Encoder implementation (original RC-car)</a></li>
<li><a href="#video-6-robot-v2">video 6: Robot v2</a></li>
<li><a href="#video-7-maze-test-outdoors">video 7: Maze Test Outdoors</a></li>
<li><a href="#video-8-maze-test-indoors">video 8: Maze Test Indoors</a></li>
</ul>
</li>
<li><a href="#real-world-application-and-limitations">Real-World Application and Limitations</a>
<ul>
<li><a href="#introduction-to-sensor-and-movement-discrepancies">Introduction to Sensor and Movement Discrepancies</a></li>
<li><a href="#real-world-application">Real-World Application</a>
<ul>
<li><a href="#sensor-based-navigation">Sensor-Based Navigation</a></li>
<li><a href="#impact-on-autonomous-vehicle-movement">Impact on Autonomous Vehicle Movement</a></li>
</ul>
</li>
<li><a href="#limitations">Limitations</a>
<ul>
<li><a href="#sensor-data-discrepancies">Sensor Data Discrepancies</a></li>
<li><a href="#movement-replication-challenges">Movement Replication Challenges</a></li>
<li><a href="#practical-considerations">Practical Considerations</a></li>
</ul>
</li>
<li><a href="#conclusion-1">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#answers-to-research-questions">Answers to Research Questions</a>
<ul>
<li><a href="#1-which-virtual-environments-exist-to-train-a-virtual-rf-car">1. Which virtual environments exist to train a virtual RF-car?</a>
<ul>
<li><a href="#introduction-2">Introduction</a></li>
</ul>
</li>
<li><a href="#executive-summary">Executive Summary</a>
<ul>
<li><a href="#virtual-environments-overview">Virtual Environments Overview</a></li>
<li><a href="#conclusion-2">Conclusion</a></li>
</ul>
</li>
<li><a href="#2-which-reinforcement-learning-techniques-can-i-best-use-in-this-application">2. Which reinforcement learning techniques can I best use in this application?</a>
<ul>
<li><a href="#introduction-3">Introduction</a></li>
</ul>
</li>
<li><a href="#executive-summary-1">Executive Summary</a>
<ul>
<li><a href="#reinforcement-learning-techniques-overview-1">Reinforcement Learning Techniques Overview</a></li>
<li><a href="#conclusion-3">Conclusion</a></li>
</ul>
</li>
<li><a href="#3-can-the-simulation-be-transferred-to-the-real-world-explore-the-difference-between-how-the-car-moves-in-the-simulation-and-in-the-real-world">3. Can the simulation be transferred to the real world? Explore the difference between how the car moves in the simulation and in the real world</a>
<ul>
<li><a href="#introduction-4">Introduction</a></li>
<li><a href="#executive-summary-2">Executive Summary</a></li>
<li><a href="#discrepancies-and-adjustments">Discrepancies and Adjustments</a></li>
<li><a href="#conclusion-4">Conclusion</a></li>
</ul>
</li>
<li><a href="#4-does-the-simulation-have-any-useful-contributions-in-terms-of-training-time-or-performance">4. Does the simulation have any useful contributions? In terms of training time or performance?</a>
<ul>
<li><a href="#introduction-5">Introduction</a></li>
<li><a href="#executive-summary-3">Executive Summary</a></li>
<li><a href="#advantages-of-simulation-training">Advantages of Simulation Training</a></li>
<li><a href="#conclusion-5">Conclusion</a></li>
</ul>
</li>
<li><a href="#5-how-can-i-transfer-my-trained-model-to-my-physical-rc-car-sim2real-how-do-you-need-to-adjust-the-agent-environment-and-model-for-it-to-work-in-the-real-world">5. How can I transfer my trained model to my physical RC car? (sim2real) How do you need to adjust the agent, environment and model for it to work in the real world?</a></li>
</ul>
</li>
<li><a href="#reflection">Reflection</a></li>
<li><a href="#advice-for-those-embarking-on-similar-research-paths">Advice for those Embarking on Similar Research Paths</a></li>
<li><a href="#general-conclusion">General Conclusion</a></li>
<li><a href="#credits">Credits</a></li>
<li><a href="#sources-and-inspiration">Sources and Inspiration</a>
<ul>
<li><a href="#websites">Websites</a></li>
<li><a href="#github-repositories">GitHub Repositories</a></li>
<li><a href="#stack-overflow-threads">Stack Overflow Threads</a></li>
<li><a href="#academic-articles">Academic Articles</a></li>
<li><a href="#online-videos">Online Videos</a></li>
<li><a href="#online-forum-discussions">Online Forum Discussions</a></li>
<li><a href="#other-online-resources">Other Online Resources</a></li>
<li><a href="#conference-papers">Conference Papers</a></li>
<li><a href="#blogs-and-articles">Blogs and Articles</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="description">Description</h2>
<p>This project explores the feasibility of transferring a trained Reinforcement Learning (RL) agent from a virtual environment to the real world, focusing on navigating a maze with a remote-controlled (RC) car.</p>
<h2 id="abstract">Abstract</h2>
<p>In this research project, I delve into the fascinating realm of artificial intelligence, specifically focusing on reinforcement learning (RL) and its application in real-world scenarios. The crux of my investigation revolves around the challenging question: &quot;Is it possible to transfer a trained RL agent from a simulation to the real world?&quot; This inquiry is particularly examined in the context of maze navigation.</p>
<p>This research is partitioned into sub-questions, which collectively aim to create a comprehensive understanding of the process. Firstly, I explore the various virtual environments available for training a virtual RF-car, seeking the most effective platform for my purposes. Secondly, I delve into identifying the most suitable reinforcement learning techniques for this specific application, considering factors like efficiency, adaptability, and real-world applicability. Lastly, the research seeks to bridge the gap between simulation and reality, investigating the practicality and challenges involved in this transition.</p>
<p>Through this study, I aspire to contribute significantly to the field of AI and robotics, offering insights and methodologies that could potentially advance the implementation of RL in real-world applications. The outcomes of this research could have far-reaching implications, not only in robotics but also in areas where simulation-based training is crucial.</p>
<p>Based on your input, it looks like you're building a comprehensive list of key terms and abbreviations for your paper on reinforcement learning and its application to a remote-controlled car in a sim2real transfer scenario. Here's how you can structure these sections for clarity and easy reference:</p>
<h2 id="glossary-of-terms">Glossary of Terms</h2>
<ol>
<li>
<p><strong>Artificial Intelligence (AI)</strong>: The simulation of human intelligence processes by machines, especially computer systems, enabling them to perform tasks that typically require human intelligence.</p>
</li>
<li>
<p><strong>Double Deep Q-Network (DDQN)</strong>: An enhancement of the Deep Q-Network (DQN) algorithm that addresses the overestimation of action values, thus improving learning stability and performance.</p>
</li>
<li>
<p><strong>Epsilon Decay</strong>: A technique in reinforcement learning that gradually decreases the rate of exploration over time, allowing the agent to transition from exploring the environment to exploiting known actions for better outcomes.</p>
</li>
<li>
<p><strong>Mean Squared Error (MSE)</strong>: A loss function used in regression models to measure the average squared difference between the estimated values and the actual value, useful for training models by minimizing error.</p>
</li>
<li>
<p><strong>Motion Processing Unit (MPU6050)</strong>: A sensor device combining a MEMS (Micro-Electro-Mechanical Systems) gyroscope and a MEMS accelerometer, providing comprehensive motion processing capabilities.</p>
</li>
<li>
<p><strong>Policy Network</strong>: In reinforcement learning, a neural network model that directly maps observed environment states to actions, guiding the agent's decisions based on the current policy.</p>
</li>
<li>
<p><strong>Raspberry Pi (RPI)</strong>: A small, affordable computer used for various programming projects, including robotics and educational applications.</p>
</li>
<li>
<p><strong>RC Car</strong>: A remote-controlled car used as a practical application platform in reinforcement learning experiments, demonstrating how algorithms can control real-world vehicles.</p>
</li>
<li>
<p><strong>Reinforcement Learning (RL)</strong>: A subset of machine learning where an agent learns to make decisions by taking actions within an environment to achieve specified goals, guided by a system of rewards and penalties.</p>
</li>
<li>
<p><strong>Sim2Real Transfer</strong>: The practice of applying models and strategies developed within a simulated environment to real-world situations, crucial for bridging the gap between theoretical research and practical application.</p>
</li>
<li>
<p><strong>Target Network</strong>: Utilized in the DDQN framework, a neural network that helps stabilize training by providing consistent targets for the duration of the update interval.</p>
</li>
<li>
<p><strong>Virtual Environment</strong>: A simulated setting designed for training reinforcement learning agents, offering a controlled, risk-free platform for experimentation and learning.</p>
</li>
</ol>
<h2 id="list-of-abbreviations">List of Abbreviations</h2>
<ol>
<li><strong>AI</strong> - Artificial Intelligence</li>
<li><strong>DDQN</strong> - Double Deep Q-Network</li>
<li><strong>DQN</strong> - Deep Q-Network</li>
<li><strong>ESP32</strong> - Espressif Systems 32-bit Microcontroller</li>
<li><strong>HC-SR04</strong> - Ultrasonic Distance Sensor</li>
<li><strong>MSE</strong> - Mean Squared Error</li>
<li><strong>MPU6050</strong> - Motion Processing Unit (Gyroscope + Accelerometer)</li>
<li><strong>PPO</strong> - Proximal Policy Optimization</li>
<li><strong>RC</strong> - Remote Controlled</li>
<li><strong>RPI</strong> - Raspberry Pi</li>
<li><strong>RL</strong> - Reinforcement Learning</li>
<li><strong>RCMazeEnv</strong> - RC Maze Environment (Custom Virtual Environment for RL Training)</li>
<li><strong>Sim2Real</strong> - Simulation to Reality Transfer</li>
</ol>
<h2 id="introduction">Introduction</h2>
<p>The journey of developing autonomous vehicles using reinforcement learning (RL) techniques in virtual environments is marked by continuous learning and adaptation. This paper, originally intended to showcase successful implementation strategies, has evolved to also highlight the challenges and iterative nature of such projects. The focus remains on the sim2real transfer and the specific challenges encountered in the alignment of an autonomous remote-controlled car.</p>
<h3 id="background-on-reinforcement-learning-rl">Background on Reinforcement Learning (RL)</h3>
<p>Reinforcement Learning (RL) is a paradigm of machine learning where an agent learns to make decisions by interacting with its environment. In RL, the agent seeks to maximize cumulative rewards through a process of trial and error, guided by feedback from its actions. The fundamental elements of RL include the agent, environment, actions, states, and rewards. The RL process can be mathematically described using Markov Decision Processes (MDP) where:</p>
<p>$$ S \text{ is a set of states} $$</p>
<p>$$ A \text{ is a set of actions} $$</p>
<p>$$ P(s*{t+1}|s_t, a_t) \text{ is the probability that action } a_t \text{ in state } s_t \text{ at time } t \text{ will lead to state } s*{t+1} $$</p>
<p>$$ R(s*{t}, a_t) \text{ is the reward received after transitioning from state } s_t \text{ to state } s*{t+1}, \text{ due to action } a_t $$</p>
<h3 id="real-world-applications-of-rl">Real-World Applications of RL</h3>
<p>RL has been successfully applied in various fields, including robotics, gaming, healthcare, finance, and autonomous vehicles. One notable example is the use of RL in AlphaGo, developed by DeepMind, which defeated the world champion in the game of Go. In robotics, RL enables robots to learn complex tasks like walking, manipulation, and navigation without explicit programming. In the financial sector, RL algorithms are used for algorithmic trading, optimizing portfolios, and managing risks.</p>
<h3 id="purpose-and-significance-of-the-study">Purpose and Significance of the Study</h3>
<p>The purpose of this study is to explore the feasibility and challenges of transferring a trained RL agent from a simulated environment to the real world. This transition, known as &quot;sim2real,&quot; is particularly examined in the context of maze navigation using a remote-controlled (RC) car. The significance of this research lies in its potential to bridge the gap between theoretical RL models and practical, real-world applications, which is a critical step in advancing the field of AI and robotics.</p>
<h3 id="overview-of-the-research-questions">Overview of the Research Questions</h3>
<p>The main research question focuses on whether a trained RL agent can be effectively transferred from a simulation to a real-world environment. Sub-questions delve into the selection of virtual environments for training, the identification of suitable RL techniques, the practical aspects of the sim2real transfer, and the evaluation of real-time learning capabilities. These questions aim to comprehensively understand the intricacies involved in applying RL in real-world scenarios.</p>
<h2 id="main-research-question">Main Research Question</h2>
<p><strong>Is it possible to transfer a trained RL-agent from a simulation to the real world? (case: maze)</strong></p>
<h3 id="sub-research-questions">Sub Research Questions</h3>
<ol>
<li>
<p>Which virtual environments exist to train a virtual RC-car?</p>
</li>
<li>
<p>Which reinforcement learning techniques can I best use in this application?</p>
</li>
<li>
<p>Can the simulation be transferred to the real world? Explore the difference between how the car moves in the simulation and in the real world.</p>
</li>
<li>
<p>Does the simulation have any useful contributions? In terms of training time or performance?</p>
</li>
<li>
<p>How can the trained model be transfered to the real RC car? (sim2real) How do you need to adjust the agent and the environment for it to translate to the real world?</p>
</li>
</ol>
<h2 id="methodology">Methodology</h2>
<h3 id="virtual-environment-design">Virtual Environment Design</h3>
<h4 id="rcmazeenv-environment">RCMazeEnv Environment</h4>
<ul>
<li>
<p><strong>Description</strong>: The RCMazeEnv is a custom class derived from the OpenAI Gym library. It simulates a robotic car navigating through a maze. The environment is designed to replicate real-world physics and constraints within a virtual setting.</p>
</li>
<li>
<p><strong>Maze Structure</strong>:</p>
<ul>
<li><strong>Starting Position</strong>: Top-left corner of the maze. With the car facing East.</li>
<li><strong>Goal</strong>: Bottom-right corner, representing the maze exit.</li>
<li><strong>Layout</strong>: The maze layout is configurable, allowing for various complexity levels.</li>
</ul>
</li>
<li>
<p><strong>Robotic Car Specifications</strong>:</p>
<ul>
<li><strong>Movement Actions</strong>: Forward, turn left, turn right.</li>
<li><strong>Orientation</strong>: North, East, South, West.</li>
<li><strong>Sensors</strong>: Front, left, and right distance sensors to walls.</li>
</ul>
</li>
<li>
<p><strong>Reward System</strong>:</p>
<ul>
<li>Negative rewards for each step and revisiting positions.</li>
<li>Substantial negative reward for hitting a wall.</li>
<li>Positive reward for proximity to the goal and reaching the exit.</li>
</ul>
</li>
<li>
<p><strong>Reset Functionality</strong>: Ability to reset the car to its starting position and reinitialize variables.</p>
</li>
</ul>
<h4 id="agent-design-double-deep-q-network-double-dqn">Agent Design: Double Deep Q-Network (Double DQN)</h4>
<ul>
<li>
<p><strong>Algorithm Overview</strong>: The Double DQN algorithm enhances traditional reinforcement learning methods by employing two neural networks, the policy network, and the target network, to reduce overestimation of Q-values.</p>
</li>
<li>
<p><strong>Network Architecture</strong>:</p>
<ul>
<li><strong>Policy Network</strong>: Selects actions based on the current state.</li>
<li><strong>Target Network</strong>: Provides a stable target for future state evaluation.</li>
<li><strong>Replay Memory</strong>: Stores experiences for learning and optimization.</li>
</ul>
</li>
<li>
<p><strong>Learning Process</strong>: The agent continually adapts through interaction with the environment, using sensor data to inform movement decisions.</p>
</li>
</ul>
<h3 id="experimental-setup">Experimental Setup</h3>
<ul>
<li>
<p><strong>Environment Setup</strong>:</p>
<ul>
<li>
<p><strong>Overview</strong>: The custom <code>RCMazeEnv</code> class, developed based on the OpenAI Gym library, simulates a robotic car navigating through a maze. This environment offers a rich platform for testing and refining reinforcement learning algorithms, focusing on sensor-based navigation and spatial decision-making.</p>
</li>
<li>
<p><strong>Key Features</strong>:</p>
<ul>
<li>
<ol>
<li><strong>Maze Configuration</strong>: A customizable maze layout with a start position at the top-left and a goal at the bottom-right corner.</li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>Robotic Car Actions</strong>: The car's actions include moving forward, turning left, or right, considering its orientation (North, East, South, West).</li>
</ol>
</li>
<li>
<ol start="3">
<li><strong>Sensors</strong>: Equipped with front, left, and right distance sensors for wall detection.</li>
</ol>
</li>
<li>
<ol start="4">
<li><strong>Reward System</strong>: Designed to encourage efficiency, penalize wall collisions and revisiting positions, and reward goal proximity and achievement.</li>
</ol>
</li>
</ul>
<p><img src="./images/reward_function.png" alt="reward equation">
$$
R(s, a, s') = \begin{cases}
-20 &amp; \text{if sensor readings indicate collision} \
500 - 200 \times \mathbb{I}(\text{steps} &gt; 1000) &amp; \text{if } s' \text{ is the goal position} \
\frac{50}{\text{distance_to_goal} + 1} + 50 \times \mathbb{I}(\text{distance_to_goal} &lt; \text{previous_distance}) - 25 \times \mathbb{I}(\text{distance_to_goal} &gt; \text{previous_distance}) &amp; \text{if getting closer/farther from goal} \
-10 \times \mathbb{I}(s' \in \text{visited_positions}) - 2 &amp; \text{otherwise}
\end{cases}
$$</p>
<ul>
<li>
<ol start="5">
<li><strong>Reset Functionality</strong>: Includes a <code>reset()</code> method to reinitialize the car's position and variables.</li>
</ol>
</li>
<li>
<ol start="6">
<li><strong>Visualization</strong>: A <code>render()</code> method for graphical representation of the maze, car, exit, and sensor readings.</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>Rendering Modes</strong>:</p>
<ul>
<li><strong>training Render Modes:</strong>
<ul>
<li><strong>Default (Human)</strong>: Utilizes a 2D Pygame rendering for visual output.</li>
<li><strong>Array Mode</strong>: A less resource-intensive mode that displays maze and car positions in the console, facilitating faster training.</li>
</ul>
</li>
<li><strong>Final Render Mode</strong>:
<ul>
<li><strong>PYopenGL</strong>: A 3D rendering mode that provides a more realistic representation of the maze and car with renders representing the sensor sensors, allowing for a more immersive experience.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Agent Setup</strong></p>
<ul>
<li>
<p><strong>Agent's Role</strong>: The Double DQN agent is central to navigating the RCMazeEnv. It leverages two neural networks (policy and target) to make informed movement decisions based on sensor input, enhancing learning efficiency and effectiveness.</p>
</li>
<li>
<p><strong>Features</strong>:</p>
</li>
</ul>
<ol>
<li><strong>Network Architecture</strong>: Comprises a sequential model with dense layers, tailored for processing sensor data and selecting optimal actions.</li>
<li><strong>Learning Mechanisms</strong>:
<ul>
<li>Utilizes experiences stored in a replay memory for batch learning.</li>
<li>Separates action selection (policy network) from Q-value generation (target network) to minimize overestimation bias.</li>
<li>Employs periodic updates to the target network to maintain stability.</li>
</ul>
</li>
</ol>
<ul>
<li>
<p><strong>Training Strategy</strong>:</p>
</li>
<li>
<p><strong>Policy Network Training</strong>: Involves fitting the network to batches of experiences, updating Q-values based on both the policy and target network predictions.</p>
</li>
<li>
<p><strong>Action Prediction</strong>: Employs the policy network for action prediction during the maze navigation.</p>
</li>
<li>
<p><strong>Target Network Updating</strong>: Ensures the target network's weights are periodically aligned with the policy network.</p>
</li>
<li>
<p><strong>Operational Flow</strong>:</p>
<ul>
<li>The agent iteratively interacts with the environment, making movement decisions based on current sensor data.</li>
<li>Its performance is continuously monitored and adjusted based on the reward system.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Real-World Implementation</strong></p>
<ul>
<li>
<p><strong>Overview</strong>:</p>
<ul>
<li>The real-world implementation involves assembling an RC robot, designed to navigate through a physical maze using sensor data and reinforcement learning algorithms. This section details the assembly process and hardware setup, crucial for the practical application of the trained reinforcement learning agent.</li>
</ul>
</li>
<li>
<p><strong>Components and Assembly</strong>:</p>
<ul>
<li>
<ol>
<li><strong>Core Components</strong>: The robot is built using an ESP32-WROOM-32 module, motor driver, hc-sro04 ultrasonic sensors, mpu6050 and a mini oled screen.</li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>Assembly Process</strong>:</li>
</ol>
<ul>
<li><strong>Base Assembly</strong>: The chassis, provided in the 2WD robot kit, forms the base of the robot.</li>
<li><strong>Motor and Driver Installation</strong>: Motors are attached to the base and connected to the motor driver for movement control.</li>
<li><strong>Sensor Integration</strong>: Ultrasonic sensors (HC-SR04) are mounted on the robot for distance measurement. The MPU6050 gyroscope is used for accurate turning. The mini OlED screen is used for displaying the IP address.</li>
<li><strong>Microcontroller and Power Setup</strong>: The ESP32 module is setup with a 1860 li ion battery as a power source.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Wiring and Configuration</strong></p>
<ul>
<li>
<ol start="2">
<li><strong>ESP32 Module Wiring</strong>: The ESP32 module is wired to the motor driver for directing the movement of the robot based on the agent's decisions. The mini OlED screen, the MPU6050 gyroscope and the HC-SR04 ultrasonic sensors are all connected to their respective pins on the ESP32 module. Refer to the wiring diagram shown later in this paper for more details.</li>
</ol>
</li>
<li>
<ol start="3">
<li><strong>Programming</strong>: The ESP32 is programmed and used to send the HC-SR04 sensor readings to the agent and receive commands based on those to control the robot's movement, with the help of the MPU6050 for accurate rotations.</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>Challenges and Adjustments</strong>:</p>
<ul>
<li><strong>Sensor Calibration</strong>: Fine-tuning sensor readings to match the real-world environment conditions.</li>
<li><strong>Motor Control</strong>: Adjusting motor commands for precise movement in the physical space.</li>
<li><strong>Integration with RL Agent</strong>: Ensuring seamless communication between the robot's hardware and the software agent.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Evaluation Metrics</strong></p>
<ul>
<li>
<p><strong>Simulation Metrics</strong></p>
<ul>
<li>
<p><strong>Objective</strong>: Assess the agent's ability to solve the maze efficiently in the virtual environment.</p>
</li>
<li>
<p><strong>Episodic Performance</strong>: Analysis of the number of episodes required for consistent maze resolution with optimal rewards.</p>
</li>
<li>
<p><strong>Step Efficiency</strong>: Monitoring the number of steps taken by the agent to complete the maze, indicating efficiency improvements.</p>
</li>
<li>
<p><strong>MSE Loss Measurement</strong>:</p>
<ul>
<li>
<p><strong>Formula</strong>:</p>
<p><img src="./images/MSE_equation.png" alt="MSE Loss"></p>
</li>
<li>
<p><strong>Visualization</strong>: <img src="./images/mse_DDQN.png" alt="MSE Loss"></p>
</li>
</ul>
</li>
<li>
<p><strong>Reward Trend Analysis</strong>:</p>
<ul>
<li>Chart:
<ul>
<li><img src="./images/DDQN_reward_history.png" alt="Reward History"></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Epsilon Decay Tracking</strong>:</p>
<ul>
<li>Chart:
<ul>
<li><img src="./images/Epsilon_history_DDQN.png" alt="Epsilon Decay"></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Simulation Test Video</strong>:</p>
<ul>
<li>
<p><strong>Clip</strong>:
<img src="https://github.com/driessenslucas/researchproject/assets/91117911/66539a97-e276-430f-ab93-4a8a5138ee5e" alt="DDQN test"></p>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/66539a97-e276-430f-ab93-4a8a5138ee5e">https://github.com/driessenslucas/researchproject/assets/91117911/66539a97-e276-430f-ab93-4a8a5138ee5e</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Real-World Metrics</strong></p>
<ul>
<li><strong>Maze Navigation</strong>: Visual assessment of the RC car's capability to navigate the real-world maze.</li>
<li><strong>Sensor Data Analysis</strong>: Evaluating real-time sensor data for navigation and collision avoidance.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Physical Maze Construction</strong>:</p>
<ul>
<li>
<p><strong>Overview</strong>: As an integral part of the research, a physical maze was constructed to mirror the virtual RCMazeEnv. This real-world maze serves as a crucial platform for testing and evaluating the RC robot's navigation capabilities and the effectiveness of the reinforcement learning algorithms in a tangible environment.</p>
</li>
<li>
<p><strong>Guide</strong>: The complete construction process, including the list of materials, tools required, assembly instructions, is documented in detail in a separate installation guide. This guide provides step-by-step instructions, accompanied by images and diagrams, to replicate the physical maze accurately.</p>
<ul>
<li>
<p>For the full assembly process, solutions, and final setup, please refer to the guide: <a href="https://github.com/driessenslucas/researchproject/blob/main/hardware_installtion.md">guide</a>.</p>
</li>
<li>
<p>Integration with the Research: The physical maze's role in the research extends beyond a mere testing ground. It allows for a direct comparison between virtual simulations and real-world applications, thereby enhancing the validity and applicability of my findings.</p>
</li>
</ul>
</li>
<li>
<p><strong>Visual Representation</strong>:</p>
<ul>
<li><img src="./images/final_test/final_maze_build.jpeg" alt="maze"></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Web Application</strong></p>
<ul>
<li><strong>Purpose</strong>: Provides a visualization platform for the simulation and a control interface for real-life testing, acting as a virtual twin of the RC car.</li>
<li><strong>Construction</strong>:
<ul>
<li><strong>Backend</strong>: Flask for web server and routing.</li>
<li><strong>Simulation Rendering</strong>: PyOpenGL.</li>
<li><strong>Real-Time Communication</strong>: SocketIO for data transfer between the app and the simulation.</li>
</ul>
</li>
<li><strong>Functionality</strong>:
<ul>
<li><strong>Simulation Streaming</strong>: Capturing and transmitting live simulation snapshots to the web interface.</li>
<li><strong>Data Display</strong>: Real-time visualization of sensor data and Q-values.</li>
<li><strong>User Controls</strong>: Model selection, IP configuration for ESP32, mode selection (simulation or real RC car), and simulation control (start/stop).</li>
</ul>
</li>
<li><strong>Visual and Interactive Interface</strong>:
<ul>
<li><strong>Visual Representation</strong>: <img src="./images/web_app_v4.png" alt="Web App Interface"></li>
<li><strong>Clip</strong>:</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/b440b295-6430-4401-845a-a94186a9345f">https://github.com/driessenslucas/researchproject/assets/91117911/b440b295-6430-4401-845a-a94186a9345f</a></p>
<!-- <https://github.com/driessenslucas/researchproject/assets/91117911/99b584b8-4bc3-4195-8342-57bf62a456ff> -->
<h4 id="training-process-of-the-double-dqn-agent">Training Process of the Double DQN Agent</h4>
<h5 id="model-architecture">Model Architecture</h5>
<p>The Double DQN model employed in this research is structured as follows:</p>
<pre class="hljs"><code><div>Model: "sequential_52"

---

<span class="hljs-section"># Layer (type) Output Shape Param</span>

=================================================================

dense_200 (Dense) (None, 32) 224

dense_201 (Dense) (None, 64) 2112

dense_202 (Dense) (None, 32) 2080

dense_203 (Dense) (None, 3) 99

=================================================================
Total params: 4515 (17.64 KB)
Trainable params: 4515 (17.64 KB)
Non-trainable params: 0 (0.00 Byte)

---
</div></code></pre>
<p>This neural network consists of four dense layers, with the output shape and parameter count as detailed above. The final layer outputs three actions corresponding to the movement capabilities of the RC car: moving forward, turning left, and turning right.</p>
<h4 id="training-parameters">Training Parameters</h4>
<p>The training of the Double DQN agent was governed by the following parameters:</p>
<ul>
<li><strong>Discount Factor (<code>DISCOUNT</code>)</strong>: 0.90</li>
<li><strong>Batch Size</strong>: 128
<ul>
<li>Number of steps (samples) used for training at a time.</li>
</ul>
</li>
<li><strong>Update Target Interval (<code>UPDATE_TARGET_INTERVAL</code>)</strong>: 2
<ul>
<li>Frequency of updating the target network.</li>
</ul>
</li>
<li><strong>Epsilon (<code>EPSILON</code>)</strong>: 0.99
<ul>
<li>Initial exploration rate.</li>
</ul>
</li>
<li><strong>Minimum Epsilon (<code>MIN_EPSILON</code>)</strong>: 0.01
<ul>
<li>Minimum value for exploration rate.</li>
</ul>
</li>
<li><strong>Epsilon Decay Rate (<code>DECAY</code>)</strong>: 0.99993
<ul>
<li>Rate at which exploration probability decreases.</li>
</ul>
</li>
<li><strong>Number of Episodes (<code>EPISODE_AMOUNT</code>)</strong>: 170
<ul>
<li>Total episodes for training the agent.</li>
</ul>
</li>
<li><strong>Replay Memory Capacity (<code>REPLAY_MEMORY_CAPACITY</code>)</strong>: 2,000,000
<ul>
<li>Maximum size of the replay buffer.</li>
</ul>
</li>
<li><strong>Learning Rate</strong>: 0.001
<ul>
<li>The rate at which the model learns from new observations.</li>
</ul>
</li>
</ul>
<h5 id="training-procedure">Training Procedure</h5>
<ol>
<li><strong>Initialization</strong>: Start with a high exploration rate (<code>EPSILON</code>) allowing the agent to explore the environment extensively.</li>
<li><strong>Episodic Training</strong>: For each episode, the agent interacts with the environment, collecting state, action, reward, and next state data.</li>
<li><strong>Replay Buffer</strong>: Store these experiences in a replay memory, which helps in breaking the correlation between sequential experiences.</li>
<li><strong>Batch Learning</strong>: Randomly sample a batch of experiences from the replay buffer to train the network.</li>
<li><strong>Target Network Update</strong>: Every <code>UPDATE_TARGET_INTERVAL</code> episodes, update the weights of the target network with those of the policy network.</li>
<li><strong>Epsilon Decay</strong>: Gradually decrease the exploration rate (<code>EPSILON</code>) following the decay rate (<code>DECAY</code>), shifting the strategy from exploration to exploitation.</li>
<li><strong>Performance Monitoring</strong>: Continuously monitor the agent's performance in terms of rewards and success rate in navigating the maze.</li>
</ol>
<h4 id="reinforcement-learning-techniques-overview">Reinforcement Learning Techniques Overview</h4>
<h5 id="1-deep-q-network-dqn">1. Deep Q-Network (DQN)</h5>
<ul>
<li>
<p><strong>Description</strong>: The Deep Q-Network (DQN) combines a deep neural network with a Q-learning framework. It excels in handling high-dimensional sensory inputs, making it ideal for environments demanding detailed interaction.</p>
</li>
<li>
<p><strong>Suitability</strong>: DQN's advanced learning capabilities are tempered by its tendency to overestimate Q-values in complex environments. This limitation could affect its effectiveness in training RC-cars, where environmental dynamics are unpredictable.</p>
</li>
<li>
<p><strong>Integration and Results</strong>:</p>
<ul>
<li><strong>Visual Representation</strong>:</li>
</ul>
<p><img src="https://github.com/driessenslucas/researchproject/assets/91117911/a7c5964e-139c-46a1-af79-85280a26c9d2" alt="DQN-agent"></p>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/a7c5964e-139c-46a1-af79-85280a26c9d2">https://github.com/driessenslucas/researchproject/assets/91117911/a7c5964e-139c-46a1-af79-85280a26c9d2</a></p>
<ul>
<li><strong>Reward History</strong>:</li>
</ul>
<p><img src="./images/reward_history_dqn.png" alt="DQN Reward History"></p>
<ul>
<li><strong>Performance</strong>: DQN's performance, while competent, was limited by Q-value overestimation in intricate scenarios.</li>
</ul>
</li>
</ul>
<h5 id="2-double-deep-q-network-ddqn">2. Double Deep Q-Network (DDQN)</h5>
<ul>
<li>
<p><strong>Description</strong>: The Double Deep Q-Network (DDQN) improves upon DQN by employing two neural networks. This structure effectively reduces overestimation bias by separating action selection from Q-value generation.</p>
</li>
<li>
<p><strong>Reason for Selection</strong>:</p>
<ul>
<li>DDQN's accuracy in Q-value approximation is crucial for navigating complex environments, such as mazes.</li>
<li>The RC-car's sensor limitations, which could lead to Q-value overestimations, are better addressed by DDQN.</li>
<li>Empirical trials showed DDQN's superior performance in maze navigation tasks.</li>
</ul>
</li>
<li>
<p><strong>Integration and Results</strong>:</p>
<ul>
<li><strong>Visual Representation</strong>:</li>
</ul>
<p><img src="https://github.com/driessenslucas/researchproject/assets/91117911/de50eaf8-49b9-4bf3-8083-8b2bc0963001" alt="DDQN Integration"></p>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/de50eaf8-49b9-4bf3-8083-8b2bc0963001">https://github.com/driessenslucas/researchproject/assets/91117911/de50eaf8-49b9-4bf3-8083-8b2bc0963001</a></p>
<ul>
<li><strong>Reward History</strong>:</li>
</ul>
<p><img src="./images/DDQN_reward_history.png" alt="DDQN Reward History"></p>
<ul>
<li><strong>Performance</strong>: DDQN solved the environment in an average of 25 steps, compared to DQN's 34 steps, highlighting its efficiency.</li>
</ul>
</li>
</ul>
<h5 id="3-proximal-policy-optimization-ppo">3. Proximal Policy Optimization (PPO)</h5>
<ul>
<li>
<p><strong>Description</strong>: Proximal Policy Optimization (PPO) is a policy gradient method that directly optimizes decision-making policies. It's known for its stability and efficiency in specific RL contexts.</p>
</li>
<li>
<p><strong>Suitability</strong>: PPO's emphasis on policy optimization over value estimation makes it less suitable for RC-car simulations, where accurate Q-value approximation is key.</p>
</li>
<li>
<p><strong>Integration and Results</strong>:</p>
<ul>
<li><strong>Visual Representation</strong>:</li>
</ul>
<p><img src="https://github.com/driessenslucas/researchproject/assets/91117911/23a34a9d-7957-4484-a7ce-cfc74c4b9790" alt="PPO Integration"></p>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/23a34a9d-7957-4484-a7ce-cfc74c4b9790">https://github.com/driessenslucas/researchproject/assets/91117911/23a34a9d-7957-4484-a7ce-cfc74c4b9790</a></p>
<ul>
<li><strong>Reward History</strong>:</li>
</ul>
<p><img src="./images/PPO_reward_history.png" alt="PPO Reward History"></p>
<ul>
<li><strong>Performance</strong>: PPO, while stable, did not align well with the precision requirements for RC-car maze navigation.</li>
</ul>
</li>
</ul>
<h4 id="hardware-setup-and-assembly">Hardware Setup and Assembly</h4>
<h5 id="introduction-to-hardware-components">Introduction to Hardware Components</h5>
<p>This section provides a detailed overview of the hardware components used in the research project, focusing on the assembly and configuration of the RC robot designed for maze navigation.</p>
<p><img src="./images/final_test/jp_final.jpeg" alt="final_robot"></p>
<h5 id="components-list">Components List</h5>
<ul>
<li><strong>Core Components</strong>:
<ul>
<li>ESP32-WROOM-32 module (Refer to the datasheet at <a href="https://www.espressif.com/sites/default/files/documentation/esp32-wroom-32_datasheet_en.pdf">Espressif</a>)</li>
<li>3D printed parts from Thingiverse (<a href="https://www.thingiverse.com/thing:3436448/files">hc-sr04</a>, <a href="https://www.thingiverse.com/thing:2544002">top plate + alternative for the robot kit</a>)</li>
<li>Motor Driver - available at <a href="https://www.dfrobot.com/product-66.html">DFRobot</a></li>
<li>2WD robot kit - available at <a href="https://www.dfrobot.com/product-367.html">DFRobot</a></li>
<li>Mini OlED screen - available at <a href="https://www.amazon.com.be/dp/B0BB1T23LF">Amazon</a></li>
<li>Sensors - available at <a href="https://www.amazon.com.be/dp/B07XF4815H">Amazon</a></li>
<li>Battery For ESP 32 - available at <a href="https://www.amazon.com.be/dp/B09Q4ZMNLW">Amazon</a></li>
</ul>
</li>
<li><strong>Supplementary Materials</strong>: List of additional materials like screws, wires, and tools required for assembly.
<ul>
<li>4mm thick screws 5mm long to hold the wood together - available at <a href="https://www.brico.be/nl/gereedschap-installatie/ijzerwaren/schroeven/universele-schroeven/sencys-universele-schroeven-torx-staal-gegalvaniseerd-20-x-4-mm-30-stuks/5368208">brico</a></li>
<li>m3 bolt &amp; nuts - available at <a href="https://www.brico.be/nl/gereedschap-installatie/ijzerwaren/bouten/sencys-cilinderkop-bout-gegalvaniseerd-staal-m3-x-12-mm-30-stuks/5367637">brico</a></li>
<li>wood for the maze - available at <a href="https://www.brico.be/nl/bouwmaterialen/hout/multiplex-panelen/sencys-vochtwerend-multiplex-paneel-topplex-250x122x1-8cm/5356349">brico</a></li>
</ul>
</li>
</ul>
<h5 id="wiring-guide">Wiring Guide</h5>
<ol>
<li><strong>ESP32 Wiring</strong>:
<ul>
<li><img src="./images/schematics/esp_updated.png" alt="ESP32 Wiring"></li>
</ul>
</li>
</ol>
<h2 id="challenges-and-solutions-in-implementing-rl-techniques-and-virtual-environments">Challenges and Solutions in Implementing RL Techniques and Virtual Environments</h2>
<h3 id="challenge-1-selection-of-an-appropriate-virtual-environment">Challenge 1: Selection of an Appropriate Virtual Environment</h3>
<ul>
<li><strong>Description</strong>: Choosing a virtual environment conducive to effective RC-car training is crucial.</li>
<li><strong>Solution</strong>: After evaluating various platforms, <strong>OpenAI Gym</strong> was selected for its simplicity, familiarity from previous coursework, and its focus on reinforcement learning.</li>
</ul>
<h3 id="challenge-2-choosing-the-optimal-reinforcement-learning-technique">Challenge 2: Choosing the Optimal Reinforcement Learning Technique</h3>
<ul>
<li><strong>Description</strong>: Selecting the most effective RL technique for training the virtual RC-car.</li>
<li><strong>Solution</strong>: Through comparative analysis and empirical testing, the Double Deep Q-Network (DDQN) was identified as the most suitable technique, demonstrating superior performance in navigating complex environments with fewer episodes.</li>
</ul>
<h3 id="challenge-3-sim2real-transfer---addressing-movement-discrepancies">Challenge 3: Sim2Real Transfer - Addressing Movement Discrepancies</h3>
<ul>
<li><strong>Description</strong>: Bridging the gap between simulation and real-world in terms of RC-car movement and control.</li>
<li><strong>Solution Attempt</strong>: Fine-tuning the frequency of action commands with an async method, waiting for the motor to finish moving or considering a queued action system. Futher more the importance of precise movement in the real world was highlighted, which was not a problem in the simulation.</li>
</ul>
<h3 id="challenge-4-alignment-issue-and-motor-encoder-implementation">Challenge 4: alignment Issue and Motor Encoder Implementation</h3>
<ul>
<li><strong>Description</strong>: Difficulty in achieving precise straight-line movement in the RC car, with a persistent ~3-degree offset.</li>
<li><strong>Solution Attempt 1</strong>: Implementation of motor encoders was pursued to enhance movement accuracy. However, this approach faced the same limitations in achieving the desired precision.</li>
<li><strong>Solution Attempt 2</strong>: The motor was replaced with a more powerful one, which initially showed promise in addressing the alignment issue. However, after adding all the other components, the car's weight increased, leading to the same problem. <a href="#video-4-new-rc-car-with-encoder-and-more-powerful-motor">view video</a></li>
<li><strong>Solution Attempt 3</strong>: The use of a MPU6050 gyroscope was explored to measure the car's orientation and adjust the movement accordingly. Even though this approach succeeded to some extent (90 degrees turns were accurate), it was not able to solve the ~3-degree offset issue when moving forward.<a href="#video-1-mpu6050-90-degree-turn">vdeo of turning 90 degrees</a> <a href="#video-2-mpu6050-to-align-forward-movement">video of moving forward</a></li>
<li><strong>Solution Attempt 4</strong>: The final solution I tried was done by removing the RPI5 (previously used for sensor data and running the web app) from the robot all together and using the ESP32 to control both all the sensors and the motors. This allowed for a more lightweight robot, which was able to move forward more precisely but it failed to rotate 90 degrees accurately. <a href="#video-6-robot-v2">view video</a></li>
</ul>
<h3 id="challenge-5-ensuring-consistent-and-effective-training">Challenge 5: Ensuring Consistent and Effective Training</h3>
<ul>
<li><strong>Description</strong>: Maximizing training efficiency and performance while maintaining consistency between simulation and real-world scenarios.</li>
<li><strong>Solution</strong>: The simulation demonstrated considerable advantages in terms of training efficiency, safety, and computational power, establishing it as an indispensable tool in autonomous vehicle model development.</li>
</ul>
<h3 id="challenge-6-accurate-sensor-data-normalization-for-sim2real-transfer">Challenge 6: Accurate Sensor Data Normalization for Sim2Real Transfer</h3>
<ul>
<li>
<p><strong>Description</strong>: Aligning sensor data between simulated and real-world environments is critical for model accuracy.</p>
</li>
<li>
<p><strong>Solution</strong>: Implementing specific normalization techniques for both real-world and simulation sensor data ensured consistency and compatibility, enhancing the model's accuracy in real-world applications.</p>
<ul>
<li>
<ol>
<li><strong>Real-World Sensor Data Normalization:</strong></li>
</ol>
<p>The function <code>map_distance</code> normalizes real-world sensor data. It can be represented as follows:</p>
<p><img src="./images/map_distance_equation.png" alt="map_distance"></p>
<p>This function keeps distances under 25 cm unchanged and applies a scaling factor of 0.5 to distances beyond 25 cm, adding this scaled value to a base of 25 cm.</p>
</li>
<li>
<ol start="2">
<li><strong>Simulation Sensor Data Normalization:</strong></li>
</ol>
<p>The function <code>normalize_distance</code> adjusts simulated sensor data to a 0-1 range. Its equation is:</p>
<p><img src="./images/normalize_distance_equation.png" alt="normalize_distance"></p>
<p>In this function, the distance is first scaled by dividing by <code>sensor_max_range</code>. It's then clamped between 0 and 1 before multiplying by 1000 to normalize it within a specific range.</p>
</li>
</ul>
</li>
</ul>
<h3 id="challenge-7-integration-of-failsafe-mechanisms">Challenge 7: Integration of Failsafe Mechanisms</h3>
<ul>
<li><strong>Description</strong>: Preventing potential collisions and ensuring safe navigation in the real world.</li>
<li><strong>Solution</strong>: Development of a failsafe system that prevents forward movement in hazardous situations, retraining the model with this protocol to align real-world behavior with the simulated environment.</li>
</ul>
<h3 id="challenge-8-training-environment-and-technique-efficacy">Challenge 8: Training Environment and Technique Efficacy</h3>
<ul>
<li><strong>Description</strong>: Determining the most effective environment and RL technique for training.</li>
<li><strong>Solution</strong>: The DDQN solved the environment more efficiently than DQN, highlighting the importance of technique selection. The simulation provided a safer, more controlled environment for training, reinforcing its selection over real-world training.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>This section has outlined the practical challenges encountered in applying reinforcement learning (RL) techniques to autonomous RC cars. My journey began with the selection of OpenAI Gym as the virtual environment, chosen for its simplicity and relevance to RL. The Double Deep Q-Network (DDQN) emerged as the most effective RL technique for navigating complex environments.</p>
<p>However, transitioning from simulated models to real-world applications revealed significant discrepancies, particularly in movement control and sensor data alignment. I explored innovative solutions such as the implementation of motor encoders, power adjustments, and gyroscope integration, which partially addressed these issues. Efforts to normalize sensor data and implement failsafe mechanisms also contributed to better alignment with real-world conditions.</p>
<p>A significant advancement was achieved by replacing the Raspberry Pi and ESP32 with just the ESP32 module in the robot's design, leading to a more lightweight and precise robot. This change marked a considerable step in overcoming the challenges previously faced.</p>
<p>Although I made substantial progress, some challenges remain. This indicates a need for ongoing research and development to fully harness the potential of RL in autonomous RC car navigation.</p>
<p>In conclusion, this project underscores the iterative and demanding nature of applying RL techniques in real-world scenarios. It highlights the importance of continuous refinement, innovation, and adaptation, beyond the theoretical knowledge base. The journey through these challenges has emphasized the significance of perseverance and creative problem-solving in the evolving field of autonomous vehicle technology.</p>
<h3 id="supplementary-materials-video-demonstrations">Supplementary Materials: Video Demonstrations</h3>
<h4 id="introduction">Introduction</h4>
<p>This section provides examples of how I attempted to solve some of the challenges encountered in this research project. For more videos, please refer to the <a href="https://github.com/driessenslucas/researchproject/tree/main/videos/testvideos">video folder</a></p>
<h4 id="video-1-mpu6050-90-degree-turn">Video 1: mpu6050 90 degree turn</h4>
<ul>
<li><strong>Description</strong>: This video demonstrates the use of the MPU6050 gyroscope to measure the car's orientation move until the car has rotated ~90 degrees since the start of the movement. This approach was used in an attempt to address the alignment issues when using a delay to measure the amount of time the car needs to make a 90 degree turn.</li>
<li><strong>test 1</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/32d9e29f-6d5a-4676-b609-2c08923ca1ac">https://github.com/driessenslucas/researchproject/assets/91117911/32d9e29f-6d5a-4676-b609-2c08923ca1ac</a></p>
<ul>
<li><strong>test 2</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/624b40f2-bee8-49f6-961d-1f72ab18fe13">https://github.com/driessenslucas/researchproject/assets/91117911/624b40f2-bee8-49f6-961d-1f72ab18fe13</a></p>
<h4 id="video-2-mpu6050-to-align-forward-movement">Video 2: mpu6050 to align forward movement</h4>
<ul>
<li><strong>Description</strong>: This video demonstrates the use of the MPU6050 gyroscope to measure the car's orientation while driving forward and adjust the movement accordingly. This approach was used in an attempt to address the alignment issues, but it was not able to solve the ~3-degree offset issue when moving forward.</li>
<li><strong>test 1</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/bb9aa643-9620-4979-a70c-ec2826c7dd33">https://github.com/driessenslucas/researchproject/assets/91117911/bb9aa643-9620-4979-a70c-ec2826c7dd33</a></p>
<ul>
<li><strong>test 2</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/689b590f-3a9a-4f63-ba9c-978ddd08ab53">https://github.com/driessenslucas/researchproject/assets/91117911/689b590f-3a9a-4f63-ba9c-978ddd08ab53</a></p>
<ul>
<li><strong>test 3</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/99da37df-d147-43dc-828f-524f55dc6f70">https://github.com/driessenslucas/researchproject/assets/91117911/99da37df-d147-43dc-828f-524f55dc6f70</a></p>
<h4 id="video-4-new-rc-car-with-encoder-and-more-powerful-motor">video 4: New RC-car with encoder and more powerful motor</h4>
<ul>
<li><strong>Description</strong>: This video demonstrates the use of a rotary encoder to measure the amount of rotations the wheels have made. This approach was used in an attempt to address the address the ~3 degree offset when moving forward. This approach was looking promising, until adding the other components to the car, which increased the weight of the car, leading to the same problem.</li>
<li><strong>test 1</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/9728e29a-d2fa-48fa-b6e0-e2e1da92228f">https://github.com/driessenslucas/researchproject/assets/91117911/9728e29a-d2fa-48fa-b6e0-e2e1da92228f</a></p>
<ul>
<li><strong>test 2</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/b9ce2cc3-85fd-4136-8670-516c123ba442">https://github.com/driessenslucas/researchproject/assets/91117911/b9ce2cc3-85fd-4136-8670-516c123ba442</a></p>
<h4 id="video-5-encoder-implementation-original-rc-car">video 5: Encoder implementation (original RC-car)</h4>
<ul>
<li><strong>Description</strong>: This video demonstrates reading out the wheel rotations measured by the rotary encoder. This approach again looked promising, but shortly after this video one of the encoders broke, so no further tests with this specific encoder were done.</li>
<li><strong>test 1</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/ae5129fa-c25f-4f89-92bb-4ee81df9f7a5">https://github.com/driessenslucas/researchproject/assets/91117911/ae5129fa-c25f-4f89-92bb-4ee81df9f7a5</a></p>
<h4 id="video-6-robot-v2">video 6: Robot v2</h4>
<ul>
<li><strong>Description</strong>: This video demonstrates the final version of the RC-car. This version uses the ESP32 to control both the sensors and the motors. This allowed for a more lightweight robot, which was able to move more precisely.</li>
<li><strong>test 1</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/1773a4f5-8618-4114-ad4c-11781bee4088">https://github.com/driessenslucas/researchproject/assets/91117911/1773a4f5-8618-4114-ad4c-11781bee4088</a></p>
<h4 id="video-7-maze-test-outdoors">video 7: Maze Test Outdoors</h4>
<ul>
<li><strong>Description</strong>: This video demonstrates an attempt to test the RC-car outdoors. This test was not successful due to surface texture.</li>
<li><strong>test 1</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/02df8a25-b7f0-4061-89b7-414e6d25d31c">https://github.com/driessenslucas/researchproject/assets/91117911/02df8a25-b7f0-4061-89b7-414e6d25d31c</a></p>
<ul>
<li><strong>test 2</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/187561a7-c0cb-4921-af3e-9c2c99cb0137">https://github.com/driessenslucas/researchproject/assets/91117911/187561a7-c0cb-4921-af3e-9c2c99cb0137</a></p>
<h4 id="video-8-maze-test-indoors">video 8: Maze Test Indoors</h4>
<ul>
<li><strong>Description</strong>: This video demonstrates the RC-car navigating the maze indoors. This test was also not successful due imperfect conditions.</li>
<li><strong>test 1</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/ce0f47e9-26cd-459e-8b26-ff345d1ee96b">https://github.com/driessenslucas/researchproject/assets/91117911/ce0f47e9-26cd-459e-8b26-ff345d1ee96b</a></p>
<ul>
<li><strong>test 2</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/ea4a9bff-e191-4ce2-b2cc-acc57c781fa3">https://github.com/driessenslucas/researchproject/assets/91117911/ea4a9bff-e191-4ce2-b2cc-acc57c781fa3</a></p>
<ul>
<li><strong>test 3</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/4783729f-10cc-4c61-afa4-71cfc93d5d3e">https://github.com/driessenslucas/researchproject/assets/91117911/4783729f-10cc-4c61-afa4-71cfc93d5d3e</a></p>
<ul>
<li><strong>test 4</strong>:</li>
</ul>
<p><a href="https://github.com/driessenslucas/researchproject/assets/91117911/77091cb5-dbc5-4447-abc2-dc820dc66188">https://github.com/driessenslucas/researchproject/assets/91117911/77091cb5-dbc5-4447-abc2-dc820dc66188</a></p>
<h3 id="real-world-application-and-limitations">Real-World Application and Limitations</h3>
<h4 id="introduction-to-sensor-and-movement-discrepancies">Introduction to Sensor and Movement Discrepancies</h4>
<p>The transition from a simulated environment to real-world application introduces unique challenges, particularly in terms of sensor data interpretation and car movement replication. This section explores these aspects in detail.</p>
<h4 id="real-world-application">Real-World Application</h4>
<h5 id="sensor-based-navigation">Sensor-Based Navigation</h5>
<p>In real-world scenarios, the application of sensor-based navigation, as developed in the simulation, can significantly enhance the capabilities of autonomous vehicles. This technology can be pivotal in environments where precision and adaptability are crucial, such as in urban traffic management or automated delivery systems.</p>
<h5 id="impact-on-autonomous-vehicle-movement">Impact on Autonomous Vehicle Movement</h5>
<p>The insights gained from the simulation regarding vehicle movement can inform the development of more sophisticated autonomous vehicle behavior, particularly in navigating complex, dynamic environments. This knowledge is invaluable for improving the safety and efficiency of autonomous transportation systems.</p>
<h4 id="limitations">Limitations</h4>
<h5 id="sensor-data-discrepancies">Sensor Data Discrepancies</h5>
<p>One major challenge lies in the discrepancies between sensor data in the simulation and the real world. These differences can affect the accuracy of the navigational algorithms and, consequently, the vehicle's ability to make real-time decisions.</p>
<h5 id="movement-replication-challenges">Movement Replication Challenges</h5>
<p>Replicating the precise movements of the simulated car in a real-world setting poses significant challenges. Factors such as surface texture, vehicle weight, and mechanical limitations can lead to variations in movement that are not present in the controlled environment of the simulation.</p>
<h5 id="practical-considerations">Practical Considerations</h5>
<p>Practical implementation of these findings necessitates considering variables such as sensor calibration, environmental factors, and hardware capabilities. Overcoming these challenges is essential for the successful application of sim2real transfer in autonomous vehicle navigation.</p>
<h4 id="conclusion">Conclusion</h4>
<p>The transition from simulation to real-world application in autonomous vehicle navigation, especially regarding sensor usage and car movement, presents both promising opportunities and significant challenges. Addressing these discrepancies is key to harnessing the full potential of sim2real transfer in practical, real-world scenarios.</p>
<h2 id="answers-to-research-questions">Answers to Research Questions</h2>
<h3 id="1-which-virtual-environments-exist-to-train-a-virtual-rf-car">1. Which virtual environments exist to train a virtual RF-car?</h3>
<h4 id="introduction">Introduction</h4>
<p>The selection of an appropriate virtual environment is crucial for the effective training of a virtual Radio Frequency (RF) car. Various platforms offer unique features suitable for simulating different aspects of autonomous driving. This document evaluates several virtual environments to determine the most suitable choice for training a virtual RF-car.</p>
<h3 id="executive-summary">Executive Summary</h3>
<p>There is no 'one-size-fits-all' solution for virtual environments. I used openAI gym because it is easy to create custom environments in it and I have experience with it from my Advanced AI coursework.</p>
<h4 id="virtual-environments-overview">Virtual Environments Overview</h4>
<ol>
<li>
<p><strong>Unity 3D</strong></p>
<ul>
<li><strong>Website:</strong> <a href="https://unity.com/">Unity 3D</a></li>
<li><strong>Description:</strong> Unity 3D is renowned for creating realistic, detailed virtual environments. It's commonly employed for simulating autonomous driving scenarios.</li>
<li><strong>Reason for Exclusion:</strong> My proficiency lies primarily in Python, and I lack experience with Unity. This mismatch in skill set led to the decision against using Unity for this project.</li>
</ul>
</li>
<li>
<p><strong>AirSim</strong></p>
<ul>
<li><strong>Website:</strong> <a href="https://microsoft.github.io/AirSim/">AirSim</a></li>
<li><strong>Description:</strong> AirSim, developed on Unreal Engine, is a versatile simulator for drones, cars, and more. It offers realistic simulations for both physical and visual aspects.</li>
<li><strong>Reason for Exclusion:</strong> AirSim's primary focus on cars, as opposed to RF-cars, makes it less suitable for this specific project.</li>
</ul>
</li>
<li>
<p><strong>CARLA</strong></p>
<ul>
<li><strong>Website:</strong> <a href="https://carla.org/">CARLA</a></li>
<li><strong>Description:</strong> CARLA is a comprehensive platform for autonomous driving systems' development, training, and validation. It provides open-source code, protocols, and digital assets.</li>
<li><strong>Reason for Exclusion:</strong> CARLA's focus on autonomous driving systems, as opposed to RF-cars, makes it less suitable for this specific project.</li>
</ul>
</li>
<li>
<p><strong>OpenAI Gym</strong></p>
<ul>
<li><strong>Website:</strong> <a href="https://gym.openai.com/">OpenAI Gym</a></li>
<li><strong>Description:</strong> This toolkit is designed for the development and comparison of reinforcement learning algorithms. It's known for its ease of use in creating custom environments.</li>
<li><strong>Reason for Selection:</strong> Familiarity from Advanced AI coursework and the flexibility to create custom environments make OpenAI Gym an ideal choice. Its simplicity and focus on reinforcement learning align well with the project's objectives, particularly in SIM2REAL transfer.</li>
</ul>
</li>
<li>
<p><strong>ISAAC Gym</strong></p>
<ul>
<li><strong>Website:</strong> <a href="https://developer.nvidia.com/isaac-gym">ISAAC Gym</a></li>
<li><strong>Description:</strong> NVIDIA's Isaac Gym focuses on training robotics AI skills within a reinforcement learning framework. It offers accurate world and sensor models through the NVIDIA Omniverse.</li>
<li><strong>Reason for Exclusion:</strong> The project's focus on RF-cars, as opposed to robots, makes ISAAC Gym less suitable for this specific project. Additionally, the amount of time it would take to create a rf-car that is an exact copy the real rf-car I already had, would be too much.</li>
</ul>
</li>
</ol>
<h4 id="conclusion">Conclusion</h4>
<p>Considering the project's needs for a simple, lightweight environment conducive to SIM2REAL transfer and the existing familiarity with the tool, <strong>OpenAI Gym</strong> emerges as the optimal choice. Its ease of use for creating custom environments and focus on reinforcement learning agents provide the necessary foundation for this project.</p>
<h3 id="2-which-reinforcement-learning-techniques-can-i-best-use-in-this-application">2. Which reinforcement learning techniques can I best use in this application?</h3>
<h4 id="introduction">Introduction</h4>
<p>The efficacy of training a virtual Radio Frequency (RF) car in a simulation environment hinges significantly on the choice of reinforcement learning (RL) techniques. This document explores various RL methods, assessing their suitability for the nuanced requirements of RF-car training, with an emphasis on navigating complex environments.</p>
<h3 id="executive-summary">Executive Summary</h3>
<p>My tests concluded that the Double Deep Q-Network (DDQN) is the most suitable technique for my application. It didn't need many episodes to be able to solve the environment efficiently.</p>
<h4 id="reinforcement-learning-techniques-overview">Reinforcement Learning Techniques Overview</h4>
<ol>
<li>
<p><strong>Deep Q-Network (DQN)</strong></p>
<ul>
<li><strong>Description:</strong> DQN integrates a deep neural network with a Q-learning framework. It stands out for its proficiency in processing high-dimensional sensory inputs, making it a strong candidate for applications requiring detailed environmental interaction.</li>
<li><strong>Suitability:</strong> While DQN offers advanced learning capabilities, its tendency to overestimate Q-values in environments with intricate and unpredictable dynamics can be a limitation for training RF-cars.</li>
<li><strong>Visual Representation of my integration:</strong> <img src="./videos/DQN-agent.gif" alt="DQN"></li>
<li><strong>Reward History of the DQN:</strong> <img src="./images/reward_history_DQN.png" alt="DQN"></li>
</ul>
</li>
<li>
<p><strong>Double Deep Q-Network (DDQN)</strong></p>
<ul>
<li><strong>Description:</strong> DDQN, an evolution of DQN, employs dual neural networks. This structure effectively mitigates the overestimation bias present in DQN by decoupling action selection from Q-value generation.</li>
<li><strong>Reason for Selection:</strong> The choice of DDQN is driven by its enhanced accuracy in Q-value approximation, crucial for navigating the intricate maze-like environments encountered by virtual RF-cars. The RF-car's limited sensor range can lead to overestimations in Q-values, a challenge adeptly addressed by DDQN’s design. Through empirical trials, DDQN demonstrated superior performance, validating its selection for this application.</li>
<li><strong>Visual Representation of my integration:</strong> <img src="./videos/DDQN_withfailsave.gif" alt="DDQN"></li>
<li><strong>Reward History of the DDQN:</strong> <img src="./images/DDQPN results.png" alt="DDQN"></li>
</ul>
</li>
<li>
<p><strong>Proximal Policy Optimization (PPO)</strong></p>
<ul>
<li><strong>Description:</strong> PPO, a policy gradient method, optimizes decision-making policies directly. It is renowned for its stability and efficiency in environments where policy output is more consequential than value estimation.</li>
<li><strong>Suitability:</strong> PPO, despite its strengths in certain RL contexts, is less aligned with the specific needs of an RF-car simulation. The focus on policy over value estimation renders it suboptimal for environments where precise Q-value approximation is critical.</li>
<li><strong>Visual Representation of my integration:</strong> <img src="./videos/ppo.gif" alt="PPO"></li>
<li><strong>Reward History of the PPO:</strong> <img src="./images/PPO_reward_history.png" alt="PPO"></li>
</ul>
</li>
</ol>
<table>
<thead>
<tr>
<th>Reinforcement Learning Technique</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Deep Q-Network (DQN)</strong></td>
<td>- Proficient in processing high-dimensional sensory inputs.<br> - Effective in detailed environmental interaction.</td>
<td>- Tendency to overestimate Q-values in complex dynamics.<br> - Less suitable for unpredictable environments.</td>
</tr>
<tr>
<td><strong>Double Deep Q-Network (DDQN)</strong></td>
<td>- Addresses overestimation bias of DQN with dual neural networks.<br> - Enhanced accuracy in Q-value approximation.<br> - Demonstrated superior performance in preliminary trials.</td>
<td>- More complex architecture than DQN.<br> - Potentially requires more computational resources.</td>
</tr>
<tr>
<td><strong>Proximal Policy Optimization (PPO)</strong></td>
<td>- Directly optimizes decision-making policies.<br> - Known for stability and efficiency.</td>
<td>- Less focused on value estimation, which is crucial for RF-car training.<br> - Suboptimal for environments where precise Q-value approximation is critical.</td>
</tr>
</tbody>
</table>
<h4 id="conclusion">Conclusion</h4>
<p>The Double Deep Q-Network (DDQN) emerges as the most suitable technique for the virtual RF-car training. Its ability to accurately estimate Q-values in complex, sensor-driven environments, coupled with its proven performance in preliminary trials, underscores its effectiveness for this specific application.</p>
<p>On top of that the DDQN sovled the environment in 25 steps on average, against the DQN which solved the environment in 34 steps on average. Which is a significant improvement.</p>
<h3 id="3-can-the-simulation-be-transferred-to-the-real-world-explore-the-difference-between-how-the-car-moves-in-the-simulation-and-in-the-real-world">3. Can the simulation be transferred to the real world? Explore the difference between how the car moves in the simulation and in the real world</h3>
<h4 id="introduction">Introduction</h4>
<p>In addressing the critical task of transferring the simulation model to a real-world RF-car, this section explores the encountered discrepancies and the adjustments made to facilitate an effective sim-to-real transition.</p>
<h4 id="executive-summary">Executive Summary</h4>
<p>There needed to be some adjustments made to the simulation to account for how the car moves in the real world. Especially in the way the distance to the walls is calculated</p>
<h4 id="discrepancies-and-adjustments">Discrepancies and Adjustments</h4>
<ol>
<li>
<p><strong>Sensor Data Normalization</strong></p>
<ul>
<li><strong>Challenge:</strong> The primary challenge was reconciling the differences in how the distance to walls was determined in the simulation versus the real world.</li>
<li><strong>Solution:</strong> Adjustments were made to the normalization of distance values in the simulation. This ensured that the sensor readings in the simulation were more aligned with those in the real world, enhancing the accuracy of the model's predictions in physical testing.</li>
</ul>
</li>
<li>
<p><strong>Action Synchronization and Failsafe Mechanisms</strong></p>
<ul>
<li><strong>Challenge:</strong> The absence of a failsafe in the simulation led to instances where the real car would collide with walls, indicating a discrepancy in action execution.</li>
<li><strong>Solution:</strong> The introduction of a failsafe mechanism and retraining of the model, as well as consideration for a rotary encoder as an alternative solution, significantly mitigated this issue.</li>
</ul>
</li>
<li>
<p><strong>Signal Processing for RF-car Movements</strong></p>
<ul>
<li><strong>Challenge:</strong> In the real-world scenario, the RF signals controlling the car were sent too rapidly, causing movement issues not present in the simulation.</li>
<li><strong>Solution:</strong> By reducing the frequency of action commands sent to the car, we achieved better synchronization between the simulation and real-world actions. Alternative solutions like direct motor driver connection or a queued action execution system were also explored, but ultimately deemed less suitable given the resources available.</li>
</ul>
</li>
<li>
<p><strong>Physical Dynamics and Motor Control</strong></p>
<ul>
<li><strong>Challenge:</strong> The real car's turns and movements were affected by factors like weight and texture of the floor, which were not accounted for in the simulation.</li>
<li><strong>Solution:</strong> Adjusting the motor control timings, specifically the delay in turning the motors on and off, allowed for more accurate replication of the simulated movements in the real car. Additionally adjusting the car's center of gravity and using a flat undergroud texutre helped the car to move more like it is supposed to.</li>
</ul>
</li>
</ol>
<h4 id="conclusion">Conclusion</h4>
<p>The process of transferring the simulation model to a real-world RF-car highlighted several key discrepancies, primarily in sensor data interpretation, action synchronization, signal processing, and physical dynamics. Through targeted adjustments and solutions, these challenges were addressed, enabling a more accurate and reliable sim-to-real transfer. This experience underscores the importance of thorough testing and adaptation in bridging the gap between simulated environments and real-world applications.</p>
<h3 id="4-does-the-simulation-have-any-useful-contributions-in-terms-of-training-time-or-performance">4. Does the simulation have any useful contributions? In terms of training time or performance?</h3>
<h4 id="introduction">Introduction</h4>
<p>This section explores the significant advantages of using simulation for the training of a virtual RF-car, particularly focusing on aspects of training efficiency and performance.</p>
<h4 id="executive-summary">Executive Summary</h4>
<p>The simulation makes the training process more efficient and easier to manage. It also allows for the utilization of more powerful computing resources, greatly accelerating the training process.</p>
<h4 id="advantages-of-simulation-training">Advantages of Simulation Training</h4>
<ol>
<li>
<p><strong>Training Efficiency</strong></p>
<ul>
<li><strong>Challenge in Real-World Training:</strong> Real-world training requires programming the car for autonomous return to the start position after each episode, continuous supervision to prevent crashes and or manual intervention to reset the episode.</li>
<li><strong>Solution with Simulation:</strong> The simulation environment eliminates these needs, allowing for uninterrupted and automated training sessions. This results in a more streamlined and time-efficient training process.</li>
</ul>
</li>
<li>
<p><strong>Safety and Supervision</strong></p>
<ul>
<li><strong>Challenge in Real-World Training:</strong> Even with failsafe mechanisms, real-world training poses risks of crashes or other accidents, necessitating constant observation.</li>
<li><strong>Solution with Simulation:</strong> The virtual environment provides a risk-free platform for training, removing the need for constant supervision and manual intervention.</li>
</ul>
</li>
<li>
<p><strong>Computational Efficiency</strong></p>
<ul>
<li><strong>Challenge in Real-World Training:</strong> Training the model on the Raspberry Pi (RPI) in real life is significantly slower compared to using a more powerful computer.</li>
<li><strong>Solution with Simulation:</strong> Simulation allows for the utilization of more powerful computing resources, greatly accelerating the training process.</li>
</ul>
</li>
<li>
<p><strong>Model Performance and Practicality</strong></p>
<ul>
<li><strong>Observation:</strong> The model trained in real life exhibits similar performance to the one trained in the simulation.</li>
<li><strong>Conclusion:</strong> Despite the similarity in performance, the challenges and inefficiencies associated with real-life training make the simulation an evidently more practical and efficient method for model development.</li>
</ul>
</li>
</ol>
<h4 id="conclusion">Conclusion</h4>
<p>The simulation offers considerable advantages in training the virtual RF-car, particularly in terms of efficiency, safety, and computational power. These benefits make simulation a highly valuable tool, significantly simplifying the training process and reducing the time and resources required, as compared to real-world training. The simulation thus stands out as an indispensable component in the development and refinement of autonomous vehicle models.</p>
<h3 id="5-how-can-i-transfer-my-trained-model-to-my-physical-rc-car-sim2real-how-do-you-need-to-adjust-the-agent-environment-and-model-for-it-to-work-in-the-real-world">5. How can I transfer my trained model to my physical RC car? (sim2real) How do you need to adjust the agent, environment and model for it to work in the real world?</h3>
<!-- -- # TODO: -- -->
<h2 id="reflection">Reflection</h2>
<!-- --
  # TODO:
  • Wat zijn volgens hen de sterke en zwakke punten van het resultaat uit jouw researchproject?   
  • Is ‘het projectresultaat’ (incl. methodiek) bruikbaar in de bedrijfswereld?  
  • Welke alternatieven/suggesties geven bedrijven en/of community?   
  • Wat zijn de mogelijke implementatiehindernissen voor een bedrijf?    
  • Wat is de meerwaarde voor het bedrijf?   
  • Is er een maatschappelijke/economische/socio-economische meerwaarde aanwezig?  
-- -->
<p>Reflecting on the journey of this research, it's clear that the path from simulation to real-world application is fraught with unexpected challenges. The process has underscored the critical importance of adaptability, precision in sensor data interpretation, and the intricacies of physical implementation. Despite rigorous simulation and testing, the transition to the real world highlighted discrepancies that demanded innovative solutions. This experience has been invaluable, revealing the necessity of continuous experimentation and the potential for unforeseen obstacles in bridging the gap between theoretical models and their practical execution.</p>
<h2 id="advice-for-those-embarking-on-similar-research-paths">Advice for those Embarking on Similar Research Paths</h2>
<ul>
<li>Expect the Unexpected: Real-world conditions introduce variables that are often not accounted for in simulations. Be prepared to iterate and adapt your solutions.</li>
<li>Try not to be too attached to a specific solution:
<ul>
<li>The journey to a successful implementation often involves multiple iterations and pivots. Be open to new ideas and approaches, this is somewhere I went wrong, I switched to new ideas too late in the process and could not test or implement them properly.</li>
<li>Explore multiple virtual environments: I chose mine based on my knowledge of it, but it limited me while trying to implement the real-world application: The movement of the car was Difficult (near impossible) to replicate in the real world. Choosing a more sophisticated environment might have helped me to better prepare for the real-world application or allowed for a larger margin of error which openAI gym did not allow for (or my implementation of it did not allow for it).</li>
</ul>
</li>
<li>Precision Matters: Pay meticulous attention to the calibration and interpretation of sensor data. Discrepancies between simulated and real-world environments can significantly impact outcomes.</li>
<li>Embrace Interdisciplinary Collaboration: The complexity of Sim2Real transfers benefits greatly from the insights and expertise of those in fields such as robotics, mechanical engineering, and computer science.</li>
</ul>
<h2 id="general-conclusion">General Conclusion</h2>
<p>This research has made significant strides in understanding the feasibility and challenges of Sim2Real transfers in reinforcement learning. While substantial progress was achieved, the journey illuminated the vast landscape of challenges that lie in the nuanced discrepancies between virtual and physical realms. Future endeavors in this domain should continue to push the boundaries of what is possible, leveraging the lessons learned to further bridge the gap between simulation and reality. The potential applications of successfully transferring RL agents to the real world are vast, promising advancements in robotics, autonomous vehicles, and beyond.</p>
<h2 id="credits">Credits</h2>
<p>I am immensely grateful to my coach and supervisor, <a href="wouter.gevaert@howest.be">Gevaert Wouter</a>, for his guidance and clever insights that significantly shaped the course of this research project. In addition to his invaluable assistance during the project, I would also like to extend my thanks for the enjoyable courses he delivered during my time at Howest.</p>
<h2 id="sources-and-inspiration">Sources and Inspiration</h2>
<h3 id="websites">Websites</h3>
<ol>
<li>&quot;10. Migrating from RPi.GPIO - GPIO Zero 1.6.2 Documentation.&quot; Accessed Jan. 29, 2024. [Online]. Available: <a href="https://gpiozero.readthedocs.io/en/stable/migrating_from_rpigpio.html">https://gpiozero.readthedocs.io/en/stable/migrating_from_rpigpio.html</a></li>
<li>&quot;14. API - Input Devices - gpiozero 2.0 Documentation.&quot; Accessed Jan. 29, 2024. [Online]. Available: <a href="https://gpiozero.readthedocs.io/en/latest/api_input.html#distancesensor-hc-sr04">https://gpiozero.readthedocs.io/en/latest/api_input.html#distancesensor-hc-sr04</a></li>
<li>&quot;ailispaw/mjpg-streamer - Docker Image | Docker Hub.&quot; Accessed Jan. 29, 2024. [Online]. Available: <a href="https://hub.docker.com/r/ailispaw/mjpg-streamer">https://hub.docker.com/r/ailispaw/mjpg-streamer</a></li>
<li>&quot;Amazon.com: ESP-WROOM-32 ESP32 ESP-32S Development Board 2.4GHz Dual-Mode WiFi + Bluetooth Dual Cores Microcontroller Processor Integrated with Antenna RF AMP Filter AP STA Compatible with Arduino IDE (1 PCS): Electronics.&quot; Accessed Jan. 29, 2024. [Online]. Available: <a href="https://www.amazon.com/ESP-WROOM-32-Development-Dual-Mode-Microcontroller-Integrated/dp/B07WCG1PLV?th=1">https://www.amazon.com/ESP-WROOM-32-Development-Dual-Mode-Microcontroller-Integrated/dp/B07WCG1PLV?th=1</a></li>
<li>&quot;BadPinFactory error when using gpiozero Python library with RaspberryPi 4 - Project help.&quot; balenaForums. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://forums.balena.io/t/badpinfactory-error-when-using-gpiozero-python-library-with-raspberrypi-4/367262">https://forums.balena.io/t/badpinfactory-error-when-using-gpiozero-python-library-with-raspberrypi-4/367262</a></li>
<li>&quot;Box 18650 battery shield V8 by WerMRolenT.&quot; Thingiverse.com. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://www.thingiverse.com/thing:6331087">https://www.thingiverse.com/thing:6331087</a></li>
<li>&quot;Build a Raspberry Pi Webcam Server in Minutes.&quot; Pi My Life Up. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://pimylifeup.com/raspberry-pi-webcam-server/">https://pimylifeup.com/raspberry-pi-webcam-server/</a></li>
<li>&quot;Create a Docker Container for Raspberry Pi to Blink an LED.&quot; IoT Bytes. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://iotbytes.wordpress.com/create-your-first-docker-container-for-raspberry-pi-to-blink-an-led/">https://iotbytes.wordpress.com/create-your-first-docker-container-for-raspberry-pi-to-blink-an-led/</a></li>
<li>&quot;Holder for a HC SR04 Case by Legieps.&quot; Thingiverse.com. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://www.thingiverse.com/thing:3436448">https://www.thingiverse.com/thing:3436448</a></li>
<li>&quot;How to Make Raspberry Pi Webcam Server and Stream Live Video || Motion + Webcam + Raspberry Pi.&quot; Instructables. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://www.instructables.com/How-to-Make-Raspberry-Pi-Webcam-Server-and-Stream-/">https://www.instructables.com/How-to-Make-Raspberry-Pi-Webcam-Server-and-Stream-/</a></li>
<li>&quot;ikaritw/rpi-motion - Docker Image | Docker Hub.&quot; Accessed Jan. 29, 2024. [Online]. Available: <a href="https://hub.docker.com/r/ikaritw/rpi-motion">https://hub.docker.com/r/ikaritw/rpi-motion</a></li>
<li>&quot;Multi-purpose 2wd robot chassis by Mayur7600.&quot; Thingiverse.com. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://www.thingiverse.com/thing:2544002">https://www.thingiverse.com/thing:2544002</a></li>
<li>&quot;Online Multiplayer Game in Repl using Pygame and Flask.&quot; replit. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://replit.com/talk/ask/Online-Multiplayer-Game-in-Repl-using-Pygame-and-Flask/77222">https://replit.com/talk/ask/Online-Multiplayer-Game-in-Repl-using-Pygame-and-Flask/77222</a></li>
<li>&quot;Open Labyrinth mission. python coding challenges - Py.CheckiO.&quot; Py.CheckiO - games for coders. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://py.checkio.org/en/mission/open-labyrinth/share/574bd1ded68c9705c5d6f07c6206be12/">https://py.checkio.org/en/mission/open-labyrinth/share/574bd1ded68c9705c5d6f07c6206be12/</a></li>
<li>&quot;Set up Docker on the Raspberry Pi.&quot; Pi My Life Up. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://pimylifeup.com/raspberry-pi-docker/">https://pimylifeup.com/raspberry-pi-docker/</a></li>
</ol>
<h3 id="github-repositories">GitHub Repositories</h3>
<ol>
<li>&quot;Add info on Docker setup - Issue #891 - gpiozero/gpiozero.&quot; GitHub. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/gpiozero/gpiozero/issues/891">https://github.com/gpiozero/gpiozero/issues/891</a></li>
<li>A. Deka, &quot;Ankur-Deka/gym.&quot; Aug. 23, 2021. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/Ankur-Deka/gym">https://github.com/Ankur-Deka/gym</a></li>
<li>armlabs, &quot;armlabs/ssd1306_linux.&quot; Jan. 28, 2024. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/armlabs/ssd1306_linux">https://github.com/armlabs/ssd1306_linux</a></li>
<li>D. Li, &quot;DailyL/Sim2Real_autonomous_vehicle.&quot; Nov. 14, 2023. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/DailyL/Sim2Real_autonomous_vehicle">https://github.com/DailyL/Sim2Real_autonomous_vehicle</a></li>
<li>FinFET, &quot;FinFetChannel/RayCastingPythonMaze.&quot; Jan. 21, 2024. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/FinFetChannel/RayCastingPythonMaze">https://github.com/FinFetChannel/RayCastingPythonMaze</a></li>
<li>J. Brink, &quot;jamesbrink/docker-opengl.&quot; Jan. 23, 2024. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/jamesbrink/docker-opengl">https://github.com/jamesbrink/docker-opengl</a></li>
<li>K. Jarzębski, &quot;jarzebski/Arduino-MPU6050.&quot; Jan. 24, 2024. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/jarzebski/Arduino-MPU6050">https://github.com/jarzebski/Arduino-MPU6050</a></li>
<li>Aleksei, &quot;lexus2k/ssd1306.&quot; Jan. 28, 2024. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/lexus2k/ssd1306">https://github.com/lexus2k/ssd1306</a></li>
<li>&quot;NVIDIA-Omniverse/IsaacGymEnvs.&quot; NVIDIA Omniverse. Jan. 29, 2024. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/NVIDIA-Omniverse/IsaacGymEnvs">https://github.com/NVIDIA-Omniverse/IsaacGymEnvs</a></li>
<li>D. de Lorenzo, &quot;Sphinkie/ArrayQueue.&quot; Sep. 08, 2020. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/Sphinkie/ArrayQueue">https://github.com/Sphinkie/ArrayQueue</a></li>
<li>&quot;ssd1306/examples/demos/ssd1306_demo/ssd1306_demo.ino at master - lexus2k/ssd1306.&quot; GitHub. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/lexus2k/ssd1306/blob/master/examples/demos/ssd1306_demo/ssd1306_demo.ino">https://github.com/lexus2k/ssd1306/blob/master/examples/demos/ssd1306_demo/ssd1306_demo.ino</a></li>
<li>&quot;utensils/Envisaged.&quot; Utensils. Jan. 23, 2024. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://github.com/utensils/Envisaged">https://github.com/utensils/Envisaged</a></li>
</ol>
<h3 id="stack-overflow-threads">Stack Overflow Threads</h3>
<ol>
<li>therion, &quot;Accessing a Video Stream running on local HTTP host.&quot; Stack Overflow. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://stackoverflow.com/q/63993265">https://stackoverflow.com/q/63993265</a></li>
<li>vonGohren, &quot;Answer to 'Docker Access to Raspberry Pi GPIO Pins.'&quot; Stack Overflow. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://stackoverflow.com/a/30263573">https://stackoverflow.com/a/30263573</a></li>
<li>Priyanshu, &quot;Answer to 'OpenGl and GLFW in a docker container.'&quot; Stack Overflow. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://stackoverflow.com/a/77780991">https://stackoverflow.com/a/77780991</a></li>
<li>Victor, &quot;Answer to 'Why doesn’t Python app print anything when run in a detached docker container?'&quot; Stack Overflow. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://stackoverflow.com/a/31796350">https://stackoverflow.com/a/31796350</a></li>
<li>Nyxynyx, &quot;Docker Access to Raspberry Pi GPIO Pins.&quot; Stack Overflow. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://stackoverflow.com/q/30059784">https://stackoverflow.com/q/30059784</a></li>
<li>jpdus, &quot;Why doesn’t Python app print anything when run in a detached docker container?&quot; Stack Overflow. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://stackoverflow.com/q/29663459">https://stackoverflow.com/q/29663459</a></li>
</ol>
<h3 id="academic-articles">Academic Articles</h3>
<ol>
<li>Q. Song et al., &quot;Autonomous Driving Decision Control Based on Improved Proximal Policy Optimization Algorithm,&quot; <em>Applied Sciences</em>, vol. 13, no. 11, Art. no. 11, Jan. 2023. [Online]. Available: <a href="https://www.mdpi.com/2076-3417/13/11/6400">https://www.mdpi.com/2076-3417/13/11/6400</a></li>
<li>J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, &quot;D4RL: Datasets for Deep Data-Driven Reinforcement Learning.&quot; arXiv, Feb. 05, 2021. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://arxiv.org/pdf/2004.07219.pdf">https://arxiv.org/pdf/2004.07219.pdf</a></li>
<li>D. Backhouse, J. Gourlay, B. Guta, K. Huang, and K. Ng, &quot;Gym2Real: An Open-Source Platform for Sim2Real Transfer.&quot; [Online]. Available: <a href="https://bguta.github.io/assets/Gym2Real_Capstone_Project_Report.pdf">https://bguta.github.io/assets/Gym2Real_Capstone_Project_Report.pdf</a></li>
<li>S. Tjiharjadi, M. Wijaya, and E. Setiawan, &quot;Optimization Maze Robot Using A*and Flood Fill Algorithm,&quot;*International Journal of Mechanical Engineering and Robotics Research, vol. 6, pp. 366–372, Sep. 2017. [online]. Available: <a href="https://www.ijmerr.com/uploadfile/2017/0904/20170904105839434.pdf">https://www.ijmerr.com/uploadfile/2017/0904/20170904105839434.pdf</a></li>
<li>S. Ramstedt and C. Pal, &quot;Real-Time Reinforcement Learning,&quot; in <em>Advances in Neural Information Processing Systems</em>, Curran Associates, Inc., 2019. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://arxiv.org/pdf/2306.09010.pdf">https://arxiv.org/pdf/2306.09010.pdf</a></li>
</ol>
<h3 id="online-videos">Online Videos</h3>
<ol>
<li>&quot;Reinforcement Learning with Multi-Fidelity Simulators -- RC Car.&quot; Dec. 30, 2014. Accessed Jan. 29, 2024. [Online Video]. Available: <a href="https://www.youtube.com/watch?v=c_d0Is3bxXA">https://www.youtube.com/watch?v=c_d0Is3bxXA</a></li>
<li>&quot;Self Driving and Drifting RC Car using Reinforcement Learning.&quot; Aug. 19, 2019. Accessed Jan. 29, 2024. [Online Video]. Available: <a href="https://www.youtube.com/watch?v=U0-Jswwf0hw">https://www.youtube.com/watch?v=U0-Jswwf0hw</a></li>
</ol>
<h3 id="online-forum-discussions">Online Forum Discussions</h3>
<ol>
<li>brinkjames, &quot;OpenGL inside Docker containers, this is how I did it.&quot; r/docker. Accessed Jan. 29, 2024. [Online]. Available: &lt;www.reddit.com/r/docker/comments/8d3qox/opengl_inside_docker_containers_this_is_how_i_did/&gt;</li>
<li>the_codingbear, &quot;Tutorial how to create a OpenGL context in Docker.&quot; r/opengl. Accessed Jan. 29, 2024. [Online]. Available: &lt;www.reddit.com/r/opengl/comments/peojvo/tutorial_how_to_create_a_opengl_context_in_docker/&gt;</li>
</ol>
<h3 id="other-online-resources">Other Online Resources</h3>
<ol>
<li>&quot;Queue.&quot; Accessed Jan. 29, 2024. [Online]. Available: <a href="https://www.arduinolibraries.info/libraries/queue">https://www.arduinolibraries.info/libraries/queue</a></li>
</ol>
<h3 id="conference-papers">Conference Papers</h3>
<ol>
<li>W. Zhao, J. P. Queralta, and T. Westerlund, &quot;Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey,&quot; in <em>2020 IEEE Symposium Series on Computational Intelligence (SSCI)</em>, Dec. 2020, pp. 737–744. [online]. Available:
<a href="https://arxiv.org/pdf/2009.13303.pdf">https://arxiv.org/pdf/2009.13303.pdf</a></li>
</ol>
<h3 id="blogs-and-articles">Blogs and Articles</h3>
<ol>
<li>M. A. Dharmasiri, &quot;Micromouse from scratch | Algorithm- Maze traversal | Shortest path | Floodfill,&quot; Medium. Accessed Jan. 29, 2024. [Online]. Available: <a href="https://medium.com/@minikiraniamayadharmasiri/micromouse-from-scratch-algorithm-maze-traversal-shortest-path-floodfill-741242e8510">https://medium.com/@minikiraniamayadharmasiri/micromouse-from-scratch-algorithm-maze-traversal-shortest-path-floodfill-741242e8510</a></li>
</ol>

</body>
</html>
