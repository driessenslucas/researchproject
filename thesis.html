%%
% Copyright (c) 2017 - 2023, Pascal Wagler;
% Copyright (c) 2014 - 2023, John MacFarlane
%
% All rights reserved.
%
% Redistribution and use in source and binary forms, with or without
% modification, are permitted provided that the following conditions
% are met:
%
% - Redistributions of source code must retain the above copyright
% notice, this list of conditions and the following disclaimer.
%
% - Redistributions in binary form must reproduce the above copyright
% notice, this list of conditions and the following disclaimer in the
% documentation and/or other materials provided with the distribution.
%
% - Neither the name of John MacFarlane nor the names of other
% contributors may be used to endorse or promote products derived
% from this software without specific prior written permission.
%
% THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
% "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
% LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
% FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
% COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
% INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
% BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
% LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
% CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
% LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
% ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
% POSSIBILITY OF SUCH DAMAGE.
%%

%%
% This is the Eisvogel pandoc LaTeX template.
%
% For usage information and examples visit the official GitHub page:
% https://github.com/Wandmalfarbe/pandoc-latex-template
%%

% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names,table}{xcolor}
%
\documentclass[
  paper=letter,
  ,captions=tableheading
]{scrartcl}
\usepackage{amsmath,amssymb}
% Use setspace anyway because we change the default line spacing.
% The spacing is changed early to affect the titlepage and the TOC.
\usepackage{setspace}
\setstretch{1.2}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\definecolor{default-linkcolor}{HTML}{A50000}
\definecolor{default-filecolor}{HTML}{A50000}
\definecolor{default-citecolor}{HTML}{4077C0}
\definecolor{default-urlcolor}{HTML}{4077C0}
\usepackage[margin=1in]{geometry}
\usepackage[export]{adjustbox}
\usepackage{graphicx}
% add backlinks to footnote references, cf. https://tex.stackexchange.com/questions/302266/make-footnote-clickable-both-ways
\usepackage{footnotebackref}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{enumitem}
\setlistdepth{9}

\setlist[itemize,1]{label=$\bullet$}
\setlist[itemize,2]{label=$\bullet$}
\setlist[itemize,3]{label=$\bullet$}
\setlist[itemize,4]{label=$\bullet$}
\setlist[itemize,5]{label=$\bullet$}
\setlist[itemize,6]{label=$\bullet$}
\setlist[itemize,7]{label=$\bullet$}
\setlist[itemize,8]{label=$\bullet$}
\setlist[itemize,9]{label=$\bullet$}
\renewlist{itemize}{itemize}{9}

\setlist[enumerate,1]{label=$\arabic*.$}
\setlist[enumerate,2]{label=$\alph*.$}
\setlist[enumerate,3]{label=$\roman*.$}
\setlist[enumerate,4]{label=$\arabic*.$}
\setlist[enumerate,5]{label=$\alpha*$}
\setlist[enumerate,6]{label=$\roman*.$}
\setlist[enumerate,7]{label=$\arabic*.$}
\setlist[enumerate,8]{label=$\alph*.$}
\setlist[enumerate,9]{label=$\roman*.$}
\renewlist{enumerate}{enumerate}{9}


\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdfauthor={Lucas Driessens},
  pdfkeywords={Markdown, Example},
  hidelinks,
  breaklinks=true,
  pdfcreator={LaTeX via pandoc with the Eisvogel template}}
\title{Exploring the Feasibility of Sim2Real Transfer in Reinforcement
Learning}
\author{Lucas Driessens}
\date{June 1, 2024}



%%
%% added
%%


%
% for the background color of the title page
%
\usepackage{pagecolor}
\usepackage{afterpage}

%
% break urls
%
\PassOptionsToPackage{hyphens}{url}

%
% When using babel or polyglossia with biblatex, loading csquotes is recommended
% to ensure that quoted texts are typeset according to the rules of your main language.
%
\usepackage{csquotes}

%
% captions
%
\definecolor{caption-color}{HTML}{777777}
\usepackage[font={stretch=1.2}, textfont={color=caption-color}, position=top, skip=4mm, labelfont=bf, singlelinecheck=false, justification=justified]{caption}
\setcapindent{0em}

%
% blockquote
%
\definecolor{blockquote-border}{RGB}{221,221,221}
\definecolor{blockquote-text}{RGB}{119,119,119}
\usepackage{mdframed}
\newmdenv[rightline=false,bottomline=false,topline=false,linewidth=3pt,linecolor=blockquote-border,skipabove=\parskip]{customblockquote}
\renewenvironment{quote}{\begin{customblockquote}\list{}{\rightmargin=0em\leftmargin=0em}%
\item\relax\color{blockquote-text}\ignorespaces}{\unskip\unskip\endlist\end{customblockquote}}

%
% Source Sans Pro as the default font family
% Source Code Pro for monospace text
%
% 'default' option sets the default
% font family to Source Sans Pro, not \sfdefault.
%
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
    \usepackage[default]{sourcesanspro}
  \usepackage{sourcecodepro}
  \else % if not pdftex
    \usepackage[default]{sourcesanspro}
  \usepackage{sourcecodepro}

  % XeLaTeX specific adjustments for straight quotes: https://tex.stackexchange.com/a/354887
  % This issue is already fixed (see https://github.com/silkeh/latex-sourcecodepro/pull/5) but the
  % fix is still unreleased.
  % TODO: Remove this workaround when the new version of sourcecodepro is released on CTAN.
  \ifxetex
    \makeatletter
    \defaultfontfeatures[\ttfamily]
      { Numbers   = \sourcecodepro@figurestyle,
        Scale     = \SourceCodePro@scale,
        Extension = .otf }
    \setmonofont
      [ UprightFont    = *-\sourcecodepro@regstyle,
        ItalicFont     = *-\sourcecodepro@regstyle It,
        BoldFont       = *-\sourcecodepro@boldstyle,
        BoldItalicFont = *-\sourcecodepro@boldstyle It ]
      {SourceCodePro}
    \makeatother
  \fi
  \fi

%
% heading color
%
\definecolor{heading-color}{RGB}{40,40,40}
\addtokomafont{section}{\color{heading-color}}
% When using the classes report, scrreprt, book,
% scrbook or memoir, uncomment the following line.
%\addtokomafont{chapter}{\color{heading-color}}

%
% variables for title, author and date
%
\usepackage{titling}
\title{Exploring the Feasibility of Sim2Real Transfer in Reinforcement
Learning}
\author{Lucas Driessens}
\date{June 1, 2024}

%
% tables
%

%
% remove paragraph indentation
%
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines

%
%
% Listings
%
%


%
% header and footer
%
\usepackage[headsepline,footsepline]{scrlayer-scrpage}

\newpairofpagestyles{eisvogel-header-footer}{
  \clearpairofpagestyles
  \ihead*{%
    \includegraphics[height=0.5cm]{\headerlogo}%
    \hspace{0.5em}% Add some space between the icon and text
    \headertitle%
  }
  \chead*{}
  \ohead*{June 1, 2024}
  \ifoot*{Lucas Driessens}
  \cfoot*{}
  \ofoot*{\thepage}
  \addtokomafont{pageheadfoot}{\upshape}
}
\pagestyle{eisvogel-header-footer}



%%
%% end added
%%


\usepackage{changepage}
\usepackage{ragged2e}

\begin{document}

\newcommand{\headerlogo}{header_image.png}

\newcommand{\headertitle}{Bachelor Thesis Howest}

%%%%% title page %%%%%
\begin{titlepage}
    \newgeometry{margin=0cm}
    \begin{center}
        %%%%% Change the cover image here %%%%%
        \includegraphics[width=18.5cm,height=18.5cm]{./images/final_test/jp_final.jpeg}
    \end{center}
    \begin{adjustwidth}{1.5cm}{1.5cm}

    \vspace{0.5em}

    \MakeUppercase{\huge\textbf{Exploring the Feasibility of Sim2Real Transfer in Reinforcement Learning}}

    \MakeUppercase{\Large\textit{Application in Maze Navigation}}

    \vspace{1em}

    \MakeUppercase{Internal promotor: Gevaert Wouter}

    \MakeUppercase{External promotor: Sam Debeuf}

    \vspace{1em}

    \MakeUppercase{\small{Research conducted by}}

    \MakeUppercase{\Large\textbf{{Lucas Driessens}}}

    \MakeUppercase{\small{for obtaining a bachelor's degree in}}

    \MakeUppercase{\Large{\textbf{{Multimedia \& Creative Technologies}}}}

    \MakeUppercase{Howest | 2023-2024}
    \end{adjustwidth}
    \restoregeometry
\end{titlepage}

% \maketitle
\pagenumbering{Roman} % set the numbering style to lowercase letter

\begin{center}
 {\LARGE \textbf{\textsf{Abstract}}}
\end{center}

\begin{abstract}
\begin{justify}
<p>In this research project, I delve into the fascinating realm of
artificial intelligence, specifically focusing on reinforcement learning
(RL) and its application in real-world scenarios. The crux of my
investigation revolves around the challenging question: ‘Is it possible
to transfer a trained RL agent from a simulation to the real world?’
This inquiry is particularly examined in the context of maze
navigation.</p>
<p>This research is partitioned into sub-questions, which collectively
aim to create a comprehensive understanding of the process. Firstly, I
explore the various virtual environments available for training a
virtual RF-car, seeking the most effective platform for my purposes.
Secondly, I delve into identifying the most suitable reinforcement
learning techniques for this specific application, considering factors
like efficiency, adaptability, and real-world applicability. Lastly, the
research seeks to bridge the gap between simulation and reality,
investigating the practicality and challenges involved in this
transition.</p>
<p>Through this study, I aspire to contribute significantly to the field
of AI and robotics, offering insights and methodologies that could
potentially advance the implementation of RL in real-world applications.
The outcomes of this research could have far-reaching implications, not
only in robotics but also in areas where simulation-based training is
crucial.</p>
\end{justify}
\end{abstract}
\pagebreak


\begin{center}
 {\LARGE \textbf{\textsf{Preface}}}
\end{center}

\begin{abstract}
\begin{justify}
<p>This bachelor thesis, entitled “Exploring the Feasibility of Sim2Real
Transfer in Reinforcement Learning,” marks the culmination of my
academic journey in New Media &amp; Communication Technology at Howest,
University of Applied Sciences. It investigates a pivotal question in
the field of artificial intelligence: “Is it possible to transfer a
trained RL-agent from a simulation to the real world?” This question
emerges from my deep-seated curiosity about the intersection of virtual
simulations and tangible applications, reflecting a broader challenge in
AI to create adaptive, real-world systems from controlled
simulations.</p>
<p>My fascination with the concept of Sim2Real transfer originated
during the ‘Researchproject’ module. Witnessing the potential of
simulated environments to approximate complex real-world behaviors
sparked my interest in exploring their practical applicability. I have
structured this thesis to not only probe the theoretical foundations of
Sim2Real transfer but to also experimentally test its viability through
a series of innovative applications. These experiments aim to refine the
methods of Sim2Real transfer, enhancing their effectiveness and
reliability.</p>
<p>The research methodology adopted for this thesis involves a
combination of qualitative analyses and quantitative experiments.
Theoretical studies provide a comprehensive background, setting the
stage for empirical tests conducted in controlled environments. By
systematically assessing the performance of RL agents in various
scenarios, the research seeks to identify optimal strategies for
effective Sim2Real transitions.</p>
<p>I extend heartfelt thanks to my coach and supervisor, Gevaert Wouter,
for his invaluable guidance and insights throughout this research. His
expertise and mentorship have been fundamental to my scholarly and
personal growth. Gratitude is also due to Amaury Van Naemen for his
technical support with 3D printing components crucial for my
experiments. His assistance was pivotal in navigating the practical
challenges of my research.</p>
<p>Additional appreciation goes to the faculty and staff at Howest,
whose commitment to fostering an innovative educational environment has
profoundly influenced my development. Their support has been
instrumental in my pursuit of technology and innovation.</p>
<p>Lucas Driessens -06-2024</p>
\end{justify}
\end{abstract}
\pagebreak

\pagenumbering{arabic} % set the numbering style to lowercase letter
\setcounter{page}{0} % Set the page counter to 3


\renewcommand*\contentsname{Table of Contents}
{
\setcounter{tocdepth}{}
\tableofcontents
\newpage
}
\listoffigures
\newpage
<!-- pandoc thesis_new.md --o thesis_new.pdf -H deeplist.tex -f markdown-implicit_figures  --template template.tex --lua-filter pagebreak.lua -->
<!-- pandoc --from markdown --to html5 --standalone --toc --number-sections --citeproc --wrap=preserve --highlight-style=kate --mathml -->
<!-- ## Abstract

In this research project, I delve into the fascinating realm of artificial intelligence, specifically focusing on reinforcement learning (RL) and its application in real-world scenarios. The crux of my investigation revolves around the challenging question: "Is it possible to transfer a trained RL agent from a simulation to the real world?" This inquiry is particularly examined in the context of maze navigation.

This research is partitioned into sub-questions, which collectively aim to create a comprehensive understanding of the process. Firstly, I explore the various virtual environments available for training a virtual RF-car, seeking the most effective platform for my purposes. Secondly, I delve into identifying the most suitable reinforcement learning techniques for this specific application, considering factors like efficiency, adaptability, and real-world applicability. Lastly, the research seeks to bridge the gap between simulation and reality, investigating the practicality and challenges involved in this transition.

Through this study, I aspire to contribute significantly to the field of AI and robotics, offering insights and methodologies that could potentially advance the implementation of RL in real-world applications. The outcomes of this research could have far-reaching implications, not only in robotics but also in areas where simulation-based training is crucial. -->
<h2 id="glossary-of-terms">Glossary of Terms</h2>
<ol type="1">
<li><p><strong>Artificial Intelligence (AI)</strong>: The simulation of
human intelligence processes by machines, especially computer systems,
enabling them to perform tasks that typically require human
intelligence.</p></li>
<li><p><strong>Double Deep Q-Network (DDQN)</strong>: An enhancement of
the Deep Q-Network (DQN) algorithm that addresses the overestimation of
action values, thus improving learning stability and
performance.</p></li>
<li><p><strong>Epsilon Decay</strong>: A technique in reinforcement
learning that gradually decreases the rate of exploration over time,
allowing the agent to transition from exploring the environment to
exploiting known actions for better outcomes.</p></li>
<li><p><strong>Mean Squared Error (MSE)</strong>: A loss function used
in regression models to measure the average squared difference between
the estimated values and the actual value, useful for training models by
minimizing error.</p></li>
<li><p><strong>Motion Processing Unit (MPU6050)</strong>: A sensor
device combining a MEMS (Micro-Electro-Mechanical Systems) gyroscope and
a MEMS accelerometer, providing comprehensive motion processing
capabilities.</p></li>
<li><p><strong>Policy Network</strong>: In reinforcement learning, a
neural network model that directly maps observed environment states to
actions, guiding the agent’s decisions based on the current
policy.</p></li>
<li><p><strong>Raspberry Pi (RPI)</strong>: A small, affordable computer
used for various programming projects, including robotics and
educational applications.</p></li>
<li><p><strong>RC Car</strong>: A remote-controlled car used as a
practical application platform in reinforcement learning experiments,
demonstrating how algorithms can control real-world vehicles.</p></li>
<li><p><strong>Reinforcement Learning (RL)</strong>: A subset of machine
learning where an agent learns to make decisions by taking actions
within an environment to achieve specified goals, guided by a system of
rewards and penalties.</p></li>
<li><p><strong>Sim2Real Transfer</strong>: The practice of applying
models and strategies developed within a simulated environment to
real-world situations, crucial for bridging the gap between theoretical
research and practical application.</p></li>
<li><p><strong>Target Network</strong>: Utilized in the DDQN framework,
a neural network that helps stabilize training by providing consistent
targets for the duration of the update interval.</p></li>
<li><p><strong>Virtual Environment</strong>: A simulated setting
designed for training reinforcement learning agents, offering a
controlled, risk-free platform for experimentation and
learning.</p></li>
</ol>
<h2 id="list-of-abbreviations">List of Abbreviations</h2>
<ol type="1">
<li><strong>AI</strong> - Artificial Intelligence</li>
<li><strong>DDQN</strong> - Double Deep Q-Network</li>
<li><strong>DQN</strong> - Deep Q-Network</li>
<li><strong>ESP32</strong> - Espressif Systems 32-bit
Microcontroller</li>
<li><strong>HC-SR04</strong> - Ultrasonic Distance Sensor</li>
<li><strong>MSE</strong> - Mean Squared Error</li>
<li><strong>MPU6050</strong> - Motion Processing Unit (Gyroscope +
Accelerometer)</li>
<li><strong>PPO</strong> - Proximal Policy Optimization</li>
<li><strong>RC</strong> - Remote Controlled</li>
<li><strong>RPI</strong> - Raspberry Pi</li>
<li><strong>RL</strong> - Reinforcement Learning</li>
<li><strong>RCMazeEnv</strong> - RC Maze Environment (Custom Virtual
Environment for RL Training)</li>
<li><strong>Sim2Real</strong> - Simulation to Reality Transfer</li>
</ol>
<!-- ## Table of contents

- [Glossary of Terms](#glossary-of-terms)
- [List of Abbreviations](#list-of-abbreviations)
- [Table of contents](#table-of-contents)
- [Introduction](#introduction)
  - [Background on Reinforcement Learning](#background-on-reinforcement-learning)
- [Research Questions](#research-questions)
  - [Main Research Question](#main-research-question)
  - [Sub Research Questions](#sub-research-questions)
- [Methodology](#methodology)
  - [Environment Setup (RCMazeEnv)](#environment-setup-rcmazeenv)
  - [Agent Design (DDQNAgent)](#agent-design-ddqnagent)
  - [Training Process](#training-process)
  - [Reward Function Components](#reward-function-components)
    - [Collision Penalty $R\_{\\text{collision}}$](#collision-penalty-r_textcollision)
    - [Goal Achievement Bonus $R\_{\\text{goal}}$](#goal-achievement-bonus-r_textgoal)
    - [Proximity Reward $R\_{\\text{proximity}}$](#proximity-reward-r_textproximity)
    - [Progress Reward $R\_{\\text{progress}}$](#progress-reward-r_textprogress)
    - [Exploration Penalty $R\_{\\text{revisit}}$](#exploration-penalty-r_textrevisit)
    - [Efficiency Penalty $R\_{\\text{efficiency}}$](#efficiency-penalty-r_textefficiency)
    - [Generic Reward based on relative distance to goal](#generic-reward-based-on-relative-distance-to-goal)
  - [Scope of Real-World Testing](#scope-of-real-world-testing)
- [Answers to Research Questions](#answers-to-research-questions)
  - [1. Virtual Environments for RF-Car Training](#1-virtual-environments-for-rf-car-training)
  - [2. Reinforcement Learning Techniques for Virtual RF-Car Training](#2-reinforcement-learning-techniques-for-virtual-rf-car-training)
  - [3. Sim-to-Real Transfer Challenges and Solutions](#3-sim-to-real-transfer-challenges-and-solutions)
  - [4. Contributions of Simulation in RF-Car Training](#4-contributions-of-simulation-in-rf-car-training)
  - [5. Practical Application of Simulated Training to Real-World RF-Cars](#5-practical-application-of-simulated-training-to-real-world-rf-cars)
- [Experimental Outcomes and Implementation Details](#experimental-outcomes-and-implementation-details)
  - [Virtual Environment and Agent Design](#virtual-environment-and-agent-design)
  - [Implementation Highlights](#implementation-highlights)
  - [Evaluation and Metrics](#evaluation-and-metrics)
  - [Unique Features](#unique-features)
- [Model Architecture and Training Insights](#model-architecture-and-training-insights)
  - [Training Parameters](#training-parameters)
  - [Training Procedure](#training-procedure)
- [Visual Insights and Further Exploration](#visual-insights-and-further-exploration)
  - [Evaluation Metrics Overview](#evaluation-metrics-overview)
    - [Simulation Metrics](#simulation-metrics)
      - [1. Episodic Performance](#1-episodic-performance)
      - [2. Step Efficiency](#2-step-efficiency)
      - [3. MSE Loss Measurement](#3-mse-loss-measurement)
      - [4. Reward Trend Analysis](#4-reward-trend-analysis)
      - [5. Epsilon Decay Tracking](#5-epsilon-decay-tracking)
    - [Real-World Metrics](#real-world-metrics)
- [results](#results)
  - [Reinforcement Learning Techniques Overview](#reinforcement-learning-techniques-overview)
    - [final choice: DDQN](#final-choice-ddqn)
      - [1. **Visit Heatmap for DDQN:**](#1-visit-heatmap-for-ddqn)
      - [2. **Reward History for DDQN:**](#2-reward-history-for-ddqn)
      - [3. **Reward Distribution for DDQN:**](#3-reward-distribution-for-ddqn)
      - [4. **Maze Solution for DDQN:**](#4-maze-solution-for-ddqn)
      - [5. **Average Steps per Episode with Moving Average for DDQN:**](#5-average-steps-per-episode-with-moving-average-for-ddqn)
      - [6. **Epsilon History for DDQN:**](#6-epsilon-history-for-ddqn)
      - [7. **Mean Squared Error over time (Sampled) for DDQN:**](#7-mean-squared-error-over-time-sampled-for-ddqn)
    - [1. Deep Q-Network (DQN)](#1-deep-q-network-dqn)
    - [2. Double Deep Q-Network (DDQN)](#2-double-deep-q-network-ddqn)
    - [3. Proximal Policy Optimization (PPO)](#3-proximal-policy-optimization-ppo)
- [Hardware Setup and Assembly](#hardware-setup-and-assembly)
  - [Introduction to Hardware Components](#introduction-to-hardware-components)
  - [Components List](#components-list)
  - [Wiring Guide](#wiring-guide)
    - [esp32 pins](#esp32-pins)
- [Challenges and Solutions in Implementing RL Techniques and Virtual Environments](#challenges-and-solutions-in-implementing-rl-techniques-and-virtual-environments)
  - [Challenge 1: Selection of an Appropriate Virtual Environment](#challenge-1-selection-of-an-appropriate-virtual-environment)
  - [Challenge 2: Choosing the Optimal Reinforcement Learning Technique](#challenge-2-choosing-the-optimal-reinforcement-learning-technique)
  - [Challenge 3: Sim2Real Transfer - Addressing Movement Discrepancies](#challenge-3-sim2real-transfer---addressing-movement-discrepancies)
  - [Challenge 4: alignment Issue and Motor Encoder Implementation](#challenge-4-alignment-issue-and-motor-encoder-implementation)
  - [Challenge 5: Ensuring Consistent and Effective Training](#challenge-5-ensuring-consistent-and-effective-training)
  - [Challenge 6: Accurate Sensor Data Normalization for Sim2Real Transfer](#challenge-6-accurate-sensor-data-normalization-for-sim2real-transfer)
  - [Challenge 7: Integration of Failsafe Mechanisms](#challenge-7-integration-of-failsafe-mechanisms)
  - [Challenge 8: Training Environment and Technique Efficacy](#challenge-8-training-environment-and-technique-efficacy)
  - [Viewing Practical Experiments](#viewing-practical-experiments)
  - [Conclusion](#conclusion)
- [Real-World Application and Limitations](#real-world-application-and-limitations)
  - [Introduction to Sensor and Movement Discrepancies](#introduction-to-sensor-and-movement-discrepancies)
  - [Real-World Application](#real-world-application)
    - [Enhanced Sensor-Based Navigation](#enhanced-sensor-based-navigation)
    - [Informing Autonomous Vehicle Movement](#informing-autonomous-vehicle-movement)
  - [Limitations](#limitations)
    - [Discrepancies in Sensor Data Interpretation](#discrepancies-in-sensor-data-interpretation)
    - [Challenges in Movement Replication](#challenges-in-movement-replication)
    - [Practical Implementation Considerations](#practical-implementation-considerations)
  - [Conclusion](#conclusion-1)
- [Reflection](#reflection)
  - [Strengths and Weaknesses](#strengths-and-weaknesses)
  - [Practical Applicability and Industry Relevance](#practical-applicability-and-industry-relevance)
  - [Encountered Alternatives and Flexibility](#encountered-alternatives-and-flexibility)
  - [Anticipated Implementation Barriers](#anticipated-implementation-barriers)
  - [Ethical Considerations](#ethical-considerations)
  - [Societal Impact](#societal-impact)
  - [Policy and Regulation](#policy-and-regulation)
  - [Lessons Learned and Forward Path](#lessons-learned-and-forward-path)
- [Advice for those Embarking on Similar Research Paths](#advice-for-those-embarking-on-similar-research-paths)
- [General Conclusion](#general-conclusion)
- [Sources of Inspiration and Conceptual Framework](#sources-of-inspiration-and-conceptual-framework)
  - [Micro mouse Competitions and Reinforcement Learning](#micro-mouse-competitions-and-reinforcement-learning)
  - [Influential YouTube Demonstrations and GitHub Insights](#influential-youtube-demonstrations-and-github-insights)
  - [Technical Exploration and Academic Foundation](#technical-exploration-and-academic-foundation)
  - [Synthesis and Research Direction](#synthesis-and-research-direction)
- [Integration of Practical Experiments](#integration-of-practical-experiments)
  - [Addressing Alignment and Orientation Challenges](#addressing-alignment-and-orientation-challenges)
  - [Enhancing Movement Precision with Encoders](#enhancing-movement-precision-with-encoders)
  - [Real-World Application Tests](#real-world-application-tests)
- [Implementation of Real-World Control Algorithms](#implementation-of-real-world-control-algorithms)
  - [Introduction](#introduction-1)
  - [System Overview](#system-overview)
  - [Code Architecture and Integration](#code-architecture-and-integration)
  - [Practical Challenges in Sim2Real Transfer](#practical-challenges-in-sim2real-transfer)
  - [Testing and Validation](#testing-and-validation)
  - [Conclusion](#conclusion-2)
- [Guest Speakers](#guest-speakers)
  - [Innovations and Best Practices in AI Projects by Jeroen Boeye at Faktion](#innovations-and-best-practices-in-ai-projects-by-jeroen-boeye-at-faktion)
  - [Pioneering AI Solutions at Noest by Toon Vanhoutte](#pioneering-ai-solutions-at-noest-by-toon-vanhoutte)
- [Installation Steps](#installation-steps)
  - [Prerequisites](#prerequisites)
  - [Repository Setup](#repository-setup)
  - [ESP32 Setup](#esp32-setup)
    - [Hardware Installation](#hardware-installation)
    - [Software Configuration](#software-configuration)
  - [Web Application Setup](#web-application-setup)
    - [Note:](#note)
    - [Steps:](#steps)
  - [Usage Instructions](#usage-instructions)
  - [Additional Information: Model Training](#additional-information-model-training)
- [References](#references) -->
<div style="page-break-after: always;"></div>
<h2 id="introduction">Introduction</h2>
<p>In the evolving landscape of artificial intelligence and robotics,
the distinction between virtual simulations and real-world applications
increasingly narrows, presenting unprecedented opportunities and
challenges. This thesis explores the potential of Reinforcement Learning
(RL) to bridge this gap, with a specific focus on the domain of
autonomous navigation using a remote-controlled (RC) car in a maze. The
endeavor to transfer a trained RL agent from a simulated environment to
the real world encapsulates the core challenge of sim-to-real
transferability, a pivotal step towards realizing the full spectrum of
RL’s applicability in complex, real-world scenarios.</p>
<p>The purpose of this study is to explore the feasibility and
challenges of transferring a trained RL agent from a simulated
environment to the real world. This transition, known as “sim2real,” is
particularly examined in the context of maze navigation using a
remote-controlled (RC) car. The significance of this research lies in
its potential to bridge the gap between theoretical RL models and
practical, real-world applications, which is a critical step in
advancing the field of AI and robotics.</p>
<h3 id="background-on-reinforcement-learning">Background on
Reinforcement Learning</h3>
<p>Reinforcement Learning (RL) employs a computational approach where
agents learn to optimize their action sequences through trials and
errors, engaging with their environment to maximize accumulated rewards
over time. This learning framework is built upon the foundation of
Markov Decision Processes (MDP), which includes:</p>
<ul>
<li><span class="math inline"><em>S</em></span>: a definitive set of
environmental states,</li>
<li><span class="math inline"><em>A</em></span>: a comprehensive set of
possible actions for the agent,</li>
<li><span
class="math inline"><em>P</em>(<em>s</em><sub><em>t</em> + 1</sub>|<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>)</span>:
the transition probability that signifies the chance of moving from
state <span class="math inline"><em>s</em><sub><em>t</em></sub></span>
to state <span
class="math inline"><em>s</em><sub><em>t</em> + 1</sub></span> after the
agent takes action <span
class="math inline"><em>a</em><sub><em>t</em></sub></span> at a given
time <span class="math inline"><em>t</em></span>,</li>
<li><span
class="math inline"><em>R</em>(<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>)</span>:
the reward received following the action <span
class="math inline"><em>a</em><sub><em>t</em></sub></span> from state
<span class="math inline"><em>s</em><sub><em>t</em></sub></span> to
state <span
class="math inline"><em>s</em><sub><em>t</em> + 1</sub></span>.</li>
</ul>
<p>The principles of Reinforcement Learning, particularly the dynamics
of Markov Decision Processes involving states <span
class="math inline"><em>S</em></span>, actions <span
class="math inline"><em>A</em></span>, transition probabilities <span
class="math inline"><em>P</em>(<em>s</em><sub><em>t</em> + 1</sub>|<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>)</span>,
and rewards <span
class="math inline"><em>R</em>(<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>)</span>,
form the foundation of how agents learn from and interact with their
environment to optimize decision-making over time. This understanding is
crucial in the development of autonomous vehicles, improving
navigational strategies, decision-making capabilities, and adaptation to
real-time environmental changes. The seminal work by R.S. Sutton and
A.G. Barto significantly elucidates these principles and complexities of
RL algorithms .</p>
<h2 id="research-questions">Research Questions</h2>
<p>This investigation is anchored by the question: “Can a trained RL
agent be effectively transferred from a simulation to a real-world
environment for maze navigation?” Addressing this question involves
exploring multiple facets of RL training and implementation:</p>
<ol type="1">
<li>Selection of virtual environments for effective RL training.</li>
<li>Identification of RL techniques suited for autonomous
navigation.</li>
<li>Evaluation of sim-to-real transfer in adapting to real-world
dynamics.</li>
<li>Assessment of training efficacy and performance optimization through
simulation.</li>
<li>Adaptation and transfer of a trained model to a real RC car,
including necessary adjustments for real-world application.</li>
</ol>
<p>A combination of qualitative and quantitative research methodologies
underpins this study, encompassing simulation experiments, real-world
trials, and an extensive review of existing literature. This
multifaceted strategy not only seeks to corroborate the effectiveness of
transferring simulations to real-world applications but also endeavors
to enrich the ongoing conversation regarding the practical
implementation and obstacles associated with Reinforcement Learning
(RL).</p>
<h3 id="main-research-question">Main Research Question</h3>
<p><strong>Is it possible to transfer a trained RL-agent from a
simulation to the real world? (case: maze)</strong></p>
<h3 id="sub-research-questions">Sub Research Questions</h3>
<ol type="1">
<li><p>Which virtual environments exist to train a virtual
RC-car?</p></li>
<li><p>Which reinforcement learning techniques can I best use in this
application?</p></li>
<li><p>Can the simulation be transferred to the real world? Explore the
difference between how the car moves in the simulation and in the real
world.</p></li>
<li><p>Does the simulation have any useful contributions? In terms of
training time or performance?</p></li>
<li><p>How can the trained model be transferred to the real RC car?
(sim2real) How do you need to adjust the agent and the environment for
it to translate to the real world?</p></li>
</ol>
<div style="page-break-after: always;"></div>
<h2 id="methodology">Methodology</h2>
<p>This section explores the Reinforcement Learning Maze Navigation
(RCMazeEnv) method, utilizing a Double Deep Q-Network (DDQNAgent)
architecture. It details the maze environment setup, the DDQN agent
design, and the comprehensive training algorithm, incorporating
mathematical functions to delineate the system’s mechanics.</p>
<h3 id="environment-setup-rcmazeenv">Environment Setup (RCMazeEnv)</h3>
<p>The RCMazeEnv, a custom maze navigation environment derived from the
OpenAI Gym framework, is designed for a 12x12 cell grid maze navigation
task. Each cell within this grid can be identified as either a wall,
represented by ‘1’, or a path, represented by ‘0’, with the goal
designated at cell position (10, 10). The agent, visualized as a car,
commences its journey from the starting position at cell (1, 1), facing
eastward initially. The agent’s navigation capabilities are enabled
through a set of possible actions: moving forward, turning left, and
turning right.</p>
<p>To assist in navigation, the agent is equipped with sensors that
provide readings in three directions: front, left, and right. These
sensors measure the distance to the nearest wall in their respective
directions, offering crucial environmental information that aids in
decision-making. The environment’s state space, denoted as <span
class="math inline">𝒮</span>, encapsulates the agent’s current position
<span class="math inline">(<em>x</em>, <em>y</em>)</span>, its
orientation <span class="math inline"><em>θ</em></span>, which can be
one of <span
class="math inline">{<em>N</em>, <em>E</em>, <em>S</em>, <em>W</em>}</span>
representing north, east, south, and west respectively, and the sensor
readings <span
class="math inline">{<em>s</em><sub>front</sub>, <em>s</em><sub>left</sub>, <em>s</em><sub>right</sub>}</span>.
The goal of the agent is to navigate through the maze, from its starting
point to the goal location, efficiently while avoiding collisions with
walls and optimizing the path taken based on the sensor inputs and past
experiences.</p>
<h3 id="agent-design-ddqnagent">Agent Design (DDQNAgent)</h3>
<p>The agent employs a Double Deep Q-Network (DDQN) architecture to
learn the optimal policy <span
class="math inline"><em>π</em><sup>*</sup></span>. This is an
enhancement over the standard DQN that aims to reduce overestimation of
Q-values by decoupling the action selection from its evaluation .</p>
<ul>
<li><strong>Policy Network:</strong> Estimates the Q-value <span
class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>; <em>θ</em>)</span>
for taking action <span class="math inline"><em>a</em></span> in state
<span class="math inline"><em>s</em></span>, parameterized by weights
<span class="math inline"><em>θ</em></span>. This network is responsible
for selecting actions based on the current policy.</li>
<li><strong>Target Network:</strong> Independently parameterized by
weights <span class="math inline"><em>θ</em><sup>−</sup></span>, used to
estimate the target Q-value for updating the policy network. It mirrors
the architecture of the policy network but is updated less frequently to
provide stable target values.</li>
</ul>
<p>The Q-function update equation in DDQN is modified to:</p>
<p><span class="math display">$$
Y_t^{DDQN} = R_{t+1} + \gamma Q\left(S_{t+1},
\underset{a}{\mathrm{argmax}}\, Q(S_{t+1}, a; \theta); \theta^-\right)
$$</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline"><em>R</em><sub><em>t</em> + 1</sub></span>
is the reward received after taking action <span
class="math inline"><em>a</em></span> in state <span
class="math inline"><em>s</em></span>.</li>
<li><span class="math inline"><em>γ</em></span> is the discount
factor.</li>
<li><span class="math inline">$\underset{a}{\mathrm{argmax}}\,
Q(S_{t+1}, a; \theta)$</span> selects the action using the policy
network.</li>
<li><span
class="math inline"><em>Q</em>(<em>S</em><sub><em>t</em> + 1</sub>, <em>a</em>; <em>θ</em><sup>−</sup>)</span>
evaluates the action using the target network.</li>
</ul>
<p>This approach is grounded in the principle of reducing overestimation
by decomposing the max operation in the target into action selection and
action evaluation, thereby mitigating the overoptimism often observed in
Q-learning .</p>
<p>The action space <span class="math inline">𝒜</span> and the rest of
the agent’s setup remain as previously described. The DDQN architecture
significantly improves the stability and performance of the agent by
addressing the overestimation of Q-values, promoting a more accurate and
reliable learning process. However, it’s important to note that the
effectiveness of DDQN can vary depending on the specific task, and it
may not always outperform traditional DQN approaches .</p>
<h3 id="training-process">Training Process</h3>
<p>The training process utilizes the experience replay mechanism,
storing transitions <span
class="math inline">(<em>s</em>, <em>a</em>, <em>r</em>, <em>s</em>′)</span>
in a replay buffer <span class="math inline"><em>D</em></span>. The DQN
is trained by minimizing the loss function <span
class="math inline"><em>L</em>(<em>θ</em>)</span> defined as the mean
squared error between the current Q-values and the target Q-values:</p>
<p><span
class="math display"><em>L</em>(<em>θ</em>) = 𝔼<sub>(<em>s</em>, <em>a</em>, <em>r</em>, <em>s</em>′) ∼ <em>U</em>(<em>D</em>)</sub>[(<em>r</em> + <em>γ</em>max<sub><em>a</em>′</sub><em>Q</em>(<em>s</em>′, <em>a</em>′; <em>θ</em><sup>−</sup>) − <em>Q</em>(<em>s</em>, <em>a</em>; <em>θ</em>))<sup>2</sup>]</span></p>
<p>where <span class="math inline"><em>θ</em><sup>−</sup></span>
represents the weights of a target network, and <span
class="math inline"><em>γ</em></span> is the discount factor. The target
network’s weights are periodically updated to match the policy network,
stabilizing training.</p>
<p>The epsilon-greedy strategy is employed for action selection, with
<span class="math inline"><em>ϵ</em></span> gradually decaying from 1 to
a minimum value, balancing exploration and exploitation.</p>
<h3 id="reward-function-components">Reward Function Components</h3>
<h4 id="collision-penalty-r_textcollision">Collision Penalty <span
class="math inline"><em>R</em><sub>collision</sub></span></h4>
<p>When the agent attempts to move into a wall or outside the designated
maze boundaries, it triggers a collision state. To discourage such
actions, which are counterproductive to the goal of reaching the
destination, a significant penalty is applied. This penalty is critical
for teaching the agent about the boundaries and obstacles within the
environment, ensuring that it learns to navigate safely and
effectively.</p>
<p><span class="math display"><em>R</em>_collision = −20</span></p>
<h4 id="goal-achievement-bonus-r_textgoal">Goal Achievement Bonus <span
class="math inline"><em>R</em><sub>goal</sub></span></h4>
<p>Reaching the goal is the primary objective of the maze navigation
task. A substantial reward is given to the agent upon achieving this
objective, signifying the completion of the episode. This reward serves
as a strong positive reinforcement, guiding the agent’s learning towards
the goal-oriented behavior. However, an additional mechanism penalizes
the agent if it takes an excessively long route to reach the goal,
promoting efficiency in navigation.</p>
<p><span class="math display">$$ R\_{\text{goal}} = \begin{cases} +500,
&amp; \text{if goal is reached} \\ -200, &amp; \text{if steps} &gt; 1000
\end{cases} $$</span></p>
<h4 id="proximity-reward-r_textproximity">Proximity Reward <span
class="math inline"><em>R</em><sub>proximity</sub></span></h4>
<p>This component of the reward function incentivizes the agent to
minimize its distance to the goal over time. By rewarding the agent
based on its proximity to the goal, it encourages exploration and path
optimization, guiding the agent to navigate the maze more effectively.
The reward decreases as the distance to the goal increases, encouraging
the agent to always move towards the goal.</p>
<p><span class="math display">$$ R*{\text{proximity}} =
\frac{50}{d*{\text{goal}} + 1} $$</span></p>
<h4 id="progress-reward-r_textprogress">Progress Reward <span
class="math inline"><em>R</em><sub>progress</sub></span></h4>
<p>The progress reward or penalty is designed to encourage the agent to
make decisions that bring it closer to the goal and to penalize
decisions that lead it away. This dynamic reward system provides
immediate feedback based on the agent’s movement relative to the goal,
promoting smarter navigation decisions.</p>
<p><span class="math display">$$ R\_{\text{progress}} = \begin{cases}
+50, &amp; \text{if distance decreases} \\ -25, &amp; \text{if distance
increases} \end{cases} $$</span></p>
<h4 id="exploration-penalty-r_textrevisit">Exploration Penalty <span
class="math inline"><em>R</em><sub>revisit</sub></span></h4>
<p>To discourage repetitive exploration of the same areas, which
indicates inefficient pathfinding, the agent receives a penalty for
re-entering previously visited cells. This penalty is crucial for
encouraging the exploration of new paths and preventing the agent from
getting stuck in loops or dead ends.</p>
<p><span class="math display"><em>R</em>_revisit = −10</span></p>
<h4 id="efficiency-penalty-r_textefficiency">Efficiency Penalty <span
class="math inline"><em>R</em><sub>efficiency</sub></span></h4>
<p>Every step the agent takes incurs a small penalty. This mechanism
ensures that the agent is incentivized to find the shortest possible
path to the goal, balancing the need to explore the environment with the
goal of reaching the destination as efficiently as possible.</p>
<p><span class="math display"><em>R</em>_efficiency = −5</span></p>
<h4 id="generic-reward-based-on-relative-distance-to-goal">Generic
Reward based on relative distance to goal</h4>
<p>The reward function <span
class="math inline"><em>R</em>(<em>s</em>, <em>a</em>)</span> is
designed to encourage reaching the goal while penalizing collisions and
inefficient paths. The reward for each step is defined as:</p>
<p><span class="math display">$$
R(s, a) = \begin{cases}
500 &amp; \text{if goal is reached} \\
-20 &amp; \text{if collision} \\
50 / (d + 1) &amp; \text{otherwise}
\end{cases}
$$</span></p>
<p>where <span class="math inline"><em>d</em></span> is the Euclidean
distance to the goal, encouraging the agent to minimize the distance to
the goal.</p>
<p>The episode terminates when the agent reaches the goal, collides with
an obstacle, or exceeds a predefined step limit, aiming to learn an
efficient navigation policy.</p>
<h3 id="scope-of-real-world-testing">Scope of Real-World Testing</h3>
<p>This study focused on conducting experiments within indoor settings,
where environmental conditions could be precisely regulated to mirror
theoretical constructs closely. Experiments were predominantly carried
out on a meticulously selected hard cloth surface to eliminate ground
flaws and ensure a uniform testing ground. This strategic selection was
crucial for the replication of simulation outcomes and for a controlled
assessment of the transition from simulation to reality (sim-to-real)
for autonomous technologies.</p>
<p>Nevertheless, the ambit of real-world experimentation was not
confined to indoor setups. Efforts were made to broaden the scope to
outdoor environments to ascertain the adaptability and resilience of the
proposed solutions under varied conditions. These ventures into the
outdoors faced substantial obstacles, mainly due to the challenges in
offsetting the differences in ground conditions. The variability and
unpredictability of outdoor landscapes exposed significant gaps in the
current method’s capacity to adjust to diverse real-world settings.</p>
<p>This issue became particularly pronounced in the section discussing
“Overcoming Navigation Challenges in Varying Environments,” where the
adaptation of the autonomous system to outdoor navigation met with
significant hurdles. While the system demonstrated successful
sim-to-real transfers in controlled indoor environments, the outdoor
experiments highlighted the imperative for additional research and
enhancement of the system’s flexibility. The outdoor testing
difficulties underscore the importance of broadening the experimental
scope and advancing autonomous technologies to navigate the intricacies
of unregulated terrains.</p>
<h2 id="experimental-outcomes-and-implementation-details">Experimental
Outcomes and Implementation Details</h2>
<p>The project embarked on a journey to bridge the virtual and
real-world through a meticulously designed environment and a
cutting-edge agent architecture.</p>
<h3 id="virtual-environment-and-agent-design">Virtual Environment and
Agent Design</h3>
<ul>
<li><p><strong>RCMazeEnv</strong>: Customized for this project, the
environment simulates a robotic car navigating a maze. Its design
replicates real-world physics and constraints, offering a rich testing
ground for reinforcement learning algorithms. The maze’s structure, from
its starting position to the goal, and the robotic car’s specifications,
including movement actions and sensor setups, are critical to the
simulation’s realism.</p></li>
<li><p><strong>Double Deep Q-Network (DDQN)</strong>: Employing two
neural networks, this model enhances traditional reinforcement learning
methods by reducing the overestimation of Q-values. The policy network
and the target network work in tandem to refine the agent’s learning
process through continuous interaction and sensor data
interpretation.</p></li>
</ul>
<h3 id="implementation-highlights">Implementation Highlights</h3>
<ul>
<li><p><strong>Environment and Agent Interaction</strong>: Central to
the DDQN agent’s strategy is its continuous adaptation to the
environment, leveraging sensor inputs to inform its decisions and
optimize its path through the maze. This iterative learning process is
visually represented through a simulation platform that allows for
detailed observation of the agent’s performance and strategy
adjustments.</p></li>
<li><p><strong>Real-World Application</strong>: Transferring the virtual
training to a physical RC robot involved comprehensive hardware setup
and calibration. Challenges such as sensor data normalization and
precise movement control were addressed to ensure a seamless transition
from virtual to real-world application.</p></li>
</ul>
<h3 id="evaluation-and-metrics">Evaluation and Metrics</h3>
<p>The project employed specific metrics to evaluate the agent’s
efficiency in navigating the maze, with emphasis on both simulation
performance and real-world applicability. This involved monitoring the
agent’s episodic performance, step efficiency, and adaptation to
real-world conditions.</p>
<h3 id="unique-features">Unique Features</h3>
<ul>
<li><strong>Physical Maze and Web Application</strong>: A constructed
physical maze served as the tangible counterpart to the virtual
<code>RCMazeEnv</code>, playing a crucial role in testing the RC robot’s
navigation capabilities. Additionally, a web application was developed
to act as a visualization and control interface, enhancing the
interaction between the virtual and real-world applications.</li>
</ul>
<h2 id="answers-to-research-questions">Answers to Research
Questions</h2>
<h3 id="virtual-environments-for-rf-car-training">1. Virtual
Environments for RF-Car Training</h3>
<p>The choice of a virtual environment is paramount in simulating the
complex dynamics of autonomous driving. Platforms such as Unity 3D,
AirSim, CARLA, OpenAI Gym, and ISAAC Gym offer varied features catering
to different aspects of driving simulation. However, for RF-car
training, OpenAI Gym is selected for its flexibility in custom
environment creation and its compatibility with Python, facilitating
ease of use and integration with existing advanced AI coursework .</p>
<p>Unity 3D and AirSim, while providing realistic simulations, require
expertise beyond Python, limiting their accessibility for the current
project scope. CARLA offers comprehensive autonomous driving simulation
capabilities but is tailored towards more traditional vehicle models
rather than RF-cars. ISAAC Gym, with its focus on robotics, presents a
similar mismatch in application. In contrast, OpenAI Gym’s simplicity
and reinforcement learning focus make it an ideal platform for this
project, supporting effective SIM2REAL transfer practices .</p>
<h3
id="reinforcement-learning-techniques-for-virtual-rf-car-training">2.
Reinforcement Learning Techniques for Virtual RF-Car Training</h3>
<p>The comparison of Deep Q-Network (DQN), Double Deep Q-Network (DDQN),
and Proximal Policy Optimization (PPO) techniques reveals that DDQN
offers the best fit for the project’s needs. DDQN’s architecture,
designed to address the overestimation bias inherent in DQN, enhances
accuracy in Q-value approximation—a critical factor in navigating the
complex, sensor-driven environments of RF-car simulations .</p>
<p>DQN, while powerful for high-dimensional sensory input processing,
falls short in environments with unpredictable dynamics, a limitation
DDQN effectively overcomes. PPO’s focus on direct policy optimization
provides stability and efficiency but lacks the precision in value
estimation necessary for RF-car training. Empirical trials further
validate DDQN’s superior performance, demonstrating its suitability for
the intricate maze-like environments encountered by virtual RF-cars
.</p>
<h3 id="sim-to-real-transfer-challenges-and-solutions">3. Sim-to-Real
Transfer Challenges and Solutions</h3>
<p>Transferring simulation models to real-world applications involves
addressing discrepancies in sensor data interpretation, action
synchronization, and physical dynamics. Solutions such as sensor data
normalization and action synchronization mechanisms were implemented to
align simulation outcomes with real-world performance .</p>
<p>The introduction of failsafe mechanisms and adjustments in motor
control timings proved critical in mitigating issues like collision
risks and movement inaccuracies, underscoring the importance of
iterative testing and adaptation in sim-to-real transfer .</p>
<h3 id="contributions-of-simulation-in-rf-car-training">4. Contributions
of Simulation in RF-Car Training</h3>
<p>Simulation training offers distinct advantages in efficiency, safety,
and computational resources. It enables uninterrupted and automated
training sessions, eliminates the risks associated with real-world
training, and leverages powerful computing resources to accelerate the
training process .</p>
<p>The comparative analysis between simulation and real-world training
outcomes highlights the practicality and effectiveness of simulation in
developing autonomous driving models, making it an indispensable tool in
the RF-car development process .</p>
<h3
id="practical-application-of-simulated-training-to-real-world-rf-cars">5.
Practical Application of Simulated Training to Real-World RF-Cars</h3>
<p>Applying a trained model to a physical RC car requires careful
consideration of environment, agent, and model adjustments. Strategies
for effective sim-to-real adaptation include fine-tuning sensor
interpretations, implementing action synchronization measures, and
adjusting physical dynamics to mirror those of the simulation .</p>
<p>This process ensures the successful application of simulation
training to real-world scenarios, facilitating the development of robust
and reliable autonomous driving systems .</p>
<h2 id="model-architecture-and-training-insights">Model Architecture and
Training Insights</h2>
<p>The Double DQN model’s architecture is central to understanding the
agent’s learning and decision-making capabilities. Structured with four
dense layers, it outputs three actions tailored to the RC car’s
movement, enabling sophisticated navigation strategies within the
maze.</p>
<p><strong>Model Architecture:</strong></p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model: &quot;sequential_52&quot;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu"># Layer (type) Output Shape Param</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">=================================================================</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>dense_200 (Dense) (None, 32) 224</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>dense_201 (Dense) (None, 64) 2112</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>dense_202 (Dense) (None, 32) 2080</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>dense_203 (Dense) (None, 3) 99</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">=================================================================</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>Total params: 4515 (17.64 KB)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>Trainable params: 4515 (17.64 KB)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>Non-trainable params: 0 (0.00 Byte)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>---</span></code></pre></div>
<p>This model is instrumental in the agent’s ability to learn from its
environment, adapting its strategy to optimize for both efficiency and
effectiveness in maze navigation.</p>
<h3 id="training-parameters">Training Parameters</h3>
<p>The training of the Double DQN agent was governed by the following
parameters:</p>
<ul>
<li><strong>Discount Factor (<code>DISCOUNT</code>)</strong>: 0.90</li>
<li><strong>Batch Size</strong>: 128
<ul>
<li>Number of steps (samples) used for training at a time.</li>
</ul></li>
<li><strong>Update Target Interval
(<code>UPDATE_TARGET_INTERVAL</code>)</strong>: 2
<ul>
<li>Frequency of updating the target network.</li>
</ul></li>
<li><strong>Epsilon (<code>EPSILON</code>)</strong>: 0.99
<ul>
<li>Initial exploration rate.</li>
</ul></li>
<li><strong>Minimum Epsilon (<code>MIN_EPSILON</code>)</strong>: 0.01
<ul>
<li>Minimum value for exploration rate.</li>
</ul></li>
<li><strong>Epsilon Decay Rate (<code>DECAY</code>)</strong>: 0.99973
<ul>
<li>Rate at which exploration probability decreases.</li>
</ul></li>
<li><strong>Number of Episodes (<code>EPISODE_AMOUNT</code>)</strong>:
170
<ul>
<li>Total episodes for training the agent.</li>
</ul></li>
<li><strong>Replay Memory Capacity
(<code>REPLAY_MEMORY_CAPACITY</code>)</strong>: 2,000,000
<ul>
<li>Maximum size of the replay buffer.</li>
</ul></li>
<li><strong>Learning Rate</strong>: 0.001
<ul>
<li>The rate at which the model learns from new observations.</li>
</ul></li>
</ul>
<h3 id="training-procedure">Training Procedure</h3>
<ol type="1">
<li><strong>Initialization</strong>: Start with a high exploration rate
(<code>EPSILON</code>) allowing the agent to explore the environment
extensively.</li>
<li><strong>Episodic Training</strong>: For each episode, the agent
interacts with the environment, collecting state, action, reward, and
next state data.</li>
<li><strong>Replay Buffer</strong>: Store these experiences in a replay
memory, which helps in breaking the correlation between sequential
experiences.</li>
<li><strong>Batch Learning</strong>: Randomly sample a batch of
experiences from the replay buffer to train the network.</li>
<li><strong>Target Network Update</strong>: Every
<code>UPDATE_TARGET_INTERVAL</code> episodes, update the weights of the
target network with those of the policy network.</li>
<li><strong>Epsilon Decay</strong>: Gradually decrease the exploration
rate (<code>EPSILON</code>) following the decay rate
(<code>DECAY</code>), shifting the strategy from exploration to
exploitation.</li>
<li><strong>Performance Monitoring</strong>: Continuously monitor the
agent’s performance in terms of rewards and success rate in navigating
the maze.</li>
</ol>
<h2 id="visual-insights-and-further-exploration">Visual Insights and
Further Exploration</h2>
<p>The project’s innovative approach to sim-to-real transfer in
reinforcement learning is encapsulated in a series of visual
representations and demonstrations, from the detailed construction of
the physical maze to the dynamic interface of the web application.</p>
<ul>
<li><strong>Maze Visualization:</strong></li>
</ul>
<figure>
<img src="./images/final_test/final_maze_build.jpeg" style="width:50.0%"
alt="Final Maze Build" />
<figcaption aria-hidden="true">Final Maze Build</figcaption>
</figure>
<ul>
<li><strong>Web Application Interface:</strong></li>
</ul>
<figure>
<img src="./images/thesis/web_app.png" style="width:100.0%"
alt="Web App Interface" />
<figcaption aria-hidden="true">Web App Interface</figcaption>
</figure>
<ul>
<li><strong>Simulation Test Video:</strong></li>
</ul>
<p><a
href="https://github.com/driessenslucas/researchproject/assets/91117911/66539a97-e276-430f-ab93-4a8a5138ee5e"
width="50%">DDQN Test in Action</a></p>
<div style="page-break-after: always;"></div>
<h3 id="evaluation-metrics-overview">Evaluation Metrics Overview</h3>
<h4 id="simulation-metrics">Simulation Metrics</h4>
<h5 id="episodic-performance">1. Episodic Performance</h5>
<ul>
<li><strong>Objective and Goal:</strong> This metric assesses the
agent’s learning curve and its ability to solve the maze with optimal
efficiency over successive episodes. The primary goal is to evaluate how
quickly and effectively the agent learns to reach the maze’s end,
reflecting on its strategy optimization and adaptation abilities.</li>
<li><strong>How it’s Assessed:</strong> By tracking the number of
episodes required before the agent can consistently solve the maze. A
decreasing trend in episode count needed over time indicates effective
learning and adaptation.</li>
<li><strong>Analytical Techniques:</strong> Statistical analysis or
visual plots (e.g., learning curves) are used to assess changes in
episodic performance across training sessions.</li>
<li><strong>Accuracy and Consistency Measures:</strong> Ensuring data
integrity and a controlled environment allows for consistent episode
comparison. Techniques might include averaging over multiple runs to
mitigate randomness in the agent’s learning process.</li>
</ul>
<h5 id="step-efficiency">2. Step Efficiency</h5>
<ul>
<li><strong>Objective and Goal:</strong> This measures the
decision-making process and path optimization capabilities of the agent
by counting the steps taken to solve the maze. Fewer steps indicate
higher efficiency and learning.</li>
<li><strong>How it’s Assessed:</strong> Tracking the number of steps
required to reach the goal in each episode and analyzing the trend over
time.</li>
<li><strong>Analytical Techniques:</strong> Quantitative analysis of
step count trends, possibly applying smoothing techniques to observe the
overall trend amidst the variability.</li>
<li><strong>Accuracy and Consistency Measures:</strong> Replication and
averaging, as well as maintaining a consistent maze configuration across
tests, help ensure reliable measurements.</li>
</ul>
<h5 id="mse-loss-measurement">3. MSE Loss Measurement</h5>
<p><span class="math display">$$
MSE(y, \hat{y}) = \frac{1}{N} \sum_{i=0}^{N-1} (y_i - \hat{y}_i)^2
$$</span></p>
<ul>
<li><strong>Objective and Goal:</strong> Quantifies the prediction
accuracy of the agent by measuring the squared discrepancy between the
agent’s predicted values and the actual outcomes. It’s a direct measure
of the agent’s learning accuracy.</li>
<li><strong>How it’s Assessed:</strong> Through the mathematical formula
provided, which averages the squared differences across all predictions
(N) in a given episode or batch of episodes.</li>
<li><strong>Analytical Techniques:</strong> MSE calculation is
straightforward but interpreting its trend over time requires
understanding its relationship with the agent’s learning phase (e.g.,
initial learning vs. strategy refinement).</li>
<li><strong>Accuracy and Consistency Measures:</strong> Regular
evaluation against a validation set or within a consistent testing
framework can provide reliable insights into the agent’s prediction
accuracy and learning progress.</li>
</ul>
<h5 id="reward-trend-analysis">4. Reward Trend Analysis</h5>
<ul>
<li><strong>Objective and Goal:</strong> To understand how the agent’s
actions lead to outcomes (rewards) and how this affects its ability to
navigate the maze efficiently, indicating learning proficiency and
strategy development.</li>
<li><strong>How it’s Assessed:</strong> Monitoring and analyzing the
history of rewards received by the agent, looking for trends of
increasing reward accumulation over time.</li>
<li><strong>Analytical Techniques:</strong> Time series analysis or
cumulative reward plots can illustrate the agent’s learning and
decision-making improvements.</li>
<li><strong>Accuracy and Consistency Measures:</strong> Averaging reward
trends over multiple runs and ensuring that reward distribution remains
unchanged throughout experiments.</li>
</ul>
<h5 id="epsilon-decay-tracking">5. Epsilon Decay Tracking</h5>
<ul>
<li><strong>Objective and Goal:</strong> This metric assesses the
agent’s balance between exploring new paths and exploiting known
successful routes, crucial for adaptive learning strategies.</li>
<li><strong>How it’s Assessed:</strong> By tracking the epsilon
parameter’s value over episodes, observing how it decreases according to
a predefined decay strategy, signaling a shift from exploration to
exploitation.</li>
<li><strong>Analytical Techniques:</strong> Analysis involves plotting
epsilon values over time to visualize the agent’s transition in learning
strategy.</li>
<li><strong>Accuracy and Consistency Measures:</strong> Consistent
application of the epsilon decay strategy across training sessions and
ensuring environmental stability for comparable results.</li>
</ul>
<h4 id="real-world-metrics">Real-World Metrics</h4>
<p>Transitioning to real-world application involved assessing how the
simulation-trained agent’s strategies fared in a physical maze with
tangible obstacles and limitations.</p>
<ul>
<li><strong>Maze Navigation</strong>: A visual assessment of the RC
car’s ability to navigate a real-world maze provided direct evidence of
the sim-to-real transfer’s effectiveness, highlighting the practical
application of the trained agent.</li>
<li><strong>Sensor Data Analysis</strong>: Evaluating the real-time
sensor data in navigation scenarios enabled a detailed understanding of
the agent’s interaction with the physical world, particularly in terms
of collision avoidance and pathfinding efficiency.</li>
</ul>
<h2 id="implementation-of-real-world-control-algorithms">Implementation
of Real-World Control Algorithms</h2>
<h3 id="introduction-1">Introduction</h3>
<p>This section outlines the practical implementation of control
algorithms developed and refined through simulations. We focus on their
application in a physical robot equipped with motion, orientation, and
communication systems. The primary objective is to evaluate the fidelity
of simulated behaviors when transferred to real-world scenarios,
demonstrating both the effectiveness and the limitations of the sim2real
transfer.</p>
<h3 id="system-overview">System Overview</h3>
<p>The experimental setup consists of an ESP32 microcontroller
interfaced with MPU6050 gyroscopic sensors, ultrasonic sensors for
distance measurement, and a motor control system. The robot’s
capabilities include navigational maneuvers such as moving forward,
turning left, and turning right, which are fundamental for testing the
real-world applicability of simulated algorithms.</p>
<h3 id="code-architecture-and-integration">Code Architecture and
Integration</h3>
<p><strong>System Initialization</strong></p>
<p>Before diving into the specific movement functions, it is essential
to understand the initial configuration and setup processes that prepare
the system for operation. The initialization sequence configures the
hardware interfaces, establishes network connections, and sets up the
sensors and actuators. This setup is crucial for ensuring that the
system operates reliably and is prepared to execute the movement
commands accurately.</p>
<!-- ```cpp
void setup() {
  Serial.begin(9600);        // Start serial communication at 9600 baud rate
  setupWifiAndOTA();          // Initialize WiFi and OTA update services
  Wire.begin();               // Start the I2C communication
  setupSensorsAndDisplay();   // Configure sensors and initialize the display
  setupMPU();                 // Initialize and calibrate MPU6050 sensor
  setupMotors();              // Set motor pins and initialize motors
}
``` -->
<ul>
<li><strong>WiFi and OTA Configuration</strong>: Establishes a network
connection and sets up Over-The-Air updates, allowing for remote
firmware upgrades and maintenance, which is critical for iterative
testing and enhancements.</li>
<li><strong>Sensor and Display Setup</strong>: Configures ultrasonic
sensors for distance measurement and initializes the display to provide
real-time feedback on the robot’s status and IP address, enhancing user
interaction and debug capabilities.</li>
<li><strong>MPU6050 Setup and Calibration</strong>: Ensures that the
gyroscopic sensor is calibrated to measure angles accurately, which is
vital for precise movement control during turns.</li>
<li><strong>Motor Setup</strong>: Prepares the motor drivers and sets
initial motor states, readying the system for movement commands.</li>
</ul>
<p><strong>Motor Control Mechanism</strong></p>
<p>This subsection describes in detail the implementation of movement
functions, showing the direct application of simulated navigation
algorithms into the real-world robotic system.</p>
<ul>
<li><p><strong>Variables for the motor control</strong></p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> initialSpeed <span class="op">=</span> <span class="dv">125</span><span class="op">;</span> <span class="co">// Set a higher initial speed</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> minSpeed <span class="op">=</span> <span class="dv">40</span><span class="op">;</span>      <span class="co">// Set a minimum speed</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> speed <span class="op">=</span> initialSpeed<span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="dt">int</span> TURN_DURATION <span class="op">=</span> <span class="dv">245</span><span class="op">;</span></span></code></pre></div>
<p>These variables define the initial and minimum speeds for the motors,
along with the duration for turning actions. The <code>speed</code>
variable dynamically adjusts the motor speed based on the turning angle,
ensuring precise and controlled movements.</p></li>
<li><p><strong>Forward Movement</strong></p>
<!-- ```cpp
void move_forward() {
  if (shouldStop) {
    shouldStop = false;
    return;
  }
  analogWrite(motorEnablePins[0], 255);    // Set maximum speed
  analogWrite(motorEnablePins[1], 255);
  digitalWrite(motorPins[0], LOW);   // Drive motors forward
  digitalWrite(motorPins[1], HIGH);

  for (int i = 0; i < 7; ++i) {  // Continue movement for a brief period
    delay(100);
    if (shouldStop) {  // Check if stopping condition was triggered
      break;
    }
  }

  stop_moving();  // Stop movement by resetting motor speeds
}
``` -->
<p>The <code>move_forward</code> function initiates forward motion at
maximum speed, incorporating real-time checks for emergency stops to
enhance safety. This reflects real-world conditions where response to
dynamic changes is critical.</p></li>
<li><p><strong>Left Turn</strong></p>
<!-- ```cpp
void move_left() {
  calibrateSensors();  // Recalibrate sensors to ensure accuracy
  isTurning = true;
  int speedIncrement = 0;  // Additional speed adjustment parameter

  while (isTurning && !shouldStop) {
    int16_t gx, gy, gz;
    mpu.getMotion6(&ax, &ay, &az, &gx, &gy, &gz);  // Read gyroscopic data
    float gyroZ = gz / 131.0;
    unsigned long currentTime = millis();
    if (lastTime == 0) lastTime = currentTime;

    float deltaTime = (currentTime - lastTime) / 1000.0;
    lastTime = currentTime;
    angleZ += gyroZ * deltaTime;  // Integrate gyro data to compute angle

    float angleDifference = abs(angleZ - initialAngleZ);
    if (!initialAngleSet) {
        initialAngleZ = angleZ;  // Set initial angle at the start of the turn
        initialAngleSet = true;
    }

    speed = initialSpeed - (int)((angleDifference / 90) * (initialSpeed - minSpeed));
    speed = max(speed, minSpeed);  // Dynamically adjust speed based on angle turned

    analogWrite(motorEnablePins[0], speed);
    analogWrite(motorEnablePins[1], speed);
    digitalWrite(motorPins[0], HIGH);
    digitalWrite(motorPins[1], HIGH);

    if (angleDifference >= 88) {
        stop_moving();  // Complete the turn once the desired angle is reached
        isTurning = false;
    }
    delay(100);
  }
}
``` -->
<p><code>move_left</code> dynamically adjusts the speed of the motors
based on the turning angle, a method developed from simulating how
physical and inertia properties affect movement. This method ensures
precise and controlled turns.</p></li>
<li><p><strong>Right Turn</strong></p>
<p>Similar to <code>move_left</code>, the <code>move_right</code>
function involves similar gyroscopic feedback and dynamic speed
adjustments but directs the motors to facilitate a right turn. The
inclusion of <code>calibrateSensors()</code> before each movement
ensures that the gyroscopic data is accurate, which is critical for
maintaining precision in physical movements that were modeled in a
virtual environment.</p></li>
<li><p><strong>Stopping Movement</strong></p>
<!-- ```cpp
void stop_moving() {
  shouldStop = true;  // Flag to indicate that stopping has been initiated
  isTurning = false;
  analogWrite(motorEnablePins[0], 0);  // Immediately cut power to motors
  analogWrite(motorEnablePins[1], 0);
  digitalWrite(motorPins[0], LOW);
  digitalWrite(motorPins[1], HIGH);
  shouldStop = false;  // Reset the stop flag after action is taken
}
``` -->
<p>The <code>stop_moving</code> function is designed to halt all motion
instantly, a feature that is essential for preventing accidents and
handling unexpected scenarios in dynamic environments.</p></li>
</ul>
<p><strong>Calibration and Sensor Data Interpretation</strong></p>
<p>The calibration process ensures that sensor data is accurate and
reliable, a critical step in maintaining the fidelity of simulated
behaviors in real-world applications. The <code>calibrateSensors</code>
function recalibrates the gyroscopic sensor to account for any drift or
inaccuracies that may arise during operation.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> calibrateSensors<span class="op">()</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">long</span> gyroZAccum <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    Serial<span class="op">.</span>println<span class="op">(</span><span class="st">&quot;Calibrating...&quot;</span><span class="op">);</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> <span class="dv">100</span><span class="op">;</span> i<span class="op">++)</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">{</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int16_t</span> ax<span class="op">,</span> ay<span class="op">,</span> az<span class="op">,</span> gx<span class="op">,</span> gy<span class="op">,</span> gz<span class="op">;</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        mpu<span class="op">.</span>getMotion6<span class="op">(&amp;</span>ax<span class="op">,</span> <span class="op">&amp;</span>ay<span class="op">,</span> <span class="op">&amp;</span>az<span class="op">,</span> <span class="op">&amp;</span>gx<span class="op">,</span> <span class="op">&amp;</span>gy<span class="op">,</span> <span class="op">&amp;</span>gz<span class="op">);</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        gyroZAccum <span class="op">+=</span> gz<span class="op">;</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        delay<span class="op">(</span><span class="dv">20</span><span class="op">);</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    mpu<span class="op">.</span>setZGyroOffset<span class="op">(-</span>gyroZAccum <span class="op">/</span> <span class="dv">13100</span><span class="op">);</span> <span class="co">// Adjust based on 100 readings</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    Serial<span class="op">.</span>println<span class="op">(</span><span class="st">&quot;Calibration Complete&quot;</span><span class="op">);</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<div style="page-break-after: always;"></div>
<h2 id="results">results</h2>
<h3 id="reinforcement-learning-techniques-overview">Reinforcement
Learning Techniques Overview</h3>
<h4 id="final-choice-ddqn">final choice: DDQN</h4>
<ul>
<li>The research project explored various reinforcement learning
techniques to train an agent for maze navigation, focusing on their
adaptability, efficiency, and real-world applicability. The following
techniques were evaluated:</li>
</ul>
<h5 id="visit-heatmap-for-ddqn">1. <strong>Visit Heatmap for
DDQN:</strong></h5>
<ul>
<li>The visit heatmap offers a graphical representation of the agent’s
frequency of visits to various states within the maze. The pattern
displayed suggests the agent’s favored paths and identifies potential
bottlenecks where the agent might have struggled. The heatmap serves as
a tool for analyzing the agent’s exploration patterns and its strategy
development throughout training.</li>
</ul>
<figure>
<img src="./images/training_images/visit_heatmap_DDQN.png"
alt="DDQN Heatmap" />
<figcaption aria-hidden="true">DDQN Heatmap</figcaption>
</figure>
<h5 id="reward-history-for-ddqn">2. <strong>Reward History for
DDQN:</strong></h5>
<ul>
<li>The reward history graph illustrates that the rewards stabilized at
around episode 50, indicating the agent’s learning progress and improved
decision-making. The consistent positive rewards signify the agent’s
successful navigation through the maze, with occasional dips reflecting
exploratory actions or suboptimal decisions. The upward trend in rewards
over time demonstrates the agent’s learning efficiency and strategy
optimization.</li>
</ul>
<figure>
<img src="./images/training_images/reward_history_DDQN.png"
alt="DDQN Reward History" />
<figcaption aria-hidden="true">DDQN Reward History</figcaption>
</figure>
<h5 id="reward-distribution-for-ddqn">3. <strong>Reward Distribution for
DDQN:</strong></h5>
<ul>
<li>Analyzing the reward distribution histogram reveals the frequency of
the received rewards. The concentration of instances near higher rewards
implies that the agent often achieved positive outcomes, while the long
tail towards negative rewards indicates the agent’s occasional
exploratory actions or suboptimal decisions.</li>
</ul>
<figure>
<img src="./images/training_images/reward_distribution_DDQN.png"
alt="DDQN Reward Distribution" />
<figcaption aria-hidden="true">DDQN Reward Distribution</figcaption>
</figure>
<h5 id="maze-solution-for-ddqn">4. <strong>Maze Solution for
DDQN:</strong></h5>
<ul>
<li>The maze solution visualization illustrates the agent’s path to
solving the maze. Notably, the agent achieved the goal in just 25 steps,
a testament to the DDQN’s efficiency in learning and path optimization.
This graphical representation highlights the agent’s capability to
derive an optimal route, avoiding backtracking and unnecessary
detours.</li>
</ul>
<figure>
<img src="./images/training_images/maze_solution_DDQN.png"
alt="DDQN Maze Path" />
<figcaption aria-hidden="true">DDQN Maze Path</figcaption>
</figure>
<h5 id="average-steps-per-episode-with-moving-average-for-ddqn">5.
<strong>Average Steps per Episode with Moving Average for
DDQN:</strong></h5>
<ul>
<li>The plot for the average steps per episode, smoothed by a moving
average, clearly shows the agent’s learning progression. The decrease in
the number of steps required to solve the maze, as portrayed by the
moving average line, underscores the DDQN’s ability to enhance the
agent’s efficiency in maze resolution.</li>
</ul>
<figure>
<img
src="./images/training_images/steps_per_episode_with_moving_avg_DDQN.png"
alt="DDQN Moving Average" />
<figcaption aria-hidden="true">DDQN Moving Average</figcaption>
</figure>
<h5 id="epsilon-history-for-ddqn">6. <strong>Epsilon History for
DDQN:</strong></h5>
<ul>
<li>The graph depicting the epsilon decay showcases the agent’s
transition from exploration to exploitation over time. Initially, a
higher epsilon value encouraged exploration, aiding the agent in
acquiring diverse experiences. As training progressed, the epsilon value
decayed, as evident in the graph’s steady decline, indicating the
agent’s increasing reliance on its learned policy. This adaptive
strategy was crucial for fine-tuning the agent’s decision-making
process, ensuring a balance between exploring new paths and exploiting
known ones for improved maze navigation.</li>
</ul>
<figure>
<img src="./images/training_images/epsilon_history_DDQN.png"
alt="DDQN Epsilon Decay" />
<figcaption aria-hidden="true">DDQN Epsilon Decay</figcaption>
</figure>
<h5 id="mean-squared-error-over-time-sampled-for-ddqn">7. <strong>Mean
Squared Error over time (Sampled) for DDQN:</strong></h5>
<ul>
<li>The MSE graph, a reflection of the agent’s prediction accuracy,
demonstrates a downward trend, indicative of the agent’s improved
learning over episodes. The initial spikes suggest a period of trial and
error, where the agent was developing its understanding of the maze.
Over time, the reduced variability in MSE values points towards the
agent making more accurate predictions, further underscoring the DDQN’s
effective learning curve.</li>
</ul>
<figure>
<img src="./images/training_images/mse_history_sampled_DDQN.png"
alt="Loss Trend" />
<figcaption aria-hidden="true">Loss Trend</figcaption>
</figure>
<h4 id="deep-q-network-dqn">1. Deep Q-Network (DQN)</h4>
<ul>
<li><p><strong>Description</strong>: The Deep Q-Network (DQN) combines a
deep neural network with a Q-learning framework. It excels in handling
high-dimensional sensory inputs, making it ideal for environments
demanding detailed interaction.</p></li>
<li><p><strong>Suitability</strong>: DQN’s advanced learning
capabilities are tempered by its tendency to overestimate Q-values in
complex environments. This limitation could affect its effectiveness in
training RC-cars, where environmental dynamics are
unpredictable.</p></li>
<li><p><strong>Integration and Results</strong>:</p>
<ul>
<li><strong>Reward History</strong>:</li>
</ul>
<figure>
<img src="./images/reward_history_dqn.png" alt="DQN Reward History" />
<figcaption aria-hidden="true">DQN Reward History</figcaption>
</figure>
<ul>
<li><strong>Performance</strong>: DQN’s performance, while competent,
was limited by Q-value overestimation in intricate scenarios.</li>
</ul></li>
</ul>
<h4 id="double-deep-q-network-ddqn">2. Double Deep Q-Network (DDQN)</h4>
<ul>
<li><p><strong>Description</strong>: The Double Deep Q-Network (DDQN)
improves upon DQN by employing two neural networks. This structure
effectively reduces overestimation bias by separating action selection
from Q-value generation.</p></li>
<li><p><strong>Reason for Selection</strong>:</p>
<ul>
<li>DDQN’s accuracy in Q-value approximation is crucial for navigating
complex environments, such as mazes.</li>
<li>The RC-car’s sensor limitations, which could lead to Q-value
overestimations, are better addressed by DDQN.</li>
<li>Empirical trials showed DDQN’s superior performance in maze
navigation tasks.</li>
</ul></li>
<li><p><strong>Integration and Results</strong>:</p>
<ul>
<li><strong>Reward History</strong>:</li>
</ul>
<figure>
<img src="./images/training_images/reward_history_DDQN.png"
alt="DDQN Reward History" />
<figcaption aria-hidden="true">DDQN Reward History</figcaption>
</figure>
<ul>
<li><strong>Performance</strong>: DDQN solved the environment in an
average of 25 steps, compared to DQN’s 34 steps, highlighting its
efficiency.</li>
</ul></li>
</ul>
<h4 id="proximal-policy-optimization-ppo">3. Proximal Policy
Optimization (PPO)</h4>
<ul>
<li><p><strong>Description</strong>: Proximal Policy Optimization (PPO)
is a policy gradient method that directly optimizes decision-making
policies. It’s known for its stability and efficiency in specific RL
contexts.</p></li>
<li><p><strong>Suitability</strong>: PPO’s emphasis on policy
optimization over value estimation makes it less suitable for RC-car
simulations, where accurate Q-value approximation is key.</p></li>
<li><p><strong>Integration and Results</strong>:</p>
<ul>
<li><strong>Reward History</strong>:</li>
</ul>
<figure>
<img src="./images/PPO_reward_history.png" alt="PPO Reward History" />
<figcaption aria-hidden="true">PPO Reward History</figcaption>
</figure>
<ul>
<li><strong>Performance</strong>: PPO, while stable, did not align well
with the precision requirements for RC-car maze navigation.</li>
</ul></li>
</ul>
<h2 id="real-world-application-and-limitations">Real-World Application
and Limitations</h2>
<h3 id="introduction-to-sensor-and-movement-discrepancies">Introduction
to Sensor and Movement Discrepancies</h3>
<p>The leap from simulated environments to real-world application
unveils a complex landscape of challenges, especially in the
interpretation of sensor data and the replication of vehicle movements.
This discussion delves into these critical aspects, highlighting both
the opportunities and constraints of applying simulation-derived
insights to actual autonomous vehicle (AV) operations.</p>
<h3 id="real-world-application">Real-World Application</h3>
<h4 id="enhanced-sensor-based-navigation">Enhanced Sensor-Based
Navigation</h4>
<p>Sensor-based navigation technologies, refined through simulation,
promise substantial improvements in autonomous vehicles’ functionality.
In real-world applications, such technologies are pivotal for
environments demanding high precision and adaptability. For instance, in
congested urban settings or in automated delivery systems, the ability
to dynamically navigate with high accuracy can significantly elevate
both safety and efficiency. Integrating simulation insights into
sensor-based navigation aids in refining these systems to better
interpret complex, variable real-world conditions.</p>
<h4 id="informing-autonomous-vehicle-movement">Informing Autonomous
Vehicle Movement</h4>
<p>Simulated environments offer a controlled setting to study vehicle
dynamics and movement responses. Applying these insights to the
development of autonomous vehicles can lead to advanced algorithms
capable of handling the unpredictable nature of real-world environments.
This knowledge is instrumental in enhancing autonomous systems’ ability
to safely and efficiently navigate through dynamic and often chaotic
traffic conditions, thereby improving the overall functionality of
autonomous transportation.</p>
<h3 id="limitations">Limitations</h3>
<h4 id="discrepancies-in-sensor-data-interpretation">Discrepancies in
Sensor Data Interpretation</h4>
<p>A substantial hurdle in the real-world application of
simulation-based insights is the variation in sensor data accuracy
between simulated and actual environments. These discrepancies can
directly impact the effectiveness of navigational algorithms,
potentially compromising the vehicle’s decision-making processes and, by
extension, its safety and operational efficiency.</p>
<h4 id="challenges-in-movement-replication">Challenges in Movement
Replication</h4>
<p>The precise replication of simulated vehicle movements in real-world
conditions encounters numerous obstacles. External factors such as road
surface variations, environmental conditions, vehicle load, and
mechanical constraints can introduce unforeseen deviations in vehicle
behavior. These real-world variances necessitate adjustments and
recalibration of the algorithms developed in simulated environments to
ensure their effectiveness and reliability outside the lab.</p>
<h4 id="practical-implementation-considerations">Practical
Implementation Considerations</h4>
<p>Successfully translating simulation insights into real-world
applications requires meticulous attention to several practical aspects.
These include, but are not limited to, sensor calibration to account for
environmental influences, adapting algorithms to hardware limitations,
and ensuring the system’s resilience to real-world unpredictabilities.
Addressing these factors is crucial for the effective deployment and
operational success of autonomous vehicles based on sim2real
insights.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Transitioning from simulation-based research to practical real-world
applications in autonomous vehicle navigation presents a unique set of
challenges and opportunities. While the application of
simulation-derived insights into sensor use and vehicle movement has the
potential to revolutionize autonomous vehicle technologies, significant
effort is required to bridge the gap between simulated accuracy and
real-world variability. Overcoming these challenges is essential for the
successful integration of sim2real technologies in enhancing the safety,
efficiency, and reliability of autonomous transportation systems.</p>
<h2
id="challenges-and-solutions-in-implementing-rl-techniques-and-virtual-environments">Challenges
and Solutions in Implementing RL Techniques and Virtual
Environments</h2>
<h3
id="challenge-1-selection-of-an-appropriate-virtual-environment">Challenge
1: Selection of an Appropriate Virtual Environment</h3>
<ul>
<li><strong>Description</strong>: Choosing a virtual environment
conducive to effective RC-car training is crucial.</li>
<li><strong>Solution</strong>: After evaluating various platforms,
<strong>OpenAI Gym</strong> was selected for its simplicity, familiarity
from previous coursework, and its focus on reinforcement learning.</li>
</ul>
<h3
id="challenge-2-choosing-the-optimal-reinforcement-learning-technique">Challenge
2: Choosing the Optimal Reinforcement Learning Technique</h3>
<ul>
<li><strong>Description</strong>: Selecting the most effective RL
technique for training the virtual RC-car.</li>
<li><strong>Solution</strong>: Through comparative analysis and
empirical testing, the Double Deep Q-Network (DDQN) was identified as
the most suitable technique, demonstrating superior performance in
navigating complex environments with fewer episodes.</li>
</ul>
<h3
id="challenge-3-sim2real-transfer---addressing-movement-discrepancies">Challenge
3: Sim2Real Transfer - Addressing Movement Discrepancies</h3>
<ul>
<li><strong>Description</strong>: Bridging the gap between simulation
and real-world in terms of RC-car movement and control.</li>
<li><strong>Solution Attempt</strong>: Fine-tuning the frequency of
action commands with an async method, waiting for the motor to finish
moving or considering a queued action system. Further more the
importance of precise movement in the real world was highlighted, which
was not a problem in the simulation.</li>
</ul>
<h3
id="challenge-4-alignment-issue-and-motor-encoder-implementation">Challenge
4: alignment Issue and Motor Encoder Implementation</h3>
<ul>
<li><p><strong>Description</strong>: Difficulty in achieving precise
straight-line movement in the RC car, with a persistent ~3-degree
offset.</p></li>
<li><p><strong>Solution Attempt 1</strong>: Implementation of motor
encoders was pursued to enhance movement accuracy. However, this
approach faced the same limitations in achieving the desired
precision.</p></li>
<li><p><strong>Solution Attempt 2</strong>: The motor was replaced with
a more powerful one, which initially showed promise in addressing the
alignment issue. However, after adding all the other components, the
car’s weight increased, leading to the same problem.</p></li>
<li><p><strong>Solution Attempt 3</strong>: The use of a MPU6050
gyroscope was explored to measure the car’s orientation and adjust the
movement accordingly. Even though this approach succeeded to some extent
(90 degrees turns were accurate), it was not able to solve the ~3-degree
offset issue when moving forward.</p></li>
<li><p><strong>Solution Attempt 4</strong>: The final solution I tried
was done by removing the RPI5 (previously used for sensor data and
running the web app) from the robot all together and using the ESP32 to
control both all the sensors and the motors. This allowed for a more
lightweight robot, which was able to move forward more precisely but it
failed to rotate 90 degrees accurately.</p></li>
</ul>
<h3
id="challenge-5-ensuring-consistent-and-effective-training">Challenge 5:
Ensuring Consistent and Effective Training</h3>
<ul>
<li><strong>Description</strong>: Maximizing training efficiency and
performance while maintaining consistency between simulation and
real-world scenarios.</li>
<li><strong>Solution</strong>: The simulation demonstrated considerable
advantages in terms of training efficiency, safety, and computational
power, establishing it as an indispensable tool in autonomous vehicle
model development.</li>
</ul>
<h3
id="challenge-6-accurate-sensor-data-normalization-for-sim2real-transfer">Challenge
6: Accurate Sensor Data Normalization for Sim2Real Transfer</h3>
<ul>
<li><p><strong>Description</strong>: Aligning sensor data between
simulated and real-world environments is critical for model
accuracy.</p></li>
<li><p><strong>Solution</strong>: Implementing specific normalization
techniques for both real-world and simulation sensor data ensured
consistency and compatibility, enhancing the model’s accuracy in
real-world applications.</p>
<ul>
<li><p><strong>Real-World Sensor Data Normalization:</strong></p>
<p>The function <code>map_distance</code> normalizes real-world sensor
data. It can be represented as follows:</p>
<!-- ![map_distance](./images/map_distance_equation.png) -->
<p><span class="math display">$$
\text{map\_distance}(d) = \begin{cases}
d &amp; \text{if } d &lt; 25 \\
25 + (d - 25) \times 0.5 &amp; \text{otherwise}
\end{cases}
$$</span></p>
<p>This function keeps distances under 25 cm unchanged and applies a
scaling factor of 0.5 to distances beyond 25 cm, adding this scaled
value to a base of 25 cm.</p></li>
<li><p><strong>Simulation Sensor Data Normalization:</strong></p>
<p>The function <code>normalize_distance</code> adjusts simulated sensor
data to a 0-1 range. Its equation is:</p>
<!-- ![normalize_distance](./images/normalize_distance_equation.png) -->
<p><span class="math display">$$
\text{normalize\_distance}(d) = \text{max}\left(0,
\text{min}\left(\frac{d}{\text{sensor\_max\_range}}, 1\right)\right)
\times 1000
$$</span></p>
<p>In this function, the distance is first scaled by dividing by
<code>sensor_max_range</code>. It’s then clamped between 0 and 1 before
multiplying by 1000 to normalize it within a specific range.</p></li>
</ul></li>
</ul>
<h3 id="challenge-7-integration-of-failsafe-mechanisms">Challenge 7:
Integration of Failsafe Mechanisms</h3>
<ul>
<li><strong>Description</strong>: Preventing potential collisions and
ensuring safe navigation in the real world.</li>
<li><strong>Solution</strong>: Development of a failsafe system that
prevents forward movement in hazardous situations, retraining the model
with this protocol to align real-world behavior with the simulated
environment.</li>
</ul>
<h3
id="challenge-8-training-environment-and-technique-efficacy">Challenge
8: Training Environment and Technique Efficacy</h3>
<ul>
<li><strong>Description</strong>: Determining the most effective
environment and RL technique for training.</li>
<li><strong>Solution</strong>: The DDQN solved the environment more
efficiently than DQN, highlighting the importance of technique
selection. The simulation provided a safer, more controlled environment
for training, reinforcing its selection over real-world training.</li>
</ul>
<h3 id="viewing-practical-experiments">Viewing Practical
Experiments</h3>
<p>For visual insights into my practical experiments addressing these
challenges, please refer to my supplementary video materials, which
illustrate the implementation and testing of solutions, from gyroscopic
adjustments to the integration of a more sophisticated control system
using the ESP32.</p>
<h3 id="conclusion-1">Conclusion</h3>
<p>This section has outlined the practical challenges encountered in
applying reinforcement learning (RL) techniques to autonomous RC cars.
My journey began with the selection of OpenAI Gym as the virtual
environment, chosen for its simplicity and relevance to RL. The Double
Deep Q-Network (DDQN) emerged as the most effective RL technique for
navigating complex environments.</p>
<p>However, transitioning from simulated models to real-world
applications revealed significant discrepancies, particularly in
movement control and sensor data alignment. I explored innovative
solutions such as the implementation of motor encoders, power
adjustments, and gyroscope integration, which partially addressed these
issues. Efforts to normalize sensor data and implement failsafe
mechanisms also contributed to better alignment with real-world
conditions.</p>
<p>A significant advancement was achieved by replacing the Raspberry Pi
and ESP32 with just the ESP32 module in the robot’s design, leading to a
more lightweight and precise robot. This change marked a considerable
step in overcoming the challenges previously faced.</p>
<p>Although I made substantial progress, some challenges remain. This
indicates a need for ongoing research and development to fully harness
the potential of RL in autonomous RC car navigation.</p>
<p>In conclusion, this project underscores the iterative and demanding
nature of applying RL techniques in real-world scenarios. It highlights
the importance of continuous refinement, innovation, and adaptation,
beyond the theoretical knowledge base. The journey through these
challenges has emphasized the significance of perseverance and creative
problem-solving in the evolving field of autonomous vehicle
technology.</p>
<h2 id="sources-of-inspiration-and-conceptual-framework">Sources of
Inspiration and Conceptual Framework</h2>
<p>The genesis of this research draws from a diverse collection of
sources, uniquely combining insights from technical documentation,
digital platforms, and academic literature. Central to the inspiration
were the challenges of micro mouse competitions and the potential of
reinforcement learning (RL) in navigating these complex mazes. These
initial sparks of interest were further fueled by dynamic demonstrations
of RL applications in autonomous vehicle control, particularly through
the lens of YouTube and GitHub repositories, alongside influential
academic research.</p>
<h3 id="micro-mouse-competitions-and-reinforcement-learning">Micro mouse
Competitions and Reinforcement Learning</h3>
<p>Micro mouse competitions, which task small robotic mice with the
navigation of mazes, served as a foundational inspiration for this
study. The direct application of RL in these competitions and related
technological showcases provided a compelling narrative on the potential
of RL in real-world problem-solving and autonomous control. The
exploration of maze traversal algorithms and the strategies for shortest
path finding, as detailed in the insightful Medium article by M. A.
Dharmasiri , enriched the conceptual foundation by illustrating
practical algorithmic approaches in similar contexts.</p>
<h3
id="influential-youtube-demonstrations-and-github-insights">Influential
YouTube Demonstrations and GitHub Insights</h3>
<p>YouTube videos such as “Self Driving and Drifting RC Car using
Reinforcement Learning” and “Reinforcement Learning with Multi-Fidelity
Simulators – RC Car” provided vivid demonstrations of RL’s applicability
in real-world settings, emphasizing the feasibility of sim-to-real
transfer. These resources, along with GitHub repositories detailing
ventures like the “Sim2Real_autonomous_vehicle” project , highlighted
the practical steps and challenges in implementing RL in physical
systems.</p>
<h3 id="technical-exploration-and-academic-foundation">Technical
Exploration and Academic Foundation</h3>
<p>The academic exploration was significantly shaped by articles on
autonomous driving decision control by Q. Song et al. and a survey on
sim-to-real transfer in deep reinforcement learning for robotics by W.
Zhao, J. P. Queralta, and T. Westerlund , which detailed the application
of advanced RL algorithms in controlling autonomous vehicles. These
articles provided a deep dive into the methodologies and challenges of
applying RL in autonomous systems, offering a broad academic perspective
on the field.</p>
<h3 id="synthesis-and-research-direction">Synthesis and Research
Direction</h3>
<p>These varied sources collectively informed the development of this
research, steering the focus towards the feasibility and intricacies of
sim2real transfer in the realm of autonomous navigation. The exploration
aims to synthesize insights from both digital and academic realms,
tackling the nuanced challenges of applying sophisticated RL models in
practical, tangible scenarios.</p>
<h2 id="integration-of-practical-experiments">Integration of Practical
Experiments</h2>
<p>Throughout this research project, I employed a series of practical
experiments to navigate and overcome encountered challenges. These
experiments, documented through video demonstrations, provide tangible
insights into my problem-solving process.</p>
<h3 id="addressing-alignment-and-orientation-challenges">Addressing
Alignment and Orientation Challenges</h3>
<p>One of the key challenges I faced was ensuring precise orientation
and alignment of the RC-car during movement. To tackle this, I utilized
the MPU6050 gyroscope, aiming to correct alignment issues and achieve
accurate 90-degree turns.</p>
<ul>
<li><p><strong>Utilizing the MPU6050 Gyroscope for Precise
Orientation</strong>: My first set of experiments focused on leveraging
the gyroscope to correct the car’s orientation for accurate navigation.
This approach was pivotal in my attempts to ensure the RC-car could
navigate mazes with high precision.</p>
<ul>
<li>To address alignment issues when attempting precise 90-degree turns,
I explored the potential of the MPU6050 gyroscope to adjust the car’s
movement based on its orientation. This experiment aimed to refine my
control over the vehicle’s navigation through the maze (<a
href="https://github.com/driessenslucas/researchproject/assets/91117911/32d9e29f-6d5a-4676-b609-2c08923ca1ac">View
Test 1</a>, <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/624b40f2-bee8-49f6-961d-1f72ab18fe13">View
Test 2</a>).</li>
<li>Further testing focused on using the gyroscope for realigning the
car’s forward movement, aiming to rectify the persistent ~3-degree
offset. Despite my efforts, completely eliminating this offset proved
challenging, showcasing the complexities of simulating real-world
physics (<a
href="https://github.com/driessenslucas/researchproject/assets/91117911/bb9aa643-9620-4979-a70c-ec2826c7dd33">View
Test 1</a>, <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/689b590f-3a9a-4f63-ba9c-978ddd08ab53">View
Test 2</a>, <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/99da37df-d147-43dc-828f-524f55dc6f70">View
Test 3</a>).</li>
</ul></li>
</ul>
<h3 id="enhancing-movement-precision-with-encoders">Enhancing Movement
Precision with Encoders</h3>
<p>The pursuit of enhancing the RC-car’s movement precision led us to
experiment with rotary encoders. These devices were integrated to
measure wheel rotations accurately, aiming to improve straight-line
movements and correct the noted ~3-degree offset.</p>
<ul>
<li><strong>Experimenting with Rotary Encoders</strong>: I introduced
rotary encoders to my setup, hoping to gain more precise control over
the car’s movements by accurately measuring wheel rotations. This
experiment represented a significant effort to refine the vehicle’s
navigation capabilities by ensuring more accurate movement and
orientation.
<ul>
<li>Initial tests with a new RC-car model, equipped with an encoder and
a more powerful motor, showed promise in addressing the forward movement
precision. However, the addition of extra components increased the
vehicle’s weight, impacting its movement and reintroducing the alignment
challenge (<a
href="https://github.com/driessenslucas/researchproject/assets/91117911/9728e29a-d2fa-48fa-b6e0-e2e1da92228f">View
Test 1</a>, <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/b9ce2cc3-85fd-4136-8670-516c123ba442">View
Test 2</a>).</li>
<li>Despite an encouraging start, a malfunction with one of the encoders
halted further tests using this specific setup, highlighting the
practical challenges of hardware reliability in real-world applications
(<a
href="https://github.com/driessenslucas/researchproject/assets/91117911/ae5129fa-c25f-4f89-92bb-4ee81df9f7a5">View
Test</a>).</li>
</ul></li>
</ul>
<h3 id="real-world-application-tests">Real-World Application Tests</h3>
<p>Moving beyond controlled environments, I conducted tests in both
outdoor and indoor settings to evaluate the RC-car’s performance in
real-world conditions. These tests were crucial for assessing the
practical application of my research findings.</p>
<ul>
<li><p><strong>Outdoor and Indoor Maze Tests</strong>: Real-world
testing scenarios presented unique challenges, such as varying surface
textures and unpredictable environmental conditions, which significantly
impacted the RC-car’s navigation capabilities.</p>
<ul>
<li>The outdoor test attempted to navigate the RC-car on uneven
surfaces, where surface texture variations greatly affected its
performance. This test underscored the importance of environmental
factors in autonomous navigation (<a
href="https://github.com/driessenslucas/researchproject/assets/91117911/02df8a25-b7f0-4061-89b7-414e6d25d31c">View
Test 1</a>, <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/187561a7-c0cb-4921-af3e-9c2c99cb0137">View
Test 2</a>).</li>
<li>Indoor testing provided a more controlled environment, allowing us
to closely monitor and adjust the RC-car’s navigation strategies.
Despite the controlled conditions, these tests highlighted the challenge
of accurately translating simulation models to real-world applications,
reflecting on the complexities of sim-to-real transfer (<a
href="https://github.com/driessenslucas/researchproject/assets/91117911/ce0f47e9-26cd-459e-8b26-ff345d1ee96b">View
Test 1</a>, <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/ea4a9bff-e191-4ce2-b2cc-acc57c781fa3">View
Test 2</a>, <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/4783729f-10cc-4c61-afa4-71cfc93d5d3e">View
Test 3</a>, <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/77091cb5-dbc5-4447-abc2-dc820dc66188">View
Test 4</a>).</li>
</ul></li>
</ul>
<h2 id="reflection">Reflection</h2>
<!-- --
  # TODO: Interviews with Sam and Wouter for feedback (have not done these interviews yet)
  • Wat zijn volgens hen de sterke en zwakke punten van het resultaat uit jouw researchproject?   
  • Is ‘het projectresultaat’ (incl. methodiek) bruikbaar in de bedrijfswereld?  
  • Welke alternatieven/suggesties geven bedrijven en/of community?   
  • Wat zijn de mogelijke implementatiehindernissen voor een bedrijf?    
  • Wat is de meerwaarde voor het bedrijf?   
  • Is er een maatschappelijke/economische/socio-economische meerwaarde aanwezig?  
-- -->
<p>The path from conceptualizing a virtual RF-car training simulation to
its real-world application traverses the rich terrain of integrating
theoretical research with tangible, practical outcomes. Reflecting on
feedback, along with the journey itself, unveils crucial insights into
the research process, its achievements, and areas ripe for growth:</p>
<h3 id="strengths-and-weaknesses">Strengths and Weaknesses</h3>
<p>The project’s resilience in adapting to unforeseen challenges stands
out as a testament to the robustness and flexibility of the research
approach. This adaptability is underscored by the ability to pivot in
methodology when confronted with real-world complexities not mirrored in
the simulation. However, an initial hesitancy to venture beyond familiar
tools and methodologies highlighted a potential limitation in fully
leveraging the breadth of available technologies and approaches. This
reticence, perhaps rooted in comfort with established practices, may
have initially narrowed the scope of exploration and innovation.</p>
<h3 id="practical-applicability-and-industry-relevance">Practical
Applicability and Industry Relevance</h3>
<p>The feedback collectively emphasizes the practical applicability and
value of the project’s findings within the industry. The methodology and
outcomes provide a concrete framework for navigating the intricacies of
sim-to-real transitions, crucial for the development of autonomous
vehicle technologies. This relevance extends beyond theoretical
interest, suggesting a solid foundation for application in real-world
autonomous system development.</p>
<h3 id="encountered-alternatives-and-flexibility">Encountered
Alternatives and Flexibility</h3>
<p>The encouragement to explore sophisticated simulation environments
and alternative machine learning methodologies resonates with a broader
industry and academic expectation for versatile, dynamic research
approaches. This suggests a pivotal learning moment: the importance of
maintaining flexibility in both tools and conceptual frameworks to
ensure research remains responsive and relevant to evolving
technological landscapes and real-world demands.</p>
<h3 id="anticipated-implementation-barriers">Anticipated Implementation
Barriers</h3>
<p>Identifying anticipated challenges in corporate implementation, such
as the need for significant investment and the integration of novel
findings into established workflows, offers a grounded perspective on
the path to practical application. This awareness is instrumental in
bridging the gap between research outcomes and their industry adoption,
guiding future strategies to mitigate these barriers.</p>
<h3 id="ethical-considerations">Ethical Considerations</h3>
<p>The deployment of autonomous systems, particularly those benefiting
from sim2real transfer technologies, raises significant ethical
considerations that must be addressed. Privacy concerns emerge as these
systems often rely on collecting and processing vast amounts of data,
potentially including personal information. Ensuring data protection and
privacy standards are paramount to maintaining public trust.</p>
<p>Safety is another critical concern, as the deployment of autonomous
systems in public spaces must not compromise human safety. The
robustness of sim2real transfer methodologies—ensuring systems can
reliably operate in unpredictable real-world conditions—is essential.
Additionally, the potential for job displacement cannot be overlooked.
As autonomous systems take on roles traditionally filled by humans,
strategies for workforce transition and re-skilling become necessary.
Our sim2real approach aims to address these concerns by advocating for
transparent, safe, and reliable system deployment, and suggesting
avenues for supporting affected workers through education and new job
opportunities in the evolving tech landscape.</p>
<h3 id="societal-impact">Societal Impact</h3>
<p>The societal impacts of deploying advanced autonomous systems are
wide-ranging. On the positive side, such systems can significantly
improve accessibility for disabled and elderly populations, offering new
levels of independence and mobility. Urban planning could also see
transformative changes, with autonomous systems contributing to more
efficient transportation networks and reduced traffic congestion.
However, these benefits come with challenges, including the risk of
increasing socio-economic divides if access to autonomous technologies
is uneven. The environmental impact, while potentially positive through
reduced emissions, also requires careful management to ensure
sustainable practices in the production and deployment of autonomous
systems.</p>
<h3 id="policy-and-regulation">Policy and Regulation</h3>
<p>Current policies and regulations around the deployment of autonomous
systems are often outpaced by technological advancements. As sim2real
transfer techniques mature, it is imperative that legislation evolves
accordingly. This includes updating safety standards to account for the
unique challenges of autonomous operation in dynamic environments, as
well as establishing clear liability frameworks for when things go
wrong. Engaging with policymakers and industry stakeholders is crucial
to developing a regulatory environment that supports innovation while
protecting public interests and safety. Our research suggests a
proactive approach, where the development of sim2real transfer
technologies goes hand-in-hand with policy formulation, ensuring a
harmonious integration of autonomous systems into society.</p>
<h3 id="lessons-learned-and-forward-path">Lessons Learned and Forward
Path</h3>
<p>This reflective journey underscores several key lessons: the value of
openness to new methodologies, the importance of bridging theory with
practice through versatile research approaches, and the critical role of
anticipatory thinking in addressing implementation barriers. Looking
forward, these insights pave the way for a research ethos characterized
by adaptability, responsiveness to industry needs, and a commitment to
contributing to societal progress through technological innovation.</p>
<h3 id="self-reflection-on-the-research-project">Self-reflection on the
Research Project</h3>
<p><strong>Proposed Success Criteria:</strong> The project aimed to
autonomously drive an RC car, with each action decided by the trained
model. If completed ahead of schedule, continual learning would be
applied to enable ongoing learning for the RC car.</p>
<p><strong>Achieved Success Criteria:</strong></p>
<ul>
<li><strong>Complete Control of the RC Car:</strong> Successfully
controlling the RC car using agent actions was the primary goal of this
project. This success illustrates the effectiveness of the reinforcement
learning techniques applied and the successful transfer from simulation
to real-world application.</li>
</ul>
<p><strong>Unachieved Success Criteria:</strong></p>
<ul>
<li><strong>Consistency in Maze Navigation:</strong> While RC car
control was successful, the project faced challenges in consistently
navigating through the entire maze in a real-world setting. This
highlights the complexity of the sim2real transfer, a core theme of the
research, and the practical difficulties of adapting simulation-based
training to physical execution.</li>
</ul>
<p><strong>What Went Smoothly:</strong></p>
<ul>
<li><strong>Creation of the Initial Virtual Environment:</strong>
Developing the virtual environment based on OpenAI Gym went smoothly,
providing a solid foundation for training the RL agent.</li>
<li><strong>Development and Integration of the Web Application:</strong>
Establishing a web application to visualize the simulation and
facilitate real-time interaction with the RC car. This not only enabled
monitoring and evaluation of the agent’s performance but also provided
an intuitive interface for experimentation and demonstration.</li>
</ul>
<p><strong>Areas for Improvement:</strong></p>
<ul>
<li><strong>Complexity of the Virtual Environment:</strong> Reflecting
on the research project, it is clear that the simplicity of the created
virtual environment limited its transferability to real-world scenarios.
The limited complexity of the environment resulted in a higher precision
requirement for the RC car, which was challenging to achieve in
practice. This insight underscores the importance of developing a more
advanced and realistic virtual environment that better reflects the
challenges of the real world.</li>
</ul>
<p><strong>Jury Feedback:</strong></p>
<p>The jury’s feedback was overwhelmingly positive, with expressions of
admiration for the depth of the research. Hans Ameel suggested potential
solutions for encountered challenges, such as increasing the distance
from the walls to reduce the impact of deviations. Another suggestion
was the use of a camera to monitor the position of the RC car within the
maze. Additionally, a correction was made to my presentation: although I
had indicated using an accelerometer to measure the rotation of my RC
car, in reality, I used the gyroscope that is part of the MPU6050, which
also includes an accelerometer and magnetometer.</p>
<div style="page-break-after: always;"></div>
<h2 id="advice">Advice</h2>
<h3 id="practical-utilization-of-simulations">Practical Utilization of
Simulations</h3>
<p>Simulations offer a controlled environment that minimizes risks and
enables the manipulation of variables with ease, which is ideal for
refining algorithms and testing hypotheses.</p>
<ul>
<li><strong>Cost-Effectiveness:</strong> By utilizing simulations,
significant cost reductions can be achieved as they negate the need for
physical prototypes and extensive real-world testing, especially during
the preliminary stages of research.</li>
</ul>
<h3
id="strategies-for-effective-transition-from-simulation-to-reality">Strategies
for Effective Transition from Simulation to Reality</h3>
<p>Transitioning from simulation environments to real-world applications
requires careful planning and execution to ensure the reliability and
validity of research outcomes.</p>
<ul>
<li><strong>Incremental Testing:</strong> Begin with simulations to hone
algorithms and methodologies, then progressively move towards real-world
testing to validate results and adjust for environmental variables.</li>
<li><strong>Feedback Loops:</strong> Implement continuous feedback
mechanisms to integrate insights from real-world tests back into the
simulation models, enhancing their accuracy and relevance.</li>
</ul>
<h3
id="overcoming-common-challenges-in-simulation-to-reality-transitions">Overcoming
Common Challenges in Simulation-to-Reality Transitions</h3>
<p>Adjustments are often necessary to bridge the gap between simulated
environments and actual conditions, particularly concerning sensor data
and mechanical operations.</p>
<ul>
<li><strong>Sensor Discrepancy Adjustments:</strong> Ensure sensors used
in real-world testing are regularly calibrated to match simulation
inputs, maintaining consistency and reliability.</li>
<li><strong>Movement and Mechanics Alignment:</strong> Prioritize the
alignment of physical movements and mechanics with those predicted by
simulations to facilitate seamless transitions.</li>
</ul>
<h3 id="insights-from-my-research">Insights from My Research</h3>
<ul>
<li><strong>Simulation Platforms:</strong> The selection of an
appropriate simulation platform, such as OpenAI Gym, is crucial, though
it may require supplementary tools for complex scenarios.</li>
<li><strong>DDQN Superiority:</strong> My research indicates that the
Double Deep Q-Network (DDQN) outperforms other models like DQN or PPO by
minimizing overestimations and stabilizing learning outcomes.</li>
</ul>
<h3 id="methodological-advice">Methodological Advice</h3>
<ul>
<li><strong>Comprehensive Evaluation:</strong> Employ both qualitative
and quantitative methods to assess the effectiveness of simulations and
real-world tests comprehensively.</li>
<li><strong>Adaptive Techniques:</strong> Remain flexible and responsive
to results and external feedback, which are vital for addressing
unforeseen challenges effectively.</li>
</ul>
<h3 id="practical-experiment-integration">Practical Experiment
Integration</h3>
<ul>
<li><strong>Prototyping and Iteration:</strong> Employ iterative designs
and prototyping to refine systems incrementally, bridging the
theoretical and practical aspects of research.</li>
<li><strong>Continuous Feedback:</strong> Actively seek and incorporate
feedback from stakeholders and peers, utilizing their insights to refine
both simulation models and real-world applications.</li>
</ul>
<h3 id="guidelines-for-future-research">Guidelines for Future
Research</h3>
<h4 id="introduction-2">Introduction</h4>
<p>This chapter offers a comprehensive methodology and advice for
researchers engaged in simulation-based studies, aiming to transition
successfully from theoretical models to practical applications.</p>
<h4 id="step-by-step-plan">Step-by-Step Plan</h4>
<h5 id="step-1-selection-of-simulation-environments">Step 1: Selection
of Simulation Environments</h5>
<ul>
<li><strong>Research and Evaluation:</strong> Investigate available
simulation tools appropriate for your study, considering platforms like
OpenAI Gym, Unity 3D, and CARLA.</li>
<li><strong>Criteria Development:</strong> Establish criteria based on
fidelity, scalability, and integration capabilities.</li>
<li><strong>Preliminary Testing:</strong> Evaluate the environments
against these criteria through initial testing.</li>
</ul>
<h5 id="step-2-managing-expectations-and-adaptability">Step 2: Managing
Expectations and Adaptability</h5>
<ul>
<li><strong>Expectation Setting:</strong> Define realistic expectations
for what simulations can achieve.</li>
<li><strong>Adaptation Strategies:</strong> Prepare to adjust your
research approach based on simulation outcomes and data
discrepancies.</li>
</ul>
<h5 id="step-3-methodology-flexibility">Step 3: Methodology
Flexibility</h5>
<ul>
<li><strong>Continuous Evaluation:</strong> Regularly reassess the
effectiveness of your methodologies.</li>
<li><strong>Integration of New Technologies:</strong> Incorporate
emerging technologies to enhance simulation outcomes.</li>
</ul>
<h5 id="step-4-sensor-data-and-validation">Step 4: Sensor Data and
Validation</h5>
<ul>
<li><strong>Data Calibration:</strong> Calibrate sensors to ensure
alignment with real-world conditions.</li>
<li><strong>Data Validation:</strong> Consistently validate simulated
data against real-world outcomes for accuracy.</li>
</ul>
<h5 id="step-5-socio-economic-impact-consideration">Step 5:
Socio-Economic Impact Consideration</h5>
<ul>
<li><strong>Impact Assessment:</strong> Evaluate how your research could
impact societal issues like safety and sustainability.</li>
<li><strong>Alignment with Broader Goals:</strong> Align your research
objectives with these broader societal impacts.</li>
</ul>
<h2 id="general-conclusion">General Conclusion</h2>
<p>This research has made significant strides in understanding the
feasibility and challenges of Sim2Real transfers in reinforcement
learning. While substantial progress was achieved, the journey
illuminated the vast landscape of challenges that lie in the nuanced
discrepancies between virtual and physical realms. Future endeavors in
this domain should continue to push the boundaries of what is possible,
leveraging the lessons learned to further bridge the gap between
simulation and reality. The potential applications of successfully
transferring RL agents to the real world are vast, promising
advancements in robotics, autonomous vehicles, and beyond.</p>
<!-- ## Credits

I am immensely grateful to my coach and supervisor, [Gevaert Wouter](wouter.gevaert@howest.be), for his guidance and clever insights that significantly shaped the course of this research project. In addition to his invaluable assistance during the project, I would also like to extend my thanks for the enjoyable courses he delivered during my time at Howest. -->
<div style="page-break-after: always;"></div>
<h2 id="guest-speakers">Guest Speakers</h2>
<h3
id="innovations-and-best-practices-in-ai-projects-by-jeroen-boeye-at-faktion">Innovations
and Best Practices in AI Projects by Jeroen Boeye at Faktion</h3>
<p>Jeroen Boeye’s comprehensive lecture, representing Faktion, offered
profound insights into the symbiotic relationship between software
engineering and artificial intelligence in the realm of AI solutions
development. He emphasized the critical importance of not merely
focusing on AI technology but also on the software engineering
principles that underpin the development of robust, scalable, and
maintainable AI systems. This approach ensures that AI solutions are not
only technically proficient but also practical and sustainable in
long-term applications.</p>
<p>The discussion delved into various aspects of AI applications,
notably highlighting Chatlayer’s contributions to the field of
conversational AI. Jeroen detailed how Chatlayer enhances chatbot
functionalities through dynamic conversational flows, significantly
improving the accuracy and contextuality of user interactions. Another
spotlight was on Metamaze, praised for its innovative approach to
automating document processing. By generating concise summaries from
documents and emails, Metamaze exemplifies the potential of supervised
machine learning to streamline and improve administrative tasks.</p>
<p>Jeroen provided a clear roadmap for the successful implementation of
AI projects, emphasizing the importance of validating business cases and
adopting a problem-first approach. He highlighted the necessity of
quality data as the foundation for any AI initiative and discussed
strategies for overcoming data limitations creatively. The lecture also
touched on the crucial mindset of embracing failure as a stepping stone
to innovation, stressing the importance of open communication with
stakeholders about challenges and setbacks.</p>
<p>The lecture further explored several practical use cases,
demonstrating the versatility and potential of AI across various
industries. From the detection of solar panels and unauthorized pools to
the damage inspection of air freight containers and early warning
systems for wind turbine gearboxes, Jeroen showcased how AI can address
complex challenges through innovative data sourcing, synthetic data
generation, and anomaly detection techniques. He also presented case
studies on energy analysis in brick ovens and egg incubation processes,
highlighting the critical role of data preprocessing and the application
of machine learning models to enhance efficiency and outcomes.</p>
<p>Key takeaways from Jeroen’s lecture underscored the importance of
mastering data preprocessing and treating data as a dynamic asset to
tailor AI models more precisely to specific needs. He offered practical
advice on operational efficiency, including the use of host mounts for
code integration and Streamlit for dashboard creation, to streamline
development processes.</p>
<p>In conclusion, Jeroen Boeye’s lecture provided a rich and detailed
perspective on the integration of AI technologies in real-world
scenarios. His insights into the critical importance of software
engineering principles, combined with a deep understanding of AI’s
capabilities and limitations, offered valuable guidance for developing
effective, sustainable AI solutions. This lecture not only highlighted
the current state and future directions of AI but also imparted
practical wisdom on navigating the complexities of AI project
implementation.</p>
<h3 id="pioneering-ai-solutions-at-noest-by-toon-vanhoutte">Pioneering
AI Solutions at Noest by Toon Vanhoutte</h3>
<p>Toon Vanhoutte’s enlightening lecture, representing Noest, a notable
entity within the Cronos Group, shared profound insights into the
harmonious blend of artificial intelligence and software engineering in
crafting state-of-the-art business solutions. With a strong team of 56
local experts, Noest prides itself on its pragmatic approach to
projects, aiming for a global impact while emphasizing craftsmanship,
partnership, and pleasure as its foundational pillars. This philosophy
extends across their diverse service offerings, including application
development, cloud computing, data analytics, AI innovations, low-code
platforms, ERP solutions, and comprehensive system integrations, all
underpinned by a strong partnership with Microsoft.</p>
<p>A particularly captivating case study presented was a project for a
packaging company, aimed at revolutionizing image search capabilities
based on product labels. The project encountered various challenges,
from dealing with inconsistent PDF formats to managing large file sizes
and overcoming processing limitations. These hurdles were adeptly
navigated using a combination of Azure Blob Storage for data management
and event-driven processing strategies for efficient and cost-effective
solutions, showcasing Noest’s adeptness in leveraging cloud technologies
to solve complex problems.</p>
<p>Enhancing searchability of images, a task that encompassed
recognizing text and objects within images, was another significant
challenge tackled by employing Azure AI Search, complemented by the
power of Large Language Models (LLMs) and vector search techniques. This
innovative approach enabled nuanced search functionalities beyond
traditional text queries, demonstrating the advanced capabilities of AI
in understanding and interpreting complex data.</p>
<p>Toon’s lecture further delved into the advancements in semantic
search, revealing how keyword, vector, and hybrid searches, alongside
semantic ranking, could dramatically enhance the accuracy and
contextuality of search results. Through practical demonstrations,
including comparisons between OCR and GPT-4 vision, attendees were shown
the potential of AI to transcend basic search functionalities and offer
deeper, more meaningful insights based on semantic understanding.</p>
<p>A key takeaway from the lecture was the importance of setting
realistic expectations with clients regarding AI’s capabilities and
potential inaccuracies, emphasizing the experimental nature of these
technologies. The journey through AI’s evolving landscape highlighted
the necessity of prompt engineering, the challenges of navigating an
immature yet rapidly developing field, and the crucial role of client
education in managing expectations around the capabilities of AI
technologies like GPT.</p>
<p>In conclusion, Toon Vanhoutte’s presentation not only showcased
Noest’s cutting-edge work in AI and software engineering but also
imparted valuable lessons on innovation, the importance of adaptable
problem-solving strategies, and the need for continuous learning in the
ever-evolving AI domain. It was a testament to Noest’s commitment to
pushing the boundaries of technology to create impactful, pragmatic
solutions that leverage the full spectrum of AI’s potential.</p>
<h2 id="installation-steps">Installation Steps</h2>
<p>This section outlines the required steps to install and set up the
project environment. Adherence to these instructions will ensure the
successful deployment of the autonomous navigation system.</p>
<h3 id="prerequisites">Prerequisites</h3>
<p>Before initiating the setup process, ensure the following
prerequisites are met:</p>
<ul>
<li><strong>Git:</strong> Necessary for cloning the project
repository.</li>
<li><strong>Docker:</strong> Utilized for containerizing the web
application and ensuring a consistent runtime environment.</li>
<li>Optionally, <strong>Python 3.11</strong> and <strong>pip</strong>
may be installed along with the dependencies listed in
<code>requirements.txt</code> for running the project without
Docker.</li>
</ul>
<h3 id="repository-setup">Repository Setup</h3>
<p>To clone the repository and navigate to the project directory,
execute the following commands:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/driessenslucas/researchproject.git</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> researchproject</span></code></pre></div>
<h3 id="hardware-setup-and-assembly">Hardware Setup and Assembly</h3>
<h4 id="introduction-to-hardware-components">Introduction to Hardware
Components</h4>
<p>This section provides an overview of the hardware components used in
the research project.</p>
<figure>
<img src="./images/final_test/jp_final.jpeg" alt="Final rc-car" />
<figcaption aria-hidden="true">Final rc-car</figcaption>
</figure>
<h4 id="components-list">Components List</h4>
<ul>
<li><strong>Core Components</strong>:
<ul>
<li>ESP32-WROOM-32 module (Refer to the datasheet at <a
href="https://www.espressif.com/sites/default/files/documentation/esp32-wroom-32_datasheet_en.pdf">Espressif</a>)</li>
<li>3D printed parts from Thingiverse (<a
href="https://www.thingiverse.com/thing:3436448/files">hc-sr04</a>, <a
href="https://www.thingiverse.com/thing:2544002">top plate + alternative
for the robot kit</a>)</li>
<li>Motor Driver - available at <a
href="https://www.dfrobot.com/product-66.html">DFRobot</a></li>
<li>2WD robot kit - available at <a
href="https://www.dfrobot.com/product-367.html">DFRobot</a></li>
<li>Mini OlED screen - available at <a
href="https://www.amazon.com.be/dp/B0BB1T23LF">Amazon</a></li>
<li>Sensors - available at <a
href="https://www.amazon.com.be/dp/B07XF4815H">Amazon</a></li>
<li>Battery For ESP 32 - available at <a
href="https://www.amazon.com.be/dp/B09Q4ZMNLW">Amazon</a></li>
</ul></li>
<li><strong>Supplementary Materials</strong>: List of additional
materials like screws, wires, and tools required for assembly.
<ul>
<li>4mm thick screws 5mm long to hold the wood together - available at
<a
href="https://www.brico.be/nl/gereedschap-installatie/ijzerwaren/schroeven/universele-schroeven/sencys-universele-schroeven-torx-staal-gegalvaniseerd-20-x-4-mm-30-stuks/5368208">brico</a></li>
<li>m3 bolt &amp; nuts - available at <a
href="https://www.brico.be/nl/gereedschap-installatie/ijzerwaren/bouten/sencys-cilinderkop-bout-gegalvaniseerd-staal-m3-x-12-mm-30-stuks/5367637">brico</a></li>
<li>wood for the maze - available at <a
href="https://www.brico.be/nl/bouwmaterialen/hout/multiplex-panelen/sencys-vochtwerend-multiplex-paneel-topplex-250x122x1-8cm/5356349">brico</a></li>
</ul></li>
</ul>
<h4 id="wiring-guide">Wiring Guide</h4>
<p><strong>esp32 pins</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> E1 <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> <span class="co">//PWM motor 1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> M1 <span class="op">=</span> <span class="dv">17</span><span class="op">;</span> <span class="co">//GPIO motor 1</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> E2 <span class="op">=</span> <span class="dv">19</span><span class="op">;</span> <span class="co">//PWM motor 2</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> M2 <span class="op">=</span> <span class="dv">4</span><span class="op">;</span> <span class="co">//GPIO motor 2</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> sensor0Trig <span class="op">=</span> <span class="dv">27</span><span class="op">;</span> <span class="co">//GPIO right sensor</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> sensor0Echo <span class="op">=</span> <span class="dv">26</span><span class="op">;</span> <span class="co">//GPIO right sensor</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> sensor1Trig <span class="op">=</span> <span class="dv">33</span><span class="op">;</span> <span class="co">//GPIO left sensor</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> sensor1Echo <span class="op">=</span> <span class="dv">32</span><span class="op">;</span> <span class="co">//GPIO left sensor</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> sensor2Trig <span class="op">=</span> <span class="dv">25</span><span class="op">;</span> <span class="co">//GPIO front sensor</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> sensor2Echo <span class="op">=</span> <span class="dv">35</span><span class="op">;</span> <span class="co">//GPIO front sensor</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">// OLED display and MPU6050 pins</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="pp">#define SDA_PIN </span><span class="dv">21</span><span class="pp"> </span><span class="co">// this is the default sda pin on the esp32</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="pp">#define SCL_PIN </span><span class="dv">22</span><span class="pp"> </span><span class="co">// this is the default scl pin on the esp32</span></span></code></pre></div>
<p><strong>ESP32 Wiring:</strong></p>
<figure>
<img src="./images/schematics/esp_updated.png" alt="ESP32 Wiring" />
<figcaption aria-hidden="true">ESP32 Wiring</figcaption>
</figure>
<h4 id="software-configuration">Software Configuration</h4>
<ol start="2" type="1">
<li><strong>Library Installation:</strong> Install the <a
href="https://github.com/lexus2k/ssd1306/tree/master">ESP32_SSD1306</a>
library to support the OLED display functionality.</li>
<li><strong>Code Upload:</strong> Transfer the scripts located in the <a
href="./esp32">esp32</a> folder to the ESP32 device. Modify the WiFi
settings in the script to match your local network configuration for
connectivity.</li>
</ol>
<h3 id="web-application-setup">Web Application Setup</h3>
<h4 id="note">Note</h4>
<p>To ensure a seamless setup of the virtual display, it is recommended
to execute <code>docker-compose down</code> following each session.</p>
<h4 id="steps">Steps</h4>
<ol type="1">
<li><p>The web application’s source code is stored within the <a
href="./web_app/">web app</a> directory. Access this directory:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ./web_app/</span></code></pre></div></li>
<li><p>To launch the Docker containers, use the following commands:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> up <span class="at">-d</span></span></code></pre></div></li>
</ol>
<h3 id="usage-instructions">Usage Instructions</h3>
<ol type="1">
<li>Access the web application by navigating to <a
href="http://localhost:8500" class="uri">http://localhost:8500</a> or <a
href="http://localhost:5000" class="uri">http://localhost:5000</a> on
your web browser.</li>
<li>Enter the ESP32’s IP address within the web app and select the
desired model for deployment.</li>
<li>The system provides an option for a virtual demonstration, allowing
for operation without engaging the physical vehicle.</li>
<li>Initiate the maze navigation by clicking the <code>Start Maze</code>
button.</li>
</ol>
<p>A demonstration of the project is available <a
href="https://github.com/driessenslucas/researchproject/assets/91117911/b440b295-6430-4401-845a-a94186a9345f">here</a>.</p>
<h3 id="additional-information-model-training">Additional Information:
Model Training</h3>
<ul>
<li>Opt between utilizing a pre-trained model or conducting new training
sessions using the script available in <a
href="./training/train.py">train</a>.</li>
<li>This training script is optimized for resource efficiency and can be
executed directly on the Raspberry Pi.</li>
<li>Upon completion, you will be prompted to save the new model. If
saved, it will be stored within the <a
href="./web_app/models">models</a> directory of the <code>web_app</code>
folder.</li>
</ul>
<div style="page-break-after: always;"></div>
<h2 id="references">References</h2>
<p>[1] G. Brockman et al., “OpenAI Gym,” arXiv preprint
arXiv:1606.01540, 2016.</p>
<p>[2] A. Dosovitskiy et al., “CARLA: An Open Urban Driving Simulator,”
in Proceedings of the 1st Annual Conference on Robot Learning, 2017.</p>
<p>[3] H. Van Hasselt, A. Guez, and D. Silver, “Deep Reinforcement
Learning with Double Q-learning,” in Proceedings of the AAAI Conference
on Artificial Intelligence, 2016.</p>
<p>[4] J. Schulman et al., “Proximal Policy Optimization Algorithms,”
arXiv preprint arXiv:1707.06347, 2017.</p>
<p>[5] J. Tobin et al., “Domain Randomization for Transferring Deep
Neural Networks from Simulation to the Real World,” in 2017 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
2017.</p>
<p>[6] K. Bousmalis et al., “Using Simulation and Domain Adaptation to
Improve Efficiency of Deep Robotic Grasping,” in IEEE International
Conference on Robotics and Automation (ICRA), 2018.</p>
<p>[7] Y. Pan and Q. Yang, “A Survey on Transfer Learning,” IEEE
Transactions on Knowledge and Data Engineering, vol. 22, no. 10,
pp. 1345-1359, Oct. 2010.</p>
<p>[8] A. A. Rusu et al., “Sim-to-Real Robot Learning from Pixels with
Progressive Nets,” in Proceedings of the Conference on Robot Learning,
2016.</p>
<p>[9] S. James et al., “Sim-to-Real via Sim-to-Sim: Data-efficient
Robotic Grasping via Randomized-to-Canonical Adaptation Networks,” in
Proceedings of the 2019 International Conference on Robotics and
Automation (ICRA), 2019.</p>
<p>[10] F. Sadeghi and S. Levine, “(CAD)^2RL: Real Single-Image Flight
without a Single Real Image,” in Proceedings of Robotics: Science and
Systems, 2016.</p>
<p>[11] “Self Driving and Drifting RC Car using Reinforcement Learning,”
YouTube, Aug. 19, 2019. [Online Video]. Available: <a
href="https://www.youtube.com/watch?v=U0-Jswwf0hw"
class="uri">https://www.youtube.com/watch?v=U0-Jswwf0hw</a>. [Accessed:
Jan. 29, 2024].</p>
<p>[12] Q. Song et al., “Autonomous Driving Decision Control Based on
Improved Proximal Policy Optimization Algorithm,” Applied Sciences,
vol. 13, no. 11, Art. no. 11, Jan. 2023. [Online]. Available: <a
href="https://www.mdpi.com/2076-3417/13/11/6400"
class="uri">https://www.mdpi.com/2076-3417/13/11/6400</a>. [Accessed:
Jan. 29, 2024].</p>
<p>[13] DailyL, “Sim2Real_autonomous_vehicle,” GitHub repository,
Nov. 14, 2023. [Online]. Available: <a
href="https://github.com/DailyL/Sim2Real_autonomous_vehicle"
class="uri">https://github.com/DailyL/Sim2Real_autonomous_vehicle</a>.
[Accessed: Jan. 29, 2024].</p>
<p>[14] “OpenGL inside Docker containers, this is how I did it,” Reddit,
r/docker. [Online]. Available: <a
href="https://www.reddit.com/r/docker/comments/8d3qox/opengl_inside_docker_containers_this_is_how_i_did/"
class="uri">https://www.reddit.com/r/docker/comments/8d3qox/opengl_inside_docker_containers_this_is_how_i_did/</a>.
[Accessed: Jan. 29, 2024].</p>
<p>[15] M. A. Dharmasiri, “Micromouse from scratch | Algorithm- Maze
traversal | Shortest path | Floodfill,” Medium, [Online]. Available: <a
href="https://medium.com/@minikiraniamayadharmasiri/micromouse-from-scratch-algorithm-maze-traversal-shortest-path-floodfill-741242e8510"
class="uri">https://medium.com/@minikiraniamayadharmasiri/micromouse-from-scratch-algorithm-maze-traversal-shortest-path-floodfill-741242e8510</a>.
[Accessed: Jan. 29, 2024].</p>
<p>[16] “Reinforcement Learning with Multi-Fidelity Simulators – RC
Car,” YouTube, Dec. 30, 2014. [Online Video]. Available: <a
href="https://www.youtube.com/watch?v=c_d0Is3bxXA"
class="uri">https://www.youtube.com/watch?v=c_d0Is3bxXA</a>. [Accessed:
Jan. 29, 2024].</p>
<p>[17] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-Real
Transfer in Deep Reinforcement Learning for Robotics: A Survey,” in 2020
IEEE Symposium Series on Computational Intelligence (SSCI), Dec. 2020,
pp. 737–744. [Online]. Available: <a
href="https://arxiv.org/pdf/2009.13303.pdf"
class="uri">https://arxiv.org/pdf/2009.13303.pdf</a>.</p>
<p>[18] R. S. Sutton and A.G. Barto, Reinforcement Learning: An
Introduction, 2nd ed. Cambridge, MA: The MIT Press, 2018.</p>
<p>[19] H. van Hasselt, A. Guez, D. Silver, et al., “Deep Reinforcement
Learning with Double Q-learning,” arXiv preprint arXiv:1509.06461,
2015.</p>
<p>[20] Papers With Code, “Double DQN Explained,” [Online]. Available:
<a href="https://paperswithcode.com/method/double-dqn"
class="uri">https://paperswithcode.com/method/double-dqn</a>.</p>
<p>[21] D. Jayakody, “Double Deep Q-Networks (DDQN) - A Quick Intro
(with Code),” 2020. [Online]. Available: <a
href="https://dilithjay.com/blog/2020/04/18/double-deep-q-networks-ddqn-a-quick-intro-with-code/"
class="uri">https://dilithjay.com/blog/2020/04/18/double-deep-q-networks-ddqn-a-quick-intro-with-code/</a>.</p>

\end{document}