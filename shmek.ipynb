{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, env, input_shape, alpha=1e-4, gamma=0.99, learning_rate=0.001, batch_size=64, epsilon=0.99, epsilon_decay=0.993, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.state_shape = input_shape\n",
    "        self.action_shape = len(env.possible_actions)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self.build_policy_network()\n",
    "        self.states = []\n",
    "        self.gradients = []\n",
    "        self.rewards = []\n",
    "        self.probs = []\n",
    "        self.total_rewards = []\n",
    "        self.batch_rewards = []\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "    \n",
    "    def build_policy_network(self):\n",
    "        # Enhanced network architecture\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=self.state_shape))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_shape, activation='softmax'))\n",
    "        \n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def hot_encode_action(self, action):\n",
    "        action_encoded = np.zeros(self.action_shape)\n",
    "        action_encoded[action] = 1\n",
    "        return action_encoded\n",
    "\n",
    "    def remember(self, state, action, action_prob, reward):\n",
    "        encoded_action = self.hot_encode_action(action)\n",
    "        self.gradients.append(encoded_action - action_prob)\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.probs.append(action_prob)\n",
    "\n",
    "    def compute_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Exploration: choose a random action\n",
    "            action = np.random.choice(self.action_shape)\n",
    "            action_probability_distribution = np.ones(self.action_shape) / self.action_shape\n",
    "        else:\n",
    "            # Exploitation: choose action based on policy\n",
    "            state = state.reshape([1, state.shape[0]])\n",
    "            action_probability_distribution = self.model.predict(state).flatten()\n",
    "            action_probability_distribution /= np.sum(action_probability_distribution)\n",
    "            action = np.random.choice(self.action_shape, 1, p=action_probability_distribution)[0]\n",
    "        \n",
    "        return action, action_probability_distribution\n",
    "\n",
    "    def get_discounted_rewards(self, rewards):\n",
    "        discounted_rewards = []\n",
    "        cumulative_total_return = 0\n",
    "        for reward in rewards[::-1]:\n",
    "            cumulative_total_return = (cumulative_total_return * self.gamma) + reward\n",
    "            discounted_rewards.insert(0, cumulative_total_return)\n",
    "\n",
    "        mean_rewards = np.mean(discounted_rewards)\n",
    "        std_rewards = np.std(discounted_rewards)\n",
    "        norm_discounted_rewards = (discounted_rewards - mean_rewards) / (std_rewards + 1e-7)\n",
    "        return norm_discounted_rewards\n",
    "\n",
    "    def train_policy_network(self):\n",
    "        states = np.vstack(self.states)\n",
    "        gradients = np.vstack(self.gradients)\n",
    "        rewards = np.vstack(self.rewards)\n",
    "        discounted_rewards = self.get_discounted_rewards(rewards)\n",
    "        gradients *= discounted_rewards\n",
    "        y_train = self.alpha * gradients + self.probs\n",
    "\n",
    "        # Gradient clipping\n",
    "        self.model.optimizer.get_gradients = lambda loss, vars: [\n",
    "            tf.clip_by_value(g, -1., 1.) for g in tf.gradients(loss, vars)]\n",
    "\n",
    "        history = self.model.train_on_batch(states, y_train)\n",
    "        self.states, self.probs, self.gradients, self.rewards = [], [], [], []\n",
    "        return history\n",
    "\n",
    "    def train(self, episodes):\n",
    "        env = self.env\n",
    "        total_rewards = np.zeros(episodes)\n",
    "        reward_history = []\n",
    "        episode_count = 0\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action, prob = self.compute_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                self.remember(state, action, prob, reward)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                env.render(delay=0, framerate=360)\n",
    "\n",
    "                if done:\n",
    "                    episode_count += 1\n",
    "                \n",
    "                # do a batch update every 2 episodes\n",
    "                \n",
    "                    \n",
    "                if episode_count % 2 == 0:\n",
    "                    states = np.vstack(self.states)\n",
    "                    gradients = np.vstack(self.gradients)\n",
    "                    rewards = np.vstack(self.rewards)\n",
    "                    discounted_rewards = self.get_discounted_rewards(rewards)\n",
    "                    gradients *= discounted_rewards\n",
    "                    y_train = self.alpha * gradients + self.probs\n",
    "                    self.train_policy_network()\n",
    "                    episode_count = 0\n",
    "\n",
    "            total_rewards[episode] = episode_reward\n",
    "            reward_history.append(episode_reward)\n",
    "\n",
    "            # Epsilon decay\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
    "\n",
    "            print(f'Episode: {episode}, Reward: {episode_reward}, Epsilon: {self.epsilon}')\n",
    "\n",
    "            \n",
    "            # Improved early stopping based on recent reward history\n",
    "            if len(reward_history) > 10:\n",
    "                last_10_rewards = reward_history[-10:]\n",
    "                if all(reward > 0 for reward in last_10_rewards):\n",
    "                    differences = [abs(last_10_rewards[i] - last_10_rewards[i-1]) for i in range(1, 10)]\n",
    "                    if all(diff < 200 for diff in differences):\n",
    "                        print('The difference between each of the last 10 positive rewards is less than 200, stopping training')\n",
    "                        break\n",
    "\n",
    "        self.total_rewards = total_rewards\n",
    "        return reward_history\n",
    "    \n",
    "    def test(self, episodes, render=False):\n",
    "        env = self.env\n",
    "        total_rewards = np.zeros(episodes)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.compute_action_for_testing(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "            total_rewards[episode] = episode_reward\n",
    "            print(f'Episode: {episode}, Reward: {episode_reward}')\n",
    "\n",
    "        average_reward = np.mean(total_rewards)\n",
    "        print(f'Average Reward: {average_reward}')\n",
    "        return total_rewards\n",
    "\n",
    "    def compute_action_for_testing(self, state):\n",
    "        state = state.reshape([1, state.shape[0]])\n",
    "        action_probability_distribution = self.model.predict(state).flatten()\n",
    "        action = np.argmax(action_probability_distribution)\n",
    "        return action\n",
    "\n",
    "    def hot_encode_action(self, action):\n",
    "\n",
    "        action_encoded=np.zeros(self.action_shape)\n",
    "        action_encoded[action]=1\n",
    "\n",
    "        return action_encoded\n",
    "    \n",
    "    def remember(self, state, action, action_prob, reward):\n",
    "\n",
    "        encoded_action=self.hot_encode_action(action)\n",
    "        self.gradients.append(encoded_action-action_prob)\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.probs.append(action_prob)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play env\n",
    "env= RCMazeEnv()\n",
    "\n",
    "alpha = 0.0001\n",
    "gamma = 0.99\n",
    "learning_rate = 0.001\n",
    "EPSILON = 0.99\n",
    "epsilon_decay = 0.93\n",
    "\n",
    "N_EPISODES=20\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "state = env.reset() # reset to env \n",
    "Agent = REINFORCE(env,input_shape=state.shape, epsilon=EPSILON, epsilon_decay=epsilon_decay , alpha=alpha, gamma=gamma, learning_rate=learning_rate)\n",
    "\n",
    "rewards = Agent.train(N_EPISODES)\n",
    "env.close_pygame()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "env = RCMazeEnv()\n",
    "state = env.reset()\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "from keras.models import load_model\n",
    "Agent = REINFORCE(env, input_shape=state.shape)\n",
    "Agent.model = load_model('./models_old/REINFORCE_RCmaze.h5')\n",
    "\n",
    "done = False\n",
    "\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    env.render(delay=100, framerate=360)\n",
    "   \n",
    "    action, prob = Agent.compute_action(state)\n",
    "    state, reward, done = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    env.render()\n",
    "    if done:\n",
    "        print('done in ', len(rewards), 'steps')\n",
    "        break\n",
    "      \n",
    "env.close()\n",
    "print(sum(rewards))\n",
    "env.close_pygame()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
