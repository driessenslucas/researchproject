{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 10:06:09.094037: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-17 10:06:09.234971: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-17 10:06:09.235083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-17 10:06:09.258762: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-17 10:06:09.309787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 10:06:09.866955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-17 10:06:10.255688: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-17 10:06:10.315708: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-17 10:06:10.315930: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np   \n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import pygame\n",
    "\n",
    "# Import Tensorflow libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# disable eager execution (optimization)\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "# ###### Tensorflow-GPU ########\n",
    "try:\n",
    "  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  print(\"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCMazeEnv(gym.Env):\n",
    "    def __init__(self, maze_size_x=12, maze_size_y=12):\n",
    "        self.maze_size_x = maze_size_x\n",
    "        self.maze_size_y = maze_size_y\n",
    "        self.maze = self.generate_maze()\n",
    "        self.car_position = (1, 1)\n",
    "        self.possible_actions = range(3)\n",
    "        self.car_orientation = 'E'\n",
    "        self.sensor_readings = {'front': 0, 'left': 0, 'right': 0}\n",
    "        self.steps = 0\n",
    "        self.previous_distance = 0\n",
    "        self.goal = (10, 10)\n",
    "        self.previous_steps = 0\n",
    "        self.visited_positions = set()\n",
    "        self.reset()\n",
    "\n",
    "            \n",
    "    def generate_maze(self):\n",
    "        # For simplicity, create a static maze with walls\n",
    "        # '1' represents a wall, and '0' represents an open path\n",
    "        maze = np.zeros((self.maze_size_y, self.maze_size_x), dtype=int)\n",
    "        # Add walls to the maze (this can be customized)\n",
    "\n",
    "        \n",
    "        layout = [\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
    "            [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "            [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
    "            [1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
    "            [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "        \n",
    "     \n",
    "        maze = np.array(layout)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    def reset(self):\n",
    "        self.car_position = (1, 1)\n",
    "        self.car_orientation = 'E'\n",
    "        self.update_sensor_readings()\n",
    "        self.steps = 0\n",
    "        self.previous_distance = 0\n",
    "        self.previous_steps = 0\n",
    "        self.visited_positions.clear()  # Clear the visited positions\n",
    "        self.visited_positions.add(self.car_position)\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.move_forward()\n",
    "        elif action == 1:\n",
    "            self.turn_left()\n",
    "        elif action == 2:\n",
    "            self.turn_right()\n",
    "        self.update_sensor_readings()\n",
    "        self.visited_positions.add(self.car_position)\n",
    "        reward = self.compute_reward()\n",
    "        self.steps += 1\n",
    "        done = self.is_done()\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    \n",
    "    def move_forward(self):\n",
    "        x, y = self.car_position\n",
    "        if self.car_orientation == 'N' and y > 0 and self.maze[y - 1][x] != 1:\n",
    "            self.car_position = (x, y - 1)\n",
    "        elif self.car_orientation == 'S' and y < self.maze_size_y - 1 and self.maze[y + 1][x] != 1:\n",
    "            self.car_position = (x, y + 1)\n",
    "        elif self.car_orientation == 'E' and x < self.maze_size_x - 1 and self.maze[y][x + 1] != 1:\n",
    "            self.car_position = (x + 1, y)\n",
    "        elif self.car_orientation == 'W' and x > 0 and self.maze[y][x - 1] != 1:\n",
    "            self.car_position = (x - 1, y)\n",
    "        \n",
    "\n",
    "    def turn_left(self):\n",
    "        orientations = ['N', 'W', 'S', 'E']\n",
    "        idx = orientations.index(self.car_orientation)\n",
    "        self.car_orientation = orientations[(idx + 1) % 4]\n",
    "\n",
    "    def turn_right(self):\n",
    "        orientations = ['N', 'E', 'S', 'W']\n",
    "        idx = orientations.index(self.car_orientation)\n",
    "        self.car_orientation = orientations[(idx + 1) % 4]\n",
    "\n",
    "    def update_sensor_readings(self):\n",
    "        # Simple sensor implementation: counts steps to the nearest wall\n",
    "        self.sensor_readings['front'] = self.distance_to_wall('front')\n",
    "        self.sensor_readings['left'] = self.distance_to_wall('left')\n",
    "        self.sensor_readings['right'] = self.distance_to_wall('right')\n",
    "\n",
    "    def distance_to_wall(self, direction):\n",
    "        x, y = self.car_position\n",
    "        sensor_max_range = 255  # Maximum range of the ultrasonic sensor\n",
    "\n",
    "        def calculate_distance(dx, dy):\n",
    "            distance = 0\n",
    "            while 0 <= x + distance * dx < self.maze_size_x and \\\n",
    "                0 <= y + distance * dy < self.maze_size_y and \\\n",
    "                self.maze[y + distance * dy][x + distance * dx] != 1:\n",
    "                distance += 1\n",
    "                if distance > sensor_max_range:  # Limiting the sensor range\n",
    "                    break\n",
    "            return distance\n",
    "\n",
    "        if direction == 'front':\n",
    "            if self.car_orientation == 'N':\n",
    "                distance = calculate_distance(0, -1)\n",
    "            elif self.car_orientation == 'S':\n",
    "                distance = calculate_distance(0, 1)\n",
    "            elif self.car_orientation == 'E':\n",
    "                distance = calculate_distance(1, 0)\n",
    "            elif self.car_orientation == 'W':\n",
    "                distance = calculate_distance(-1, 0)\n",
    "\n",
    "        elif direction == 'left':\n",
    "            if self.car_orientation == 'N':\n",
    "                distance = calculate_distance(-1, 0)\n",
    "            elif self.car_orientation == 'S':\n",
    "                distance = calculate_distance(1, 0)\n",
    "            elif self.car_orientation == 'E':\n",
    "                distance = calculate_distance(0, -1)\n",
    "            elif self.car_orientation == 'W':\n",
    "                distance = calculate_distance(0, 1)\n",
    "\n",
    "        elif direction == 'right':\n",
    "            if self.car_orientation == 'N':\n",
    "                distance = calculate_distance(1, 0)\n",
    "            elif self.car_orientation == 'S':\n",
    "                distance = calculate_distance(-1, 0)\n",
    "            elif self.car_orientation == 'E':\n",
    "                distance = calculate_distance(0, 1)\n",
    "            elif self.car_orientation == 'W':\n",
    "                distance = calculate_distance(0, -1)\n",
    "\n",
    "        # Normalize the distance to a range of 0-1\n",
    "        normalized_distance = distance / sensor_max_range\n",
    "        normalized_distance = max(0, min(normalized_distance, 1))\n",
    "\n",
    "        return normalized_distance * 1000\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        # Initialize reward\n",
    "        reward = 0\n",
    "\n",
    "        # Check for collision or out of bounds\n",
    "        if any(self.sensor_readings[direction] == 0 for direction in ['front', 'left', 'right']):\n",
    "            reward -= 20\n",
    "\n",
    "        # Check if goal is reached\n",
    "        if self.car_position == self.goal:\n",
    "            reward += 500\n",
    "            # Additional penalty if it takes too many steps to reach the goal\n",
    "            if self.steps > 1000:\n",
    "                reward -= 200\n",
    "            return reward  # Return immediately as this is the terminal state\n",
    "\n",
    "        # Calculate the Euclidean distance to the goal\n",
    "        distance_to_goal = ((self.car_position[0] - self.goal[0]) ** 2 + (self.car_position[1] - self.goal[1]) ** 2) ** 0.5\n",
    "\n",
    "        # Define a maximum reward when the car is at the goal\n",
    "        max_reward_at_goal = 50\n",
    "\n",
    "        # Reward based on proximity to the goal\n",
    "        reward += max_reward_at_goal / (distance_to_goal + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "        # # Reward or penalize based on movement towards or away from the goal\n",
    "        if distance_to_goal < self.previous_distance:\n",
    "            reward += 50  # Positive reward for moving closer to the goal\n",
    "        elif distance_to_goal > self.previous_distance:\n",
    "            reward -= 25  # Negative reward for moving farther from the goal\n",
    "\n",
    "        if self.car_position in self.visited_positions:\n",
    "            # Apply a penalty for revisiting the same position\n",
    "            reward -= 10\n",
    "            \n",
    "        # Penalize for each step taken to encourage efficiency\n",
    "        reward -= 2\n",
    "        \n",
    "        # Update the previous_distance for the next step\n",
    "        self.previous_distance = distance_to_goal\n",
    "        return reward\n",
    "\n",
    "    def is_done(self):\n",
    "        #is done if it reaches the goal or goes out of bounds or takes more than 3000 steps\n",
    "        return self.car_position == self.goal or self.steps > 3000 or self.car_position[0] < 0 or self.car_position[1] < 0 or self.car_position[0] > 11 or self.car_position[1] > 11\n",
    "        \n",
    "    def get_state(self):\n",
    "        car_position = [float(coord) for coord in self.car_position]\n",
    "        sensor_readings = [float(value) for value in self.sensor_readings.values()]\n",
    "        \n",
    "        state = car_position + [self.car_orientation] + sensor_readings\n",
    "        \n",
    "        # cast state to this ['1.0' '1.0' 'N' '1.0' '1.0' '10.0']\n",
    "        state = np.array(state, dtype=str)\n",
    "        \n",
    "        #get the orientation and convert do label encoding\n",
    "        if state[2] == 'N':\n",
    "            state[2] = 0\n",
    "        elif state[2] == 'E':\n",
    "            state[2] = 1\n",
    "        elif state[2] == 'S':\n",
    "            state[2] = 2\n",
    "        elif state[2] == 'W':\n",
    "            state[2] = 3\n",
    "            \n",
    "        state = np.array(state, dtype=float)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    \n",
    "    def init_pygame(self):\n",
    "        # Initialize Pygame and set up the display\n",
    "        pygame.init()\n",
    "        self.cell_size = 40  # Size of each cell in pixels\n",
    "        self.maze_size_x = 12  # Assuming the maze size_x is 12\n",
    "        self.maze_size_y = 12  # Assuming the maze size_y is 12\n",
    "        self.width = 600\n",
    "        self.height = 600\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def render(self, framerate=60, delay=0):\n",
    "        # Render the environment using Pygame\n",
    "        for y in range(self.maze_size_y):\n",
    "            for x in range(self.maze_size_x):\n",
    "                rect = pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "                if (x, y) == self.goal:  # Goal position\n",
    "                    color = (0, 255, 0)  # Green color for the goal\n",
    "                elif self.maze[y][x] == 0:\n",
    "                    color = (255, 255, 255)  # White color for empty space\n",
    "                else:\n",
    "                    color = (0, 0, 0)  # Black color for walls\n",
    "                pygame.draw.rect(self.screen, color, rect)\n",
    "        \n",
    "        # Draw the car\n",
    "        car_x, car_y = self.car_position\n",
    "        car_rect = pygame.Rect(car_x * self.cell_size, car_y * self.cell_size, self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), car_rect)  # Red color for the car\n",
    "        pygame.time.delay(delay)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(framerate)  # Limit the frame rate to 60 FPS\n",
    "\n",
    "\n",
    "    def close_pygame(self):\n",
    "        # Close the Pygame window\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAgent:\n",
    "    def __init__(self, replayCapacity, input_shape, output_shape, learning_rate=0.001, discount_factor=0.90):\n",
    "        self.capacity = replayCapacity\n",
    "        self.memory = collections.deque(maxlen=self.capacity)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.policy_model = self.buildNetwork()\n",
    "        self.target_model = self.buildNetwork()\n",
    "        self.target_model.set_weights(self.policy_model.get_weights())\n",
    "\n",
    "\n",
    "    def addToReplayMemory(self, step):\n",
    "        self.step = step\n",
    "        self.memory.append(self.step)\n",
    "\n",
    "    def sampleFromReplayMemory(self, batchSize):\n",
    "        self.batchSize = batchSize\n",
    "        if self.batchSize > len(self.memory):\n",
    "            self.populated = False\n",
    "            return self.populated\n",
    "        else:\n",
    "            return random.sample(self.memory, self.batchSize)\n",
    "\n",
    "\n",
    "    def buildNetwork(self):\n",
    "      model = Sequential()\n",
    "      model.add(Dense(32, input_shape=self.input_shape, activation='relu'))\n",
    "      model.add(Dense(64, activation='relu'))\n",
    "      model.add(Dense(32, activation='relu'))\n",
    "      model.add(Dense(self.output_shape, activation='linear'))\n",
    "      model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate), metrics=['MeanSquaredError'])\n",
    "      return model\n",
    "\n",
    "    def policy_network_fit(self, batch, batch_size):\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "\n",
    "            # Predict Q-values for starting state using the policy network\n",
    "            q_values = self.policy_model.predict(states)\n",
    "\n",
    "            # Predict Q-values for next state using the policy network\n",
    "            q_values_next_state_policy = self.policy_model.predict(next_states)\n",
    "\n",
    "            # Select the best action for the next state using the policy network\n",
    "            best_actions = np.argmax(q_values_next_state_policy, axis=1)\n",
    "\n",
    "            # Predict Q-values for next state using the target network\n",
    "            q_values_next_state_target = self.target_model.predict(next_states)\n",
    "\n",
    "            # Update Q-values for actions taken\n",
    "            for i in range(batch_size):\n",
    "                if dones[i]:\n",
    "                    q_values[i, actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    # Double DQN update rule\n",
    "                    q_values[i, actions[i]] = rewards[i] + self.discount_factor * q_values_next_state_target[i, best_actions[i]]\n",
    "\n",
    "            # Train the policy network\n",
    "            self.policy_model.fit(states, q_values, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    def policy_network_predict(self, state):\n",
    "        self.state = state\n",
    "        self.qPolicy = self.policy_model.predict(self.state)\n",
    "        return self.qPolicy\n",
    "\n",
    "    def target_network_predict(self, state):\n",
    "        self.state = state\n",
    "        self.qTarget = self.target_model.predict(self.state)\n",
    "        return self.qTarget\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.policy_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 09:44:56.044079: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-15 09:44:56.044177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-15 09:44:56.044217: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-15 09:44:56.097768: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-15 09:44:56.097842: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-15 09:44:56.097889: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-15 09:44:56.097927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12838 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-01-15 09:44:56.101817: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-01-15 09:44:56.117422: W tensorflow/c/c_api.cc:305] Operation '{name:'count_1/Assign' id:319 op device:{requested: '', assigned: ''} def:{{{node count_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_1, count_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/lucasdriessens/.local/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-01-15 09:44:56.283330: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_6/BiasAdd' id:170 op device:{requested: '', assigned: ''} def:{{{node dense_6/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_6/MatMul, dense_6/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-15 09:44:56.610989: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_10/BiasAdd' id:309 op device:{requested: '', assigned: ''} def:{{{node dense_10/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_10/MatMul, dense_10/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-15 09:44:56.684722: W tensorflow/c/c_api.cc:305] Operation '{name:'loss/mul' id:213 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/dense_6_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-15 09:44:56.696576: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/dense_4/bias/m/Assign' id:534 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_4/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_4/bias/m, training/Adam/dense_4/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -1827.4637301425705 with epsilon =  0.9313105653335852\n",
      "episodeReward for episode  1 =  -606.7575232499382 with epsilon =  0.9011679575628045\n",
      "episodeReward for episode  2 =  -998.566537993184 with epsilon =  0.8689541643267257\n",
      "episodeReward for episode  3 =  -28.47839240663984 with epsilon =  0.8378332559104105\n",
      "episodeReward for episode  4 =  635.1801011180077 with epsilon =  0.826821340956385\n",
      "episodeReward for episode  5 =  -694.1923054619722 with epsilon =  0.8009011443689735\n",
      "episodeReward for episode  6 =  716.5549709272075 with epsilon =  0.7857407121940392\n",
      "episodeReward for episode  7 =  93.72426780518049 with epsilon =  0.7694655036558129\n",
      "episodeReward for episode  8 =  -2035.2855395027577 with epsilon =  0.7116408702101266\n",
      "episodeReward for episode  9 =  841.1499600749269 with epsilon =  0.7055891231065862\n",
      "episodeReward for episode  10 =  310.23357171418337 with epsilon =  0.6867787196310973\n",
      "episodeReward for episode  11 =  641.0463513560799 with epsilon =  0.6773252992447135\n",
      "episodeReward for episode  12 =  795.3873514623358 with epsilon =  0.6695469263695952\n",
      "episodeReward for episode  13 =  944.1090755821045 with epsilon =  0.6645971014910746\n",
      "episodeReward for episode  14 =  720.439782683339 with epsilon =  0.6554490082138951\n",
      "episodeReward for episode  15 =  763.6397455401483 with epsilon =  0.6471512662768713\n",
      "episodeReward for episode  16 =  1022.4149239009633 with epsilon =  0.6427268468439239\n",
      "episodeReward for episode  17 =  1043.6439301440128 with epsilon =  0.6368597779338382\n",
      "episodeReward for episode  18 =  1020.2247455180477 with epsilon =  0.6295461092616982\n",
      "episodeReward for episode  19 =  639.36276160837 with epsilon =  0.6139651828591289\n",
      "episodeReward for episode  20 =  998.8757466264765 with epsilon =  0.6080200634592247\n",
      "episodeReward for episode  21 =  1276.2189453153096 with epsilon =  0.6025119878524501\n",
      "episodeReward for episode  22 =  645.9413360070236 with epsilon =  0.5958429778969392\n",
      "episodeReward for episode  23 =  880.8452460316724 with epsilon =  0.5916450781770765\n",
      "episodeReward for episode  24 =  1060.8824305953115 with epsilon =  0.5892477850515996\n",
      "episodeReward for episode  25 =  849.4209911132912 with epsilon =  0.5845232169213613\n",
      "episodeReward for episode  26 =  1006.8171071384673 with epsilon =  0.5817474009374559\n",
      "episodeReward for episode  27 =  1036.1367299110498 with epsilon =  0.5767598833258519\n",
      "episodeReward for episode  28 =  869.9024970561486 with epsilon =  0.5708552489081975\n",
      "episodeReward for episode  29 =  1033.995530478484 with epsilon =  0.5628398773774911\n",
      "episodeReward for episode  30 =  877.6706916443655 with epsilon =  0.557155749425711\n",
      "episodeReward for episode  31 =  1093.9696094204605 with epsilon =  0.5545098975758453\n",
      "episodeReward for episode  32 =  891.7967972991539 with epsilon =  0.5459209089264541\n",
      "episodeReward for episode  33 =  1053.659171474405 with epsilon =  0.5422265332499155\n",
      "episodeReward for episode  34 =  1111.8970647795327 with epsilon =  0.537314483597889\n",
      "episodeReward for episode  35 =  1052.5433577031997 with epsilon =  0.5310696309944649\n",
      "episodeReward for episode  36 =  1133.3173489255382 with epsilon =  0.5288067192223218\n",
      "episodeReward for episode  37 =  1083.5521795548166 with epsilon =  0.5238328601388184\n",
      "episodeReward for episode  38 =  1130.2246394067806 with epsilon =  0.5177446955717802\n",
      "episodeReward for episode  39 =  1103.99997129641 with epsilon =  0.512193189827254\n",
      "episodeReward for episode  40 =  943.872575011235 with epsilon =  0.508762670975566\n",
      "episodeReward for episode  41 =  1087.5104155805293 with epsilon =  0.5060985730847043\n",
      "episodeReward for episode  42 =  1234.929248421062 with epsilon =  0.5034131841545363\n",
      "episodeReward for episode  43 =  920.3286961104862 with epsilon =  0.4994117936775839\n",
      "episodeReward for episode  44 =  1091.7281039213185 with epsilon =  0.49384939448515486\n",
      "episodeReward for episode  45 =  719.7345893403697 with epsilon =  0.48735857128876847\n",
      "episodeReward for episode  46 =  1159.3034718872182 with epsilon =  0.485485782672391\n",
      "episodeReward for episode  47 =  1095.3900105772123 with epsilon =  0.48071744157787194\n",
      "episodeReward for episode  48 =  1095.6676701509118 with epsilon =  0.47840109374525586\n",
      "episodeReward for episode  49 =  1117.950682896758 with epsilon =  0.4741004280735177\n",
      "episodeReward for episode  50 =  1266.849951181047 with epsilon =  0.47158482319129746\n",
      "episodeReward for episode  51 =  1096.4375849062053 with epsilon =  0.46914824471264305\n",
      "episodeReward for episode  52 =  916.8299330179043 with epsilon =  0.46541920984077173\n",
      "episodeReward for episode  53 =  1229.0244081849255 with epsilon =  0.46217253834498806\n",
      "episodeReward for episode  54 =  1259.7201720950625 with epsilon =  0.4603643051584046\n",
      "episodeReward for episode  55 =  1190.8116310518678 with epsilon =  0.45788953044498454\n",
      "episodeReward for episode  56 =  1067.3418104220568 with epsilon =  0.45533242614962083\n",
      "episodeReward for episode  57 =  1217.2534288165773 with epsilon =  0.45282129960704526\n",
      "episodeReward for episode  58 =  1229.264706222471 with epsilon =  0.4509233721581523\n",
      "episodeReward for episode  59 =  1028.162309367981 with epsilon =  0.44737051401212313\n",
      "episodeReward for episode  60 =  1195.1329590715686 with epsilon =  0.4444674899550712\n",
      "episodeReward for episode  61 =  1188.9596488541622 with epsilon =  0.4429765332147918\n",
      "episodeReward for episode  62 =  1228.231721317085 with epsilon =  0.4409963679216592\n",
      "episodeReward for episode  63 =  1134.5195778749671 with epsilon =  0.4385336042381213\n",
      "episodeReward for episode  64 =  1049.4728081692456 with epsilon =  0.43577943089001653\n",
      "episodeReward for episode  65 =  1160.518451704348 with epsilon =  0.43325480517335885\n",
      "episodeReward for episode  66 =  1265.908388192077 with epsilon =  0.4318619196982962\n",
      "episodeReward for episode  67 =  1118.8374192637161 with epsilon =  0.42957043483705587\n",
      "episodeReward for episode  68 =  1137.1277920890675 with epsilon =  0.4270817800497233\n",
      "episodeReward for episode  69 =  1175.6220202145018 with epsilon =  0.4249941251903063\n",
      "episodeReward for episode  70 =  1185.4008645808908 with epsilon =  0.42342026321928994\n",
      "episodeReward for episode  71 =  1243.134291602884 with epsilon =  0.4217046020507213\n",
      "episodeReward for episode  72 =  1224.831844230446 with epsilon =  0.41987830608567495\n",
      "episodeReward for episode  73 =  1033.5795656568293 with epsilon =  0.4177673695679722\n",
      "episodeReward for episode  74 =  1248.1334417535522 with epsilon =  0.41525987736442455\n",
      "episodeReward for episode  75 =  1127.1785641781562 with epsilon =  0.4136351882515754\n",
      "episodeReward for episode  76 =  1184.80558511542 with epsilon =  0.4116420777693442\n",
      "episodeReward for episode  77 =  1224.9808837543571 with epsilon =  0.4097159293574189\n",
      "episodeReward for episode  78 =  1230.2025709875384 with epsilon =  0.40825580026574765\n",
      "episodeReward for episode  79 =  1107.735824565796 with epsilon =  0.4054930368807113\n",
      "episodeReward for episode  80 =  1072.9994867695182 with epsilon =  0.40328500104064186\n",
      "episodeReward for episode  81 =  1089.8473507835374 with epsilon =  0.40136985905902145\n",
      "episodeReward for episode  82 =  1180.9742522258236 with epsilon =  0.3997435441628334\n",
      "episodeReward for episode  83 =  1168.4427491037504 with epsilon =  0.39762248129980626\n",
      "episodeReward for episode  84 =  1265.5956329188289 with epsilon =  0.3962609249380072\n",
      "episodeReward for episode  85 =  1174.5574891682968 with epsilon =  0.3945724391735257\n",
      "episodeReward for episode  86 =  1067.3702564672976 with epsilon =  0.3921218087448058\n",
      "episodeReward for episode  87 =  1167.4280259130112 with epsilon =  0.39072438087002137\n",
      "episodeReward for episode  88 =  1256.586648926651 with epsilon =  0.3888416666298059\n",
      "episodeReward for episode  89 =  1172.917695749437 with epsilon =  0.38612918478821656\n",
      "episodeReward for episode  90 =  1175.6887156703297 with epsilon =  0.38434932031140706\n",
      "episodeReward for episode  91 =  1188.244770361558 with epsilon =  0.3826044424400913\n",
      "episodeReward for episode  92 =  1105.1888512477778 with epsilon =  0.3805476804609732\n",
      "episodeReward for episode  93 =  1271.3038056276114 with epsilon =  0.3786344785075285\n",
      "episodeReward for episode  94 =  1227.5995960784617 with epsilon =  0.3771002853470083\n",
      "episodeReward for episode  95 =  1125.9463355477803 with epsilon =  0.3757563905709952\n",
      "episodeReward for episode  96 =  1117.879754947172 with epsilon =  0.3735794997413921\n",
      "episodeReward for episode  97 =  1184.0531640147924 with epsilon =  0.37180542487726054\n",
      "episodeReward for episode  98 =  1273.6160217690942 with epsilon =  0.3702989024334001\n",
      "episodeReward for episode  99 =  1011.3501191155706 with epsilon =  0.36799903131233147\n",
      "episodeReward for episode  100 =  1078.3508564437861 with epsilon =  0.3661745498611769\n",
      "episodeReward for episode  101 =  1256.1586775191038 with epsilon =  0.36515065940059127\n",
      "episodeReward for episode  102 =  1162.6248433413484 with epsilon =  0.3630352113554771\n",
      "episodeReward for episode  103 =  1201.1300321172293 with epsilon =  0.3616148493421431\n",
      "episodeReward for episode  104 =  1201.301996261051 with epsilon =  0.35999838183098154\n",
      "episodeReward for episode  105 =  1293.2935977686536 with epsilon =  0.3583138836979993\n",
      "episodeReward for episode  106 =  1272.9809126624054 with epsilon =  0.35673714355183617\n",
      "episodeReward for episode  107 =  1167.3766269314847 with epsilon =  0.35529167644092235\n",
      "episodeReward for episode  108 =  1243.0432210680608 with epsilon =  0.35405028582507525\n",
      "episodeReward for episode  109 =  1211.3556270473186 with epsilon =  0.3524676329406739\n",
      "episodeReward for episode  110 =  1211.667348699122 with epsilon =  0.35111319411575026\n",
      "episodeReward for episode  111 =  1135.689971611876 with epsilon =  0.34954367043036855\n",
      "episodeReward for episode  112 =  1129.6583700001897 with epsilon =  0.347591628372962\n",
      "episodeReward for episode  113 =  1199.1719619781104 with epsilon =  0.3461347541202118\n",
      "episodeReward for episode  114 =  1189.56835409235 with epsilon =  0.3447322469463202\n",
      "episodeReward for episode  115 =  1167.5448349908402 with epsilon =  0.3432392987817575\n",
      "episodeReward for episode  116 =  1166.1757229265945 with epsilon =  0.3418245943381739\n",
      "episodeReward for episode  117 =  1224.4558294730928 with epsilon =  0.340511053863485\n",
      "episodeReward for episode  118 =  1263.7552331685908 with epsilon =  0.3393688167766988\n",
      "episodeReward for episode  119 =  1295.7921212701424 with epsilon =  0.3382067351711314\n",
      "episodeReward for episode  120 =  1248.4803033853518 with epsilon =  0.33688351366460817\n",
      "episodeReward for episode  121 =  1177.99300317368 with epsilon =  0.3356124533128933\n",
      "episodeReward for episode  122 =  1241.6434365169764 with epsilon =  0.3339251630356255\n",
      "episodeReward for episode  123 =  1243.5310073147812 with epsilon =  0.3321068365520059\n",
      "episodeReward for episode  124 =  1235.2760130569586 with epsilon =  0.3307843241525311\n",
      "episodeReward for episode  125 =  1249.8161836220736 with epsilon =  0.32976704025022363\n",
      "episodeReward for episode  126 =  1242.964191681414 with epsilon =  0.32852283032850665\n",
      "episodeReward for episode  127 =  1155.6462244968566 with epsilon =  0.32696272305285967\n",
      "episodeReward for episode  128 =  1161.6277381331072 with epsilon =  0.3258659335535759\n",
      "episodeReward for episode  129 =  1147.513402726982 with epsilon =  0.3243865599045928\n",
      "episodeReward for episode  130 =  1232.9744937984751 with epsilon =  0.3226427590780952\n",
      "episodeReward for episode  131 =  1282.94486933337 with epsilon =  0.3217180695592262\n",
      "episodeReward for episode  132 =  1225.2371367479436 with epsilon =  0.32057154368844387\n",
      "episodeReward for episode  133 =  1269.425600762891 with epsilon =  0.31965279024316823\n",
      "episodeReward for episode  134 =  1199.6421194925133 with epsilon =  0.318535922045995\n",
      "episodeReward for episode  135 =  1251.934420228523 with epsilon =  0.3176452378297914\n",
      "episodeReward for episode  136 =  1236.6020479823314 with epsilon =  0.3166240294491506\n",
      "episodeReward for episode  137 =  1239.1342408303344 with epsilon =  0.3151866124086703\n",
      "episodeReward for episode  138 =  1233.937264265272 with epsilon =  0.314063363081252\n",
      "episodeReward for episode  139 =  1256.7011428327146 with epsilon =  0.3126156863827861\n",
      "episodeReward for episode  140 =  1276.822956936997 with epsilon =  0.31156702371281986\n",
      "episodeReward for episode  141 =  1210.1890630342518 with epsilon =  0.31017427678249543\n",
      "episodeReward for episode  142 =  1293.9680625853282 with epsilon =  0.3088093722762488\n",
      "episodeReward for episode  143 =  1224.5893312967228 with epsilon =  0.30779502345762644\n",
      "episodeReward for episode  144 =  1257.2041245510823 with epsilon =  0.30680548286315074\n",
      "episodeReward for episode  145 =  1228.278708097658 with epsilon =  0.3058405324133515\n",
      "episodeReward for episode  146 =  912.0882904662574 with epsilon =  0.3038984523405056\n",
      "episodeReward for episode  147 =  1199.983510079714 with epsilon =  0.30262471098851174\n",
      "episodeReward for episode  148 =  1180.6458223171214 with epsilon =  0.3010821892262279\n",
      "episodeReward for episode  149 =  1208.2359178955573 with epsilon =  0.29982025178688676\n",
      "episodeReward for episode  150 =  1224.4537061064475 with epsilon =  0.29862631069034545\n",
      "episodeReward for episode  151 =  1209.326471789173 with epsilon =  0.2973538504431505\n",
      "episodeReward for episode  152 =  1197.7270549683305 with epsilon =  0.2961489991432837\n",
      "episodeReward for episode  153 =  1291.777082723855 with epsilon =  0.2952382316506438\n",
      "episodeReward for episode  154 =  1246.96504021811 with epsilon =  0.294309661985687\n",
      "episodeReward for episode  155 =  1244.5443474494646 with epsilon =  0.29340455113377856\n",
      "episodeReward for episode  156 =  1230.9094537113695 with epsilon =  0.2922566165369837\n",
      "episodeReward for episode  157 =  1269.230005453518 with epsilon =  0.291398613935788\n",
      "episodeReward for episode  158 =  1144.7324173249372 with epsilon =  0.290156951232111\n",
      "episodeReward for episode  159 =  1221.636600465417 with epsilon =  0.2892848614389999\n",
      "episodeReward for episode  160 =  1189.7195445916745 with epsilon =  0.2882740989131381\n",
      "episodeReward for episode  161 =  1194.961649017491 with epsilon =  0.28700556491114965\n",
      "episodeReward for episode  162 =  1305.4679553905116 with epsilon =  0.2860428108488778\n",
      "episodeReward for episode  163 =  1189.1470077522927 with epsilon =  0.2851232021758291\n",
      "episodeReward for episode  164 =  1216.369462632621 with epsilon =  0.2840474333033858\n",
      "episodeReward for episode  165 =  1280.069465293765 with epsilon =  0.2830747856085103\n",
      "episodeReward for episode  166 =  1231.8333618086617 with epsilon =  0.2821449674095118\n",
      "episodeReward for episode  167 =  1169.8696277567558 with epsilon =  0.2809427349131699\n",
      "episodeReward for episode  168 =  1228.7091321126227 with epsilon =  0.2800591267850088\n",
      "episodeReward for episode  169 =  1232.0092826980194 with epsilon =  0.279139214146656\n"
     ]
    }
   ],
   "source": [
    "env = RCMazeEnv()\n",
    "state = env.reset()\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "# Model parameters\n",
    "REPLAY_MEMORY_CAPACITY = 20000\n",
    "# MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "POSSIBLE_ACTIONS = env.possible_actions\n",
    "\n",
    "# state = state[0]\n",
    "# create DQN agent\n",
    "agent = DQAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, \n",
    "                input_shape=state.shape, \n",
    "                output_shape=len(POSSIBLE_ACTIONS),\n",
    "                learning_rate=0.001, \n",
    "                discount_factor=0.90)\n",
    "\n",
    "\n",
    "# reset the parameters\n",
    "DISCOUNT = 0.90\n",
    "BATCH_SIZE = 128  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_INTERVAL = 2\n",
    "EPSILON = 0.99 # Exploration percentage\n",
    "MIN_EPSILON = 0.01\n",
    "DECAY = 0.99993\n",
    "EPISODE_AMOUNT = 170\n",
    "\n",
    "\n",
    "\n",
    "# Fill the replay memory with the first batch of samples\n",
    "update_counter = 0\n",
    "reward_history = []\n",
    "epsilon_history = []\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "for episode in range(EPISODE_AMOUNT):\n",
    "    episode_reward = 0\n",
    "    step_counter = 0  # count the number of successful steps within the episode\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    epsilon_history.append(EPSILON)\n",
    "    \n",
    "    # early stopping\n",
    "    # if len(reward_history) > 10:\n",
    "    #     last_10_rewards = reward_history[-10:]\n",
    "    #     if all(reward > 0 for reward in last_10_rewards):\n",
    "    #         differences = [abs(last_10_rewards[i] - last_10_rewards[i-1]) for i in range(1, 10)]\n",
    "    #         if all(diff < 200 for diff in differences):\n",
    "    #             print('The difference between each of the last 10 positive rewards is less than 200, stopping training')\n",
    "    #             break\n",
    "    while not done:\n",
    "        env.render(delay=0, framerate=360)\n",
    "\n",
    "        if random.random() <= EPSILON:\n",
    "            action = random.sample(POSSIBLE_ACTIONS, 1)[0]\n",
    "        else:\n",
    "            qValues = agent.policy_network_predict(state.reshape(1,-1))\n",
    "            action = np.argmax(qValues[0])\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        step_counter +=1\n",
    "\n",
    "        # store step in replay memory\n",
    "        step = (state, action, reward, new_state, done)\n",
    "        agent.addToReplayMemory(step)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        # When enough steps in replay memory -> train policy network\n",
    "        if len(agent.memory) >= (BATCH_SIZE):\n",
    "            EPSILON = DECAY * EPSILON\n",
    "            if EPSILON < MIN_EPSILON:\n",
    "                EPSILON = MIN_EPSILON\n",
    "            # sample minibatch from replay memory\n",
    "            \n",
    "            miniBatch = agent.sampleFromReplayMemory(BATCH_SIZE)\n",
    "            miniBatch_states = np.asarray(list(zip(*miniBatch))[0],dtype=float)\n",
    "            miniBatch_actions = np.asarray(list(zip(*miniBatch))[1], dtype = int)\n",
    "            miniBatch_rewards = np.asarray(list(zip(*miniBatch))[2], dtype = float)\n",
    "            miniBatch_next_state = np.asarray(list(zip(*miniBatch))[3],dtype=float)\n",
    "            miniBatch_done = np.asarray(list(zip(*miniBatch))[4],dtype=bool)\n",
    "            \n",
    "            # current state q values1tch_states)\n",
    "            y = agent.policy_network_predict(miniBatch_states)\n",
    "\n",
    "            next_state_q_values = agent.target_network_predict(miniBatch_next_state)\n",
    "            max_q_next_state = np.max(next_state_q_values,axis=1)\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                if miniBatch_done[i]:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i]\n",
    "                else:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i] + DISCOUNT *  max_q_next_state[i]\n",
    "                    \n",
    "            agent.policy_model.fit(miniBatch_states, y, batch_size=BATCH_SIZE, verbose = 0)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        if update_counter == UPDATE_TARGET_INTERVAL:\n",
    "            agent.update_target_network()\n",
    "            update_counter = 0\n",
    "        update_counter += 1\n",
    "    print('episodeReward for episode ', episode, '= ', episode_reward, 'with epsilon = ', EPSILON)\n",
    "    reward_history.append(episode_reward)\n",
    "    \n",
    "\n",
    "env.close_pygame()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the reward history\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward History for Double DQN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucasdriessens/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "agent.policy_model.save('./models/DDQN_RCmaze.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 09:48:42.285398: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_21/kernel/Assign' id:1492 op device:{requested: '', assigned: ''} def:{{{node dense_21/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_21/kernel, dense_21/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-15 09:48:42.374414: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_6_1/kernel/Assign' id:1839 op device:{requested: '', assigned: ''} def:{{{node dense_6_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_6_1/kernel, dense_6_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-15 09:48:42.452782: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_5_1/kernel/m/Assign' id:1978 op device:{requested: '', assigned: ''} def:{{{node dense_5_1/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_5_1/kernel/m, dense_5_1/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-15 09:48:42.619322: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_6_1/BiasAdd' id:1849 op device:{requested: '', assigned: ''} def:{{{node dense_6_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_6_1/MatMul, dense_6_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in  34 steps\n",
      "1296.992428778307\n"
     ]
    }
   ],
   "source": [
    "# try it out\n",
    "# load model\n",
    "env = RCMazeEnv()\n",
    "state = env.reset()\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "REPLAY_MEMORY_CAPACITY = 20000\n",
    "POSSIBLE_ACTIONS = env.possible_actions\n",
    "\n",
    "# create DQN agent\n",
    "test_agent = DQAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, input_shape=state.shape, output_shape=len(POSSIBLE_ACTIONS))\n",
    "\n",
    "test_agent.policy_model = load_model('./models/DDQN_RCmaze.h5')\n",
    "\n",
    "done = False\n",
    "\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    env.render(delay=100, framerate=60)\n",
    "   \n",
    "    qValues = test_agent.policy_network_predict(np.array([state]))\n",
    "    action = np.argmax(qValues[0])\n",
    "    state, reward, done = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    env.render()\n",
    "    if done:\n",
    "        print('done in ', len(rewards), 'steps')\n",
    "        break\n",
    "env.close()\n",
    "print(sum(rewards))\n",
    "env.close_pygame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training with failsafe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCMazeEnv(gym.Env):\n",
    "    def __init__(self, maze_size_x=12, maze_size_y=12):\n",
    "        self.maze_size_x = maze_size_x\n",
    "        self.maze_size_y = maze_size_y\n",
    "        self.maze = self.generate_maze()\n",
    "        self.car_position = (1, 1)\n",
    "        self.possible_actions = range(3)\n",
    "        self.car_orientation = 'E'\n",
    "        self.sensor_readings = {'front': 0, 'left': 0, 'right': 0}\n",
    "        self.steps = 0\n",
    "        self.previous_distance = 0\n",
    "        self.goal = (10, 10)\n",
    "        self.previous_steps = 0\n",
    "        self.visited_positions = set()\n",
    "        self.reset()\n",
    "\n",
    "            \n",
    "    def generate_maze(self):\n",
    "        # For simplicity, create a static maze with walls\n",
    "        # '1' represents a wall, and '0' represents an open path\n",
    "        maze = np.zeros((self.maze_size_y, self.maze_size_x), dtype=int)\n",
    "        # Add walls to the maze (this can be customized)\n",
    "\n",
    "        \n",
    "        layout = [\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
    "            [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "            [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
    "            [1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
    "            [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "        \n",
    "     \n",
    "        maze = np.array(layout)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    def reset(self):\n",
    "        self.car_position = (1, 1)\n",
    "        self.car_orientation = 'E'\n",
    "        self.update_sensor_readings()\n",
    "        self.steps = 0\n",
    "        self.previous_distance = 0\n",
    "        self.previous_steps = 0\n",
    "        self.visited_positions.clear()  # Clear the visited positions\n",
    "        self.visited_positions.add(self.car_position)\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        if action == 0:\n",
    "            self.move_forward()\n",
    "        elif action == 1:  # Turn left\n",
    "            self.turn_left()\n",
    "        elif action == 2:  # Turn right\n",
    "            self.turn_right()\n",
    "            \n",
    "        self.update_sensor_readings()\n",
    "        self.visited_positions.add(self.car_position)\n",
    "        reward = self.compute_reward()\n",
    "        self.steps += 1\n",
    "        done = self.is_done()\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    \n",
    "    def move_forward(self):\n",
    "        x, y = self.car_position\n",
    "        \n",
    "        # Check sensor reading in the direction of car's orientation\n",
    "        if self.sensor_readings['front'] <= 4:\n",
    "            # If the sensor reading is 4 or less, do not move forward\n",
    "            return\n",
    "        \n",
    "        if self.car_orientation == 'N' and y > 0 and self.maze[y - 1][x] != 1:\n",
    "            self.car_position = (x, y - 1)\n",
    "        elif self.car_orientation == 'S' and y < self.maze_size_y - 1 and self.maze[y + 1][x] != 1:\n",
    "            self.car_position = (x, y + 1)\n",
    "        elif self.car_orientation == 'E' and x < self.maze_size_x - 1 and self.maze[y][x + 1] != 1:\n",
    "            self.car_position = (x + 1, y)\n",
    "        elif self.car_orientation == 'W' and x > 0 and self.maze[y][x - 1] != 1:\n",
    "            self.car_position = (x - 1, y)\n",
    "        \n",
    "\n",
    "    def turn_left(self):\n",
    "        orientations = ['N', 'W', 'S', 'E']\n",
    "        idx = orientations.index(self.car_orientation)\n",
    "        self.car_orientation = orientations[(idx + 1) % 4]\n",
    "\n",
    "    def turn_right(self):\n",
    "        orientations = ['N', 'E', 'S', 'W']\n",
    "        idx = orientations.index(self.car_orientation)\n",
    "        self.car_orientation = orientations[(idx + 1) % 4]\n",
    "\n",
    "    def update_sensor_readings(self):\n",
    "        # Simple sensor implementation: counts steps to the nearest wall\n",
    "        self.sensor_readings['front'] = self.distance_to_wall('front')\n",
    "        self.sensor_readings['left'] = self.distance_to_wall('left')\n",
    "        self.sensor_readings['right'] = self.distance_to_wall('right')\n",
    "\n",
    "    def distance_to_wall(self, direction):\n",
    "        x, y = self.car_position\n",
    "        sensor_max_range = 255  # Maximum range of the ultrasonic sensor\n",
    "\n",
    "        def calculate_distance(dx, dy):\n",
    "            distance = 0\n",
    "            while 0 <= x + distance * dx < self.maze_size_x and \\\n",
    "                0 <= y + distance * dy < self.maze_size_y and \\\n",
    "                self.maze[y + distance * dy][x + distance * dx] != 1:\n",
    "                distance += 1\n",
    "                if distance > sensor_max_range:  # Limiting the sensor range\n",
    "                    break\n",
    "            return distance\n",
    "\n",
    "        if direction == 'front':\n",
    "            if self.car_orientation == 'N':\n",
    "                distance = calculate_distance(0, -1)\n",
    "            elif self.car_orientation == 'S':\n",
    "                distance = calculate_distance(0, 1)\n",
    "            elif self.car_orientation == 'E':\n",
    "                distance = calculate_distance(1, 0)\n",
    "            elif self.car_orientation == 'W':\n",
    "                distance = calculate_distance(-1, 0)\n",
    "\n",
    "        elif direction == 'left':\n",
    "            if self.car_orientation == 'N':\n",
    "                distance = calculate_distance(-1, 0)\n",
    "            elif self.car_orientation == 'S':\n",
    "                distance = calculate_distance(1, 0)\n",
    "            elif self.car_orientation == 'E':\n",
    "                distance = calculate_distance(0, -1)\n",
    "            elif self.car_orientation == 'W':\n",
    "                distance = calculate_distance(0, 1)\n",
    "\n",
    "        elif direction == 'right':\n",
    "            if self.car_orientation == 'N':\n",
    "                distance = calculate_distance(1, 0)\n",
    "            elif self.car_orientation == 'S':\n",
    "                distance = calculate_distance(-1, 0)\n",
    "            elif self.car_orientation == 'E':\n",
    "                distance = calculate_distance(0, 1)\n",
    "            elif self.car_orientation == 'W':\n",
    "                distance = calculate_distance(0, -1)\n",
    "\n",
    "        # Normalize the distance to a range of 0-1\n",
    "        normalized_distance = distance / sensor_max_range\n",
    "        normalized_distance = max(0, min(normalized_distance, 1))\n",
    "\n",
    "        return normalized_distance * 1000\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        # Initialize reward\n",
    "        reward = 0\n",
    "\n",
    "        # Check for collision or out of bounds\n",
    "        if any(self.sensor_readings[direction] == 0 for direction in ['front', 'left', 'right']):\n",
    "            reward -= 20\n",
    "\n",
    "        # Check if goal is reached\n",
    "        if self.car_position == self.goal:\n",
    "            reward += 500\n",
    "            # Additional penalty if it takes too many steps to reach the goal\n",
    "            if self.steps > 1000:\n",
    "                reward -= 200\n",
    "            return reward  # Return immediately as this is the terminal state\n",
    "\n",
    "        # Calculate the Euclidean distance to the goal\n",
    "        distance_to_goal = ((self.car_position[0] - self.goal[0]) ** 2 + (self.car_position[1] - self.goal[1]) ** 2) ** 0.5\n",
    "\n",
    "        # Define a maximum reward when the car is at the goal\n",
    "        max_reward_at_goal = 50\n",
    "\n",
    "        # Reward based on proximity to the goal\n",
    "        reward += max_reward_at_goal / (distance_to_goal + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "        # # Reward or penalize based on movement towards or away from the goal\n",
    "        if distance_to_goal < self.previous_distance:\n",
    "            reward += 50  # Positive reward for moving closer to the goal\n",
    "        elif distance_to_goal > self.previous_distance:\n",
    "            reward -= 25  # Negative reward for moving farther from the goal\n",
    "\n",
    "        if self.car_position in self.visited_positions:\n",
    "            # Apply a penalty for revisiting the same position\n",
    "            reward -= 10\n",
    "            \n",
    "        # Penalize for each step taken to encourage efficiency\n",
    "        reward -= 2\n",
    "        \n",
    "        # Update the previous_distance for the next step\n",
    "        self.previous_distance = distance_to_goal\n",
    "        return reward\n",
    "\n",
    "    def is_done(self):\n",
    "        #is done if it reaches the goal or goes out of bounds or takes more than 3000 steps\n",
    "        return self.car_position == self.goal or self.steps > 3000 or self.car_position[0] < 0 or self.car_position[1] < 0 or self.car_position[0] > 11 or self.car_position[1] > 11\n",
    "        \n",
    "    def get_state(self):\n",
    "        car_position = [float(coord) for coord in self.car_position]\n",
    "        sensor_readings = [float(value) for value in self.sensor_readings.values()]\n",
    "        \n",
    "        state = car_position + [self.car_orientation] + sensor_readings\n",
    "        \n",
    "        # cast state to this ['1.0' '1.0' 'N' '1.0' '1.0' '10.0']\n",
    "        state = np.array(state, dtype=str)\n",
    "        \n",
    "        #get the orientation and convert do label encoding\n",
    "        if state[2] == 'N':\n",
    "            state[2] = 0\n",
    "        elif state[2] == 'E':\n",
    "            state[2] = 1\n",
    "        elif state[2] == 'S':\n",
    "            state[2] = 2\n",
    "        elif state[2] == 'W':\n",
    "            state[2] = 3\n",
    "            \n",
    "        state = np.array(state, dtype=float)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    \n",
    "    def init_pygame(self):\n",
    "        # Initialize Pygame and set up the display\n",
    "        pygame.init()\n",
    "        self.cell_size = 40  # Size of each cell in pixels\n",
    "        self.maze_size_x = 12  # Assuming the maze size_x is 12\n",
    "        self.maze_size_y = 12  # Assuming the maze size_y is 12\n",
    "        self.width = 600\n",
    "        self.height = 600\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def render(self,render_mode='human', framerate=60, delay=0):\n",
    "        if render_mode == 'human':\n",
    "            # Render the environment using Pygame\n",
    "            for y in range(self.maze_size_y):\n",
    "                for x in range(self.maze_size_x):\n",
    "                    rect = pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "                    if (x, y) == self.goal:  # Goal position\n",
    "                        color = (0, 255, 0)  # Green color for the goal\n",
    "                    elif self.maze[y][x] == 0:\n",
    "                        color = (255, 255, 255)  # White color for empty space\n",
    "                    else:\n",
    "                        color = (0, 0, 0)  # Black color for walls\n",
    "                    pygame.draw.rect(self.screen, color, rect)\n",
    "            \n",
    "            # Draw the car\n",
    "            car_x, car_y = self.car_position\n",
    "            car_rect = pygame.Rect(car_x * self.cell_size, car_y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(self.screen, (255, 0, 0), car_rect)  # Red color for the car\n",
    "            pygame.time.delay(delay)\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(framerate)  # Limit the frame rate to 60 FPS\n",
    "        elif render_mode == 'rgb_array':\n",
    "            rendered_maze = np.array(self.maze, dtype=str)\n",
    "            x, y = self.car_position\n",
    "            rendered_maze[y][x] = 'C'  # Representing the car\n",
    "            #print array\n",
    "            print(rendered_maze, '\\n')\n",
    "\n",
    "\n",
    "    def close_pygame(self):\n",
    "        # Close the Pygame window\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, replayCapacity, input_shape, output_shape, learning_rate=0.001, discount_factor=0.90):\n",
    "        self.capacity = replayCapacity\n",
    "        self.memory = collections.deque(maxlen=self.capacity)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.policy_model = self.buildNetwork()\n",
    "        self.target_model = self.buildNetwork()\n",
    "        self.target_model.set_weights(self.policy_model.get_weights())\n",
    "\n",
    "\n",
    "    def addToReplayMemory(self, step):\n",
    "        self.step = step\n",
    "        self.memory.append(self.step)\n",
    "\n",
    "    def sampleFromReplayMemory(self, batchSize):\n",
    "        self.batchSize = batchSize\n",
    "        if self.batchSize > len(self.memory):\n",
    "            self.populated = False\n",
    "            return self.populated\n",
    "        else:\n",
    "            return random.sample(self.memory, self.batchSize)\n",
    "\n",
    "\n",
    "    def buildNetwork(self):\n",
    "      model = Sequential()\n",
    "      model.add(Dense(32, input_shape=self.input_shape, activation='relu'))\n",
    "      model.add(Dense(64, activation='relu'))\n",
    "      model.add(Dense(32, activation='relu'))\n",
    "      model.add(Dense(self.output_shape, activation='linear'))\n",
    "      model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate), metrics=['MeanSquaredError'])\n",
    "      return model\n",
    "\n",
    "    def policy_network_fit(self, batch, batch_size):\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "\n",
    "            # Predict Q-values for starting state using the policy network\n",
    "            q_values = self.policy_model.predict(states)\n",
    "\n",
    "            # Predict Q-values for next state using the policy network\n",
    "            q_values_next_state_policy = self.policy_model.predict(next_states)\n",
    "\n",
    "            # Select the best action for the next state using the policy network\n",
    "            best_actions = np.argmax(q_values_next_state_policy, axis=1)\n",
    "\n",
    "            # Predict Q-values for next state using the target network\n",
    "            q_values_next_state_target = self.target_model.predict(next_states)\n",
    "\n",
    "            # Update Q-values for actions taken\n",
    "            for i in range(batch_size):\n",
    "                if dones[i]:\n",
    "                    q_values[i, actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    # Double DQN update rule\n",
    "                    q_values[i, actions[i]] = rewards[i] + self.discount_factor * q_values_next_state_target[i, best_actions[i]]\n",
    "\n",
    "            # Train the policy network\n",
    "            self.policy_model.fit(states, q_values, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    def policy_network_predict(self, state):\n",
    "        self.state = state\n",
    "        self.qPolicy = self.policy_model.predict(self.state)\n",
    "        return self.qPolicy\n",
    "\n",
    "    def target_network_predict(self, state):\n",
    "        self.state = state\n",
    "        self.qTarget = self.target_model.predict(self.state)\n",
    "        return self.qTarget\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.policy_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 10:40:05.541641: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_203/bias/Assign' id:17084 op device:{requested: '', assigned: ''} def:{{{node dense_203/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_203/bias, dense_203/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/lucasdriessens/.local/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-01-17 10:40:06.012541: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_203/BiasAdd' id:17089 op device:{requested: '', assigned: ''} def:{{{node dense_203/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_203/MatMul, dense_203/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 10:40:06.373178: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_207/BiasAdd' id:17228 op device:{requested: '', assigned: ''} def:{{{node dense_207/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_207/MatMul, dense_207/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 10:40:06.681873: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_71/mul' id:17132 op device:{requested: '', assigned: ''} def:{{{node loss_71/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_71/mul/x, loss_71/dense_203_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 10:40:06.850803: W tensorflow/c/c_api.cc:305] Operation '{name:'training_6/Adam/dense_202/bias/v/Assign' id:17501 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/dense_202/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/dense_202/bias/v, training_6/Adam/dense_202/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -8377.18356468399 with epsilon =  0.8219161895924173\n",
      "episodeReward for episode  1 =  -3369.2971397927704 with epsilon =  0.7072704840937897\n",
      "episodeReward for episode  2 =  687.6225152470016 with epsilon =  0.6796525955087557\n",
      "episodeReward for episode  3 =  -2171.563107387079 with epsilon =  0.6495567410638928\n",
      "episodeReward for episode  4 =  1119.3279116817816 with epsilon =  0.6370381298821582\n",
      "episodeReward for episode  5 =  1199.124680706279 with epsilon =  0.6336136118696807\n",
      "episodeReward for episode  6 =  762.7017583840852 with epsilon =  0.6270391372758648\n",
      "episodeReward for episode  7 =  722.019608147666 with epsilon =  0.6186678195423083\n",
      "episodeReward for episode  8 =  1360.345950196871 with epsilon =  0.6136214465809589\n",
      "episodeReward for episode  9 =  1165.6679846522143 with epsilon =  0.6067445164744087\n",
      "episodeReward for episode  10 =  729.3357965446633 with epsilon =  0.5976392301333544\n",
      "episodeReward for episode  11 =  713.0438823497082 with epsilon =  0.5901559575403122\n",
      "episodeReward for episode  12 =  1184.7876742816024 with epsilon =  0.5855060756964425\n",
      "episodeReward for episode  13 =  992.8116094023311 with epsilon =  0.5821547809429455\n",
      "episodeReward for episode  14 =  1039.557517985559 with epsilon =  0.5781342548050187\n",
      "episodeReward for episode  15 =  929.3330814966386 with epsilon =  0.573820055085334\n",
      "episodeReward for episode  16 =  1069.797742963482 with epsilon =  0.5710950668647109\n",
      "episodeReward for episode  17 =  837.823422633178 with epsilon =  0.5663970865517689\n",
      "episodeReward for episode  18 =  1017.4227509677495 with epsilon =  0.562564143747043\n",
      "episodeReward for episode  19 =  1247.668414971423 with epsilon =  0.5578582327287291\n",
      "episodeReward for episode  20 =  880.2683448427475 with epsilon =  0.5545875371135623\n",
      "episodeReward for episode  21 =  1162.2060226043527 with epsilon =  0.5511816595509388\n",
      "episodeReward for episode  22 =  856.2187174498765 with epsilon =  0.5461502518939525\n",
      "episodeReward for episode  23 =  1070.1561054250772 with epsilon =  0.5383686896678345\n",
      "episodeReward for episode  24 =  988.8208170316581 with epsilon =  0.5347628550418662\n",
      "episodeReward for episode  25 =  1109.463941302753 with epsilon =  0.5322978628923394\n",
      "episodeReward for episode  26 =  1030.6651573052109 with epsilon =  0.5279929551518113\n",
      "episodeReward for episode  27 =  800.4078231960061 with epsilon =  0.5229169222547868\n",
      "episodeReward for episode  28 =  1033.7140925091012 with epsilon =  0.5190874394325632\n",
      "episodeReward for episode  29 =  1108.7245466241384 with epsilon =  0.515394226312299\n",
      "episodeReward for episode  30 =  1047.5743235249029 with epsilon =  0.5126236279309111\n",
      "episodeReward for episode  31 =  1249.0185503951498 with epsilon =  0.5091189419951889\n",
      "episodeReward for episode  32 =  1162.6694212149787 with epsilon =  0.5061340024648768\n",
      "episodeReward for episode  33 =  1199.9477122238736 with epsilon =  0.5022164435760489\n",
      "episodeReward for episode  34 =  1156.9965463300641 with epsilon =  0.4995866245181342\n",
      "episodeReward for episode  35 =  1070.8531725366174 with epsilon =  0.4956156495324139\n",
      "episodeReward for episode  36 =  1219.5736192293 with epsilon =  0.4926409021785365\n",
      "episodeReward for episode  37 =  996.772524412587 with epsilon =  0.48650640982178345\n",
      "episodeReward for episode  38 =  1123.9896548081836 with epsilon =  0.483349429718297\n",
      "episodeReward for episode  39 =  1156.8706366365136 with epsilon =  0.47944038772051817\n",
      "episodeReward for episode  40 =  1168.85993500816 with epsilon =  0.477832115138633\n",
      "episodeReward for episode  41 =  1146.561467996349 with epsilon =  0.4756628419284349\n",
      "episodeReward for episode  42 =  1165.415490854717 with epsilon =  0.4731389467859896\n",
      "episodeReward for episode  43 =  1144.546722109636 with epsilon =  0.4704966814519154\n",
      "episodeReward for episode  44 =  1109.5033382781694 with epsilon =  0.4679019250999938\n",
      "episodeReward for episode  45 =  1197.949522677618 with epsilon =  0.4654517914661744\n",
      "episodeReward for episode  46 =  1149.7041106008776 with epsilon =  0.4633062890231195\n",
      "episodeReward for episode  47 =  1236.4833278250117 with epsilon =  0.46146131552498526\n",
      "episodeReward for episode  48 =  1177.2028184552962 with epsilon =  0.4596236890611768\n",
      "episodeReward for episode  49 =  1079.8931126832867 with epsilon =  0.45731293278828056\n",
      "episodeReward for episode  50 =  1145.0543432290247 with epsilon =  0.4553643016507364\n",
      "episodeReward for episode  51 =  1072.6484405253495 with epsilon =  0.4528847012460847\n",
      "episodeReward for episode  52 =  1139.6172375619622 with epsilon =  0.45086024509557465\n",
      "episodeReward for episode  53 =  1171.3992917725582 with epsilon =  0.448342396154538\n",
      "episodeReward for episode  54 =  1194.610731811566 with epsilon =  0.4456201932054553\n",
      "episodeReward for episode  55 =  1395.5231699818087 with epsilon =  0.44344191914822134\n",
      "episodeReward for episode  56 =  1210.4537291644203 with epsilon =  0.44164513201429023\n",
      "episodeReward for episode  57 =  1198.2943557020585 with epsilon =  0.4399172115557078\n",
      "episodeReward for episode  58 =  1048.1483640654992 with epsilon =  0.4372461638454745\n",
      "episodeReward for episode  59 =  1047.314681913685 with epsilon =  0.4351392833449069\n",
      "episodeReward for episode  60 =  1065.1809848982964 with epsilon =  0.432739520575177\n",
      "episodeReward for episode  61 =  1220.9520652154192 with epsilon =  0.43092576266442256\n",
      "episodeReward for episode  62 =  1117.241729447359 with epsilon =  0.42863924510771423\n",
      "episodeReward for episode  63 =  1103.893189446284 with epsilon =  0.42621565313768806\n",
      "episodeReward for episode  64 =  1156.0736206384604 with epsilon =  0.42413223206707024\n",
      "episodeReward for episode  65 =  1108.809512915654 with epsilon =  0.42238411710616114\n",
      "episodeReward for episode  66 =  1107.7352498941314 with epsilon =  0.4205548783332869\n",
      "episodeReward for episode  67 =  1014.4574848712251 with epsilon =  0.4182648260768558\n",
      "episodeReward for episode  68 =  1085.3251225538388 with epsilon =  0.4166575462400893\n",
      "episodeReward for episode  69 =  1167.9441841093173 with epsilon =  0.41531801985214556\n",
      "episodeReward for episode  70 =  1354.9828230468001 with epsilon =  0.4136351882515754\n",
      "episodeReward for episode  71 =  1118.9821637482478 with epsilon =  0.41175735772440564\n",
      "episodeReward for episode  72 =  1167.8044933420342 with epsilon =  0.40988805221538926\n",
      "episodeReward for episode  73 =  1167.1117056435405 with epsilon =  0.408227222359729\n",
      "episodeReward for episode  74 =  1061.7746806636105 with epsilon =  0.4062886104858334\n",
      "episodeReward for episode  75 =  1214.782390442001 with epsilon =  0.40495407076954065\n",
      "episodeReward for episode  76 =  1162.058508438116 with epsilon =  0.4034261804372958\n",
      "episodeReward for episode  77 =  1083.3215779184102 with epsilon =  0.4009486271491753\n",
      "episodeReward for episode  78 =  1104.247990717437 with epsilon =  0.3987932525627208\n",
      "episodeReward for episode  79 =  1145.9827382283631 with epsilon =  0.3969272261490154\n",
      "episodeReward for episode  80 =  1158.6399186786539 with epsilon =  0.39518057015535063\n",
      "episodeReward for episode  81 =  1168.0081284048392 with epsilon =  0.39338652032724275\n",
      "episodeReward for episode  82 =  1227.5652632115095 with epsilon =  0.3921218087448058\n",
      "episodeReward for episode  83 =  1224.9019280771017 with epsilon =  0.3907790880275277\n",
      "episodeReward for episode  84 =  1095.9140896450742 with epsilon =  0.3882432903877528\n",
      "episodeReward for episode  85 =  1121.2531311543153 with epsilon =  0.386561906899777\n",
      "episodeReward for episode  86 =  1082.7233880544668 with epsilon =  0.3843762266472724\n",
      "episodeReward for episode  87 =  1191.6663647490632 with epsilon =  0.3829527831457606\n",
      "episodeReward for episode  88 =  1181.3053224391515 with epsilon =  0.38113419591814673\n",
      "episodeReward for episode  89 =  1166.6949347423379 with epsilon =  0.3794836001201363\n",
      "episodeReward for episode  90 =  1211.629495578442 with epsilon =  0.3783430319796437\n",
      "episodeReward for episode  91 =  1299.590053375623 with epsilon =  0.3765990578145252\n",
      "episodeReward for episode  92 =  1230.4343500318446 with epsilon =  0.37512562774701214\n",
      "episodeReward for episode  93 =  1208.0702193412042 with epsilon =  0.37365796242085664\n",
      "episodeReward for episode  94 =  1181.2877934640044 with epsilon =  0.37250882625452336\n",
      "episodeReward for episode  95 =  1074.5313634098056 with epsilon =  0.37071388407499084\n",
      "episodeReward for episode  96 =  1215.4369445241664 with epsilon =  0.3693927488980565\n",
      "episodeReward for episode  97 =  1136.537193929796 with epsilon =  0.36743272914789404\n",
      "episodeReward for episode  98 =  1192.9922715872742 with epsilon =  0.3664053205898058\n",
      "episodeReward for episode  99 =  1156.6779559307197 with epsilon =  0.36471637336715707\n",
      "episodeReward for episode  100 =  1205.7640078814688 with epsilon =  0.36331486590942996\n",
      "episodeReward for episode  101 =  1218.9961539140968 with epsilon =  0.36169079909322205\n",
      "episodeReward for episode  102 =  1102.319565109643 with epsilon =  0.36052798715321555\n",
      "episodeReward for episode  103 =  1110.5245840134692 with epsilon =  0.35891637806371646\n",
      "episodeReward for episode  104 =  1230.020477677448 with epsilon =  0.35803807855286757\n",
      "episodeReward for episode  105 =  1119.2896475887835 with epsilon =  0.35671217195178756\n",
      "episodeReward for episode  106 =  1238.6953799177722 with epsilon =  0.35564004514966685\n",
      "episodeReward for episode  107 =  1195.9704972754716 with epsilon =  0.35417422945206106\n",
      "episodeReward for episode  108 =  1207.8616585535012 with epsilon =  0.353183894064037\n",
      "episodeReward for episode  109 =  1192.2700337462963 with epsilon =  0.3517774488247763\n",
      "episodeReward for episode  110 =  1126.4768647890805 with epsilon =  0.35020495583058914\n",
      "episodeReward for episode  111 =  1203.8936344014064 with epsilon =  0.3489324825606619\n",
      "episodeReward for episode  112 =  1225.3536784163562 with epsilon =  0.34827359994767054\n",
      "episodeReward for episode  113 =  1184.8129157410667 with epsilon =  0.34742134423806526\n",
      "episodeReward for episode  114 =  1179.3740191205102 with epsilon =  0.34635289535761443\n",
      "episodeReward for episode  115 =  1161.4172759592582 with epsilon =  0.3449012130545224\n",
      "episodeReward for episode  116 =  1121.3089163504258 with epsilon =  0.3435277510216596\n",
      "episodeReward for episode  117 =  1173.590023289085 with epsilon =  0.34247127637762803\n",
      "episodeReward for episode  118 =  1188.6718682031865 with epsilon =  0.34156148150291876\n",
      "episodeReward for episode  119 =  1229.3673025999874 with epsilon =  0.34043955154756933\n",
      "episodeReward for episode  120 =  1167.0654477510493 with epsilon =  0.33910759422847286\n",
      "episodeReward for episode  121 =  1185.2965705440256 with epsilon =  0.3383251323906971\n",
      "episodeReward for episode  122 =  1240.384096490642 with epsilon =  0.33737911395287135\n",
      "episodeReward for episode  123 =  1268.5403785155372 with epsilon =  0.33603560634502166\n",
      "episodeReward for episode  124 =  1162.6638360237487 with epsilon =  0.33474431140258065\n",
      "episodeReward for episode  125 =  1240.6544848301423 with epsilon =  0.3338784151490338\n",
      "episodeReward for episode  126 =  1180.338005127056 with epsilon =  0.3329448305391908\n",
      "episodeReward for episode  127 =  1164.6644559151512 with epsilon =  0.3316886308255173\n",
      "episodeReward for episode  128 =  1294.5621512558982 with epsilon =  0.330853798586797\n",
      "episodeReward for episode  129 =  1200.1316995558243 with epsilon =  0.32985939118234114\n",
      "episodeReward for episode  130 =  1316.301277645239 with epsilon =  0.3289600716975252\n",
      "episodeReward for episode  131 =  1268.842686456647 with epsilon =  0.3276730274243314\n",
      "episodeReward for episode  132 =  1294.1073168136736 with epsilon =  0.3268940656872585\n",
      "episodeReward for episode  133 =  1202.9394649952467 with epsilon =  0.3260484759725319\n",
      "episodeReward for episode  134 =  1175.1332830946235 with epsilon =  0.3252050735779211\n",
      "episodeReward for episode  135 =  1210.013032944479 with epsilon =  0.32422764386572256\n",
      "episodeReward for episode  136 =  1154.3622230266033 with epsilon =  0.32338895148901475\n",
      "episodeReward for episode  137 =  1181.549346808669 with epsilon =  0.32243955104476363\n",
      "episodeReward for episode  138 =  1155.6600714677525 with epsilon =  0.32142542904501914\n",
      "episodeReward for episode  139 =  1164.4059937940317 with epsilon =  0.3203696401555051\n",
      "episodeReward for episode  140 =  1201.0699403894296 with epsilon =  0.31960804041883284\n",
      "episodeReward for episode  141 =  1151.1719276544602 with epsilon =  0.3183575856266598\n",
      "episodeReward for episode  142 =  1160.1031303418085 with epsilon =  0.3170454343297858\n",
      "episodeReward for episode  143 =  1215.564072718067 with epsilon =  0.3162917370718885\n",
      "episodeReward for episode  144 =  1141.4673077262948 with epsilon =  0.31538524949097757\n",
      "episodeReward for episode  145 =  1202.0049909144172 with epsilon =  0.3142392939782841\n",
      "episodeReward for episode  146 =  1179.5582262567805 with epsilon =  0.31303175643354625\n",
      "episodeReward for episode  147 =  1152.127925125027 with epsilon =  0.31217831534485146\n",
      "episodeReward for episode  148 =  1192.5193577151963 with epsilon =  0.3113707914393278\n",
      "episodeReward for episode  149 =  1093.5273198553728 with epsilon =  0.31008743710369446\n",
      "episodeReward for episode  150 =  1239.4958248044168 with epsilon =  0.3091554446502247\n",
      "episodeReward for episode  151 =  1241.8991002114435 with epsilon =  0.3078812211482625\n",
      "episodeReward for episode  152 =  1234.608411884765 with epsilon =  0.3072138199312789\n",
      "episodeReward for episode  153 =  1250.1684713427521 with epsilon =  0.3065907866631289\n",
      "episodeReward for episode  154 =  1124.0839213326049 with epsilon =  0.30534850812095526\n",
      "episodeReward for episode  155 =  1164.3963486263383 with epsilon =  0.30394100259155754\n",
      "episodeReward for episode  156 =  1220.72026009192 with epsilon =  0.3032609129271385\n",
      "episodeReward for episode  157 =  1214.7407740138283 with epsilon =  0.3022436306457333\n",
      "episodeReward for episode  158 =  1200.0975933570116 with epsilon =  0.3011243451590409\n",
      "episodeReward for episode  159 =  1203.8611362690494 with epsilon =  0.30040849636054845\n",
      "episodeReward for episode  160 =  1152.9389027544748 with epsilon =  0.2995684997146343\n",
      "episodeReward for episode  161 =  1066.9627171587028 with epsilon =  0.29764541026227365\n",
      "episodeReward for episode  162 =  1201.728937690615 with epsilon =  0.2966677318063302\n",
      "episodeReward for episode  163 =  1211.5431747220819 with epsilon =  0.29556909529951136\n",
      "episodeReward for episode  164 =  1307.264378457422 with epsilon =  0.2947632639252414\n",
      "episodeReward for episode  165 =  1209.0424426723202 with epsilon =  0.29395962954725474\n",
      "episodeReward for episode  166 =  1203.099264643237 with epsilon =  0.29297354822308896\n",
      "episodeReward for episode  167 =  1210.516391089783 with epsilon =  0.2922361585738261\n",
      "episodeReward for episode  168 =  1061.0503519884526 with epsilon =  0.291133552541076\n",
      "episodeReward for episode  169 =  1114.159290422086 with epsilon =  0.29003510663713905\n"
     ]
    }
   ],
   "source": [
    "env = RCMazeEnv()\n",
    "state = env.reset()\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "# Model parameters\n",
    "REPLAY_MEMORY_CAPACITY = 2000000\n",
    "# MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "POSSIBLE_ACTIONS = env.possible_actions\n",
    "\n",
    "# state = state[0]\n",
    "# create DQN agent\n",
    "agent = DQNAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, \n",
    "                input_shape=state.shape, \n",
    "                output_shape=len(POSSIBLE_ACTIONS),\n",
    "                learning_rate=0.001, \n",
    "                discount_factor=0.90)\n",
    "\n",
    "\n",
    "# reset the parameters\n",
    "DISCOUNT = 0.90\n",
    "BATCH_SIZE = 128  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_INTERVAL = 2\n",
    "EPSILON = 0.99 # Exploration percentage\n",
    "MIN_EPSILON = 0.01\n",
    "DECAY = 0.99993\n",
    "EPISODE_AMOUNT = 170\n",
    "\n",
    "\n",
    "\n",
    "# Fill the replay memory with the first batch of samples\n",
    "update_counter = 0\n",
    "reward_history = []\n",
    "epsilon_history = []\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "for episode in range(EPISODE_AMOUNT):\n",
    "    episode_reward = 0\n",
    "    step_counter = 0  # count the number of successful steps within the episode\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    epsilon_history.append(EPSILON)\n",
    "    \n",
    "    # early stopping\n",
    "    # if len(reward_history) > 10:\n",
    "    #     last_10_rewards = reward_history[-10:]\n",
    "    #     if all(reward > 0 for reward in last_10_rewards):\n",
    "    #         differences = [abs(last_10_rewards[i] - last_10_rewards[i-1]) for i in range(1, 10)]\n",
    "    #         if all(diff < 200 for diff in differences):\n",
    "    #             print('The difference between each of the last 10 positive rewards is less than 200, stopping training')\n",
    "    #             break\n",
    "    while not done:\n",
    "        env.render(delay=0, framerate=360)\n",
    "\n",
    "        if random.random() <= EPSILON:\n",
    "            action = random.sample(POSSIBLE_ACTIONS, 1)[0]\n",
    "        else:\n",
    "            qValues = agent.policy_network_predict(state.reshape(1,-1))\n",
    "            action = np.argmax(qValues[0])\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        step_counter +=1\n",
    "\n",
    "        # store step in replay memory\n",
    "        step = (state, action, reward, new_state, done)\n",
    "        agent.addToReplayMemory(step)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        # When enough steps in replay memory -> train policy network\n",
    "        if len(agent.memory) >= (BATCH_SIZE):\n",
    "            EPSILON = DECAY * EPSILON\n",
    "            if EPSILON < MIN_EPSILON:\n",
    "                EPSILON = MIN_EPSILON\n",
    "            # sample minibatch from replay memory\n",
    "            \n",
    "            miniBatch = agent.sampleFromReplayMemory(BATCH_SIZE)\n",
    "            miniBatch_states = np.asarray(list(zip(*miniBatch))[0],dtype=float)\n",
    "            miniBatch_actions = np.asarray(list(zip(*miniBatch))[1], dtype = int)\n",
    "            miniBatch_rewards = np.asarray(list(zip(*miniBatch))[2], dtype = float)\n",
    "            miniBatch_next_state = np.asarray(list(zip(*miniBatch))[3],dtype=float)\n",
    "            miniBatch_done = np.asarray(list(zip(*miniBatch))[4],dtype=bool)\n",
    "            \n",
    "            # current state q values1tch_states)\n",
    "            y = agent.policy_network_predict(miniBatch_states)\n",
    "\n",
    "            next_state_q_values = agent.target_network_predict(miniBatch_next_state)\n",
    "            max_q_next_state = np.max(next_state_q_values,axis=1)\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                if miniBatch_done[i]:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i]\n",
    "                else:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i] + DISCOUNT *  max_q_next_state[i]\n",
    "                    \n",
    "            agent.policy_model.fit(miniBatch_states, y, batch_size=BATCH_SIZE, verbose = 0)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        if update_counter == UPDATE_TARGET_INTERVAL:\n",
    "            agent.update_target_network()\n",
    "            update_counter = 0\n",
    "        update_counter += 1\n",
    "    print('episodeReward for episode ', episode, '= ', episode_reward, 'with epsilon = ', EPSILON)\n",
    "    reward_history.append(episode_reward)\n",
    "    \n",
    "\n",
    "env.close_pygame()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close_pygame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the reward history\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward History for Double DQN with fail safe and 2M replay memory')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucasdriessens/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "agent.policy_model.save('./models/DDQN_RCmaze_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 14:00:58.880729: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_424/kernel/Assign' id:32090 op device:{requested: '', assigned: ''} def:{{{node dense_424/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_424/kernel, dense_424/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 14:01:00.182020: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_202_3/kernel/Assign' id:32461 op device:{requested: '', assigned: ''} def:{{{node dense_202_3/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_202_3/kernel, dense_202_3/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 14:01:01.298868: W tensorflow/c/c_api.cc:305] Operation '{name:'learning_rate_25/Assign' id:32595 op device:{requested: '', assigned: ''} def:{{{node learning_rate_25/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](learning_rate_25, learning_rate_25/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 14:01:02.376126: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_203_3/BiasAdd' id:32495 op device:{requested: '', assigned: ''} def:{{{node dense_203_3/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_203_3/MatMul, dense_203_3/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in  25 steps\n",
      "1240.5006973297018\n"
     ]
    }
   ],
   "source": [
    "# try it out\n",
    "# load model\n",
    "env = RCMazeEnv()\n",
    "state = env.reset()\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "REPLAY_MEMORY_CAPACITY = 20000\n",
    "POSSIBLE_ACTIONS = env.possible_actions\n",
    "\n",
    "# create DQN agent\n",
    "test_agent = DQNAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, input_shape=state.shape, output_shape=len(POSSIBLE_ACTIONS))\n",
    "\n",
    "test_agent.policy_model = load_model('./models/DDQN_RCmaze_v2.h5')\n",
    "\n",
    "\n",
    "done = False\n",
    "\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    env.render(delay=100, framerate=60)\n",
    "   \n",
    "    qValues = test_agent.policy_network_predict(np.array([state]))\n",
    "    action = np.argmax(qValues[0])\n",
    "    state, reward, done = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    env.render()\n",
    "    if done:\n",
    "        print('done in ', len(rewards), 'steps')\n",
    "        break\n",
    "env.close()\n",
    "print(sum(rewards))\n",
    "env.close_pygame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_200 (Dense)           (None, 32)                224       \n",
      "                                                                 \n",
      " dense_201 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4515 (17.64 KB)\n",
      "Trainable params: 4515 (17.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_agent.policy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucasdriessens/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "test_agent.policy_model.save('./main_web_app/models/DDQN_RCmaze_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent_performance(agent, env, num_eval_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the agent over a specified number of episodes.\n",
    "    \n",
    "    Parameters:\n",
    "        agent: The trained DQN agent.\n",
    "        env: The environment in which to evaluate the agent.\n",
    "        num_eval_episodes (int): Number of episodes to run for the evaluation.\n",
    "        \n",
    "    Returns:\n",
    "        float: The average reward achieved by the agent over the evaluation episodes.\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for episode in range(num_eval_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # For evaluation, choose the best action (no exploration)\n",
    "            q_values = agent.policy_network_predict(state.reshape(1, -1))\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "            state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    average_reward = total_reward / num_eval_episodes\n",
    "    return average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10\n",
      "Learning Rate: 0.06468181818181819, Discount Factor: 0.99, Batch Size: 32, Epsilon Decay: 0.9960183673469388, Min Epsilon: 0.1, Replay Memory Capacity: 15000, Update Target Interval: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 11:32:16.179133: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_324/kernel/Assign' id:24339 op device:{requested: '', assigned: ''} def:{{{node dense_324/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_324/kernel, dense_324/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/lucasdriessens/.local/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-01-17 11:32:16.849519: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_323/BiasAdd' id:24282 op device:{requested: '', assigned: ''} def:{{{node dense_323/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_323/MatMul, dense_323/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:32:17.191661: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_327/BiasAdd' id:24421 op device:{requested: '', assigned: ''} def:{{{node dense_327/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_327/MatMul, dense_327/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:32:17.514183: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_103/mul' id:24325 op device:{requested: '', assigned: ''} def:{{{node loss_103/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_103/mul/x, loss_103/dense_323_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:32:17.796456: W tensorflow/c/c_api.cc:305] Operation '{name:'training_18/Adam/dense_321/kernel/m/Assign' id:24633 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/dense_321/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/dense_321/kernel/m, training_18/Adam/dense_321/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -4186.683873939 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  1 =  -14022.919205623848 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  2 =  -4086.6738477236513 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  3 =  -14300.573130031802 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  4 =  -5624.371420756949 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  5 =  -14451.578469011482 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  6 =  -11357.164240105234 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  7 =  -6607.744494760525 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  8 =  -1994.086775497702 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  9 =  -14579.802293604627 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  10 =  -3490.0712451936233 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  11 =  -178.3732998725701 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  12 =  -10422.989058424211 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  13 =  -14830.208172198338 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  14 =  -4468.2530436004445 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  15 =  -9709.18568060062 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  16 =  -10585.33584862697 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  17 =  -12087.176763202246 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  18 =  -12458.217315693544 with epsilon =  0.09968854039978094\n",
      "episodeReward for episode  19 =  -7125.196986955824 with epsilon =  0.09968854039978094\n",
      "Iteration 2/10\n",
      "Learning Rate: 0.06468181818181819, Discount Factor: 0.98, Batch Size: 32, Epsilon Decay: 0.9984624489795919, Min Epsilon: 0.01, Replay Memory Capacity: 50000, Update Target Interval: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 11:38:28.037914: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_335/bias/Assign' id:25009 op device:{requested: '', assigned: ''} def:{{{node dense_335/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_335/bias, dense_335/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:38:28.705073: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_331/BiasAdd' id:24875 op device:{requested: '', assigned: ''} def:{{{node dense_331/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_331/MatMul, dense_331/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:38:29.052075: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_335/BiasAdd' id:25014 op device:{requested: '', assigned: ''} def:{{{node dense_335/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_335/MatMul, dense_335/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:38:29.380736: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_105/mul' id:24918 op device:{requested: '', assigned: ''} def:{{{node loss_105/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_105/mul/x, loss_105/dense_331_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:38:29.664931: W tensorflow/c/c_api.cc:305] Operation '{name:'training_20/Adam/iter/Assign' id:25189 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_20/Adam/iter, training_20/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -5104.382640510846 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  1 =  -23900.098715664306 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  2 =  -23943.97707612456 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  3 =  -23668.979449372953 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  4 =  -21443.66158236873 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  5 =  -21402.45481706249 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  6 =  -21644.127977027896 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  7 =  -18553.36045517816 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  8 =  -20643.813839884348 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  9 =  -15926.170459264004 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  10 =  -19889.52029961911 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  11 =  -20973.124511759357 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  12 =  -17065.800103621772 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  13 =  -22054.846524859575 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  14 =  -20415.25442765256 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  15 =  -21556.831264923767 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  16 =  -14878.330929850232 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  17 =  -20083.917385144556 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  18 =  -18351.365088205046 with epsilon =  0.00999022247845212\n",
      "episodeReward for episode  19 =  -22237.91402957506 with epsilon =  0.00999022247845212\n",
      "Iteration 3/10\n",
      "Learning Rate: 0.058627272727272736, Discount Factor: 0.89, Batch Size: 64, Epsilon Decay: 0.995, Min Epsilon: 0.08, Replay Memory Capacity: 15000, Update Target Interval: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 11:48:23.066352: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_337/bias/Assign' id:25415 op device:{requested: '', assigned: ''} def:{{{node dense_337/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_337/bias, dense_337/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:48:23.712236: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_339/BiasAdd' id:25468 op device:{requested: '', assigned: ''} def:{{{node dense_339/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_339/MatMul, dense_339/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:48:24.122881: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_343/BiasAdd' id:25607 op device:{requested: '', assigned: ''} def:{{{node dense_343/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_343/MatMul, dense_343/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:48:24.457690: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_107/mul' id:25511 op device:{requested: '', assigned: ''} def:{{{node loss_107/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_107/mul/x, loss_107/dense_339_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:48:24.746018: W tensorflow/c/c_api.cc:305] Operation '{name:'training_22/Adam/dense_336/kernel/v/Assign' id:25851 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/dense_336/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/dense_336/kernel/v, training_22/Adam/dense_336/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -13039.17077473326 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  1 =  -19069.19092833686 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  2 =  -12285.397013227568 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  3 =  -9923.261100696003 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  4 =  -10241.108246700642 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  5 =  -5393.040669606068 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  6 =  -13916.34661160804 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  7 =  -16699.994816092792 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  8 =  -15281.195468288557 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  9 =  -14589.739036577334 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  10 =  -6025.8050632378645 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  11 =  -4320.278904435679 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  12 =  -95.84526170956394 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  13 =  -10604.189940846536 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  14 =  -7778.06001167062 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  15 =  -13421.315443823589 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  16 =  -3956.1237229049448 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  17 =  -8316.531967604935 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  18 =  -16736.88588059254 with epsilon =  0.07983945381656987\n",
      "episodeReward for episode  19 =  -14436.515950963088 with epsilon =  0.07983945381656987\n",
      "Iteration 4/10\n",
      "Learning Rate: 0.009181818181818182, Discount Factor: 0.88, Batch Size: 256, Epsilon Decay: 0.9996844897959184, Min Epsilon: 0.04000000000000001, Replay Memory Capacity: 45000, Update Target Interval: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 11:56:10.137482: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_346/kernel/Assign' id:26027 op device:{requested: '', assigned: ''} def:{{{node dense_346/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_346/kernel, dense_346/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:56:10.799571: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_347/BiasAdd' id:26061 op device:{requested: '', assigned: ''} def:{{{node dense_347/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_347/MatMul, dense_347/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:56:11.621521: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_351/BiasAdd' id:26200 op device:{requested: '', assigned: ''} def:{{{node dense_351/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_351/MatMul, dense_351/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:56:11.961601: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_109/mul' id:26104 op device:{requested: '', assigned: ''} def:{{{node loss_109/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_109/mul/x, loss_109/dense_347_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 11:56:12.257522: W tensorflow/c/c_api.cc:305] Operation '{name:'training_24/Adam/dense_346/bias/v/Assign' id:26473 op device:{requested: '', assigned: ''} def:{{{node training_24/Adam/dense_346/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_24/Adam/dense_346/bias/v, training_24/Adam/dense_346/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -5057.921090136337 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  1 =  -3147.3494464006426 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  2 =  -936.425325210928 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  3 =  -706.6941910992815 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  4 =  -528.3357443278315 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  5 =  -498.5050693355373 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  6 =  -646.1825422924762 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  7 =  -599.5571323846904 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  8 =  -496.0909981123989 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  9 =  1205.5812595399052 with epsilon =  0.3363270336049504\n",
      "episodeReward for episode  10 =  -125.93335937933425 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  11 =  475.2100180184766 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  12 =  1636.6838661916822 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  13 =  1206.2714964379657 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  14 =  1079.7039922736872 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  15 =  803.3755356892896 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  16 =  1279.0960693377838 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  17 =  1314.7994698617504 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  18 =  1204.7875736930723 with epsilon =  0.13046284547465326\n",
      "episodeReward for episode  19 =  1157.1750499268085 with epsilon =  0.13046284547465326\n",
      "Iteration 5/10\n",
      "Learning Rate: 0.09697272727272728, Discount Factor: 0.87, Batch Size: 256, Epsilon Decay: 0.9964257142857142, Min Epsilon: 0.05000000000000001, Replay Memory Capacity: 45000, Update Target Interval: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:08:50.705292: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_356/bias/Assign' id:26716 op device:{requested: '', assigned: ''} def:{{{node dense_356/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_356/bias, dense_356/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:08:51.395587: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_355/BiasAdd' id:26654 op device:{requested: '', assigned: ''} def:{{{node dense_355/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_355/MatMul, dense_355/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:08:52.219652: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_359/BiasAdd' id:26793 op device:{requested: '', assigned: ''} def:{{{node dense_359/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_359/MatMul, dense_359/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:08:52.563182: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_111/mul' id:26697 op device:{requested: '', assigned: ''} def:{{{node loss_111/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_111/mul/x, loss_111/dense_355_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:08:52.851666: W tensorflow/c/c_api.cc:305] Operation '{name:'training_26/Adam/dense_354/kernel/v/Assign' id:27061 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/dense_354/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/dense_354/kernel/v, training_26/Adam/dense_354/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -7802.6423217264455 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  1 =  -15366.000732601127 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  2 =  -829.2288656982187 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  3 =  -18356.48000362609 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  4 =  -15312.32238263278 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  5 =  -14787.5910918154 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  6 =  -15831.562903086766 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  7 =  -4907.726464320114 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  8 =  -15093.048231330149 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  9 =  -17268.81418150366 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  10 =  -4552.236585968762 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  11 =  -15938.701585352597 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  12 =  -14714.670748413686 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  13 =  -15078.650919544096 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  14 =  -9339.871704111321 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  15 =  -13975.036585144418 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  16 =  -15146.042204239913 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  17 =  -19249.839010674135 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  18 =  -14928.564811259166 with epsilon =  0.04987789694580385\n",
      "episodeReward for episode  19 =  -16761.681584756978 with epsilon =  0.04987789694580385\n",
      "Iteration 6/10\n",
      "Learning Rate: 0.07477272727272728, Discount Factor: 0.95, Batch Size: 128, Epsilon Decay: 0.9973422448979592, Min Epsilon: 0.030000000000000006, Replay Memory Capacity: 10000, Update Target Interval: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:26:51.280794: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_360/kernel/Assign' id:27165 op device:{requested: '', assigned: ''} def:{{{node dense_360/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_360/kernel, dense_360/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:26:52.212866: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_363/BiasAdd' id:27247 op device:{requested: '', assigned: ''} def:{{{node dense_363/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_363/MatMul, dense_363/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:26:52.511505: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_367/BiasAdd' id:27386 op device:{requested: '', assigned: ''} def:{{{node dense_367/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_367/MatMul, dense_367/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:26:52.854187: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_113/mul' id:27290 op device:{requested: '', assigned: ''} def:{{{node loss_113/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_113/mul/x, loss_113/dense_363_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:26:53.150071: W tensorflow/c/c_api.cc:305] Operation '{name:'training_28/Adam/decay/Assign' id:27576 op device:{requested: '', assigned: ''} def:{{{node training_28/Adam/decay/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_28/Adam/decay, training_28/Adam/decay/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -13417.480573932868 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  1 =  -17580.271907045797 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  2 =  -19374.310055039165 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  3 =  -19862.726732675896 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  4 =  -9535.165990041 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  5 =  -16194.277101770944 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  6 =  -18813.555071650793 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  7 =  -14896.213412570449 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  8 =  -18245.655041917304 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  9 =  -20486.85300309152 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  10 =  -21495.82858402314 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  11 =  -15647.894055431203 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  12 =  -4054.784102349011 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  13 =  -23080.440622035567 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  14 =  -16969.666354044024 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  15 =  -19314.921029410336 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  16 =  -17460.73750815936 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  17 =  -20226.5501858785 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  18 =  -11913.428269819153 with epsilon =  0.029987051558754804\n",
      "episodeReward for episode  19 =  -14348.965697145786 with epsilon =  0.029987051558754804\n",
      "Iteration 7/10\n",
      "Learning Rate: 0.07275454545454546, Discount Factor: 0.92, Batch Size: 128, Epsilon Decay: 0.9972404081632653, Min Epsilon: 0.05000000000000001, Replay Memory Capacity: 30000, Update Target Interval: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:40:02.254266: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_368/kernel/Assign' id:27758 op device:{requested: '', assigned: ''} def:{{{node dense_368/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_368/kernel, dense_368/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:40:03.002329: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_371/BiasAdd' id:27840 op device:{requested: '', assigned: ''} def:{{{node dense_371/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_371/MatMul, dense_371/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:40:03.519412: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_375/BiasAdd' id:27979 op device:{requested: '', assigned: ''} def:{{{node dense_375/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_375/MatMul, dense_375/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:40:03.880887: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_115/mul' id:27883 op device:{requested: '', assigned: ''} def:{{{node loss_115/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_115/mul/x, loss_115/dense_371_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:40:04.183983: W tensorflow/c/c_api.cc:305] Operation '{name:'training_30/Adam/dense_371/bias/m/Assign' id:28218 op device:{requested: '', assigned: ''} def:{{{node training_30/Adam/dense_371/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_30/Adam/dense_371/bias/m, training_30/Adam/dense_371/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -13832.654116310578 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  1 =  -17199.89227638377 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  2 =  -10159.425714894722 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  3 =  -5117.895155716674 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  4 =  -20415.577010792695 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  5 =  -13234.10880612121 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  6 =  -4638.858784940201 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  7 =  -8580.662262023057 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  8 =  -6935.923249790182 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  9 =  -15256.32854468294 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  10 =  -9018.128972295483 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  11 =  -15898.821900613206 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  12 =  -16018.808587134832 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  13 =  -7470.8988636762015 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  14 =  -13238.975296793831 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  15 =  -15516.280819612024 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  16 =  -12786.365337099605 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  17 =  -4309.802489939769 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  18 =  -2578.666798135539 with epsilon =  0.049922034792869906\n",
      "episodeReward for episode  19 =  -8148.293008740815 with epsilon =  0.049922034792869906\n",
      "Iteration 8/10\n",
      "Learning Rate: 0.06064545454545455, Discount Factor: 0.92, Batch Size: 256, Epsilon Decay: 0.9998881632653062, Min Epsilon: 0.01, Replay Memory Capacity: 20000, Update Target Interval: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:51:11.960006: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_377/kernel/Assign' id:28375 op device:{requested: '', assigned: ''} def:{{{node dense_377/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_377/kernel, dense_377/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:51:12.739461: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_379/BiasAdd' id:28433 op device:{requested: '', assigned: ''} def:{{{node dense_379/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_379/MatMul, dense_379/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:51:13.495784: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_383/BiasAdd' id:28572 op device:{requested: '', assigned: ''} def:{{{node dense_383/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_383/MatMul, dense_383/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:51:13.851056: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_117/mul' id:28476 op device:{requested: '', assigned: ''} def:{{{node loss_117/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_117/mul/x, loss_117/dense_379_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 12:51:14.157573: W tensorflow/c/c_api.cc:305] Operation '{name:'training_32/Adam/iter/Assign' id:28747 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_32/Adam/iter, training_32/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -1942.7068552689484 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  1 =  -11484.085349681507 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  2 =  -1817.4382167082135 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  3 =  -8776.509917528132 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  4 =  -1774.0029778035532 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  5 =  -6072.112955006758 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  6 =  -1411.0931645384608 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  7 =  -10051.23063864821 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  8 =  -2446.2499001788888 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  9 =  -8934.110632031457 with epsilon =  0.8846489470446746\n",
      "episodeReward for episode  10 =  -9920.642676576254 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  11 =  78.84635621450633 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  12 =  -2756.793063505159 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  13 =  -2375.657269868373 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  14 =  -4214.073851792882 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  15 =  -6454.093183290856 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  16 =  -3452.65453812972 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  17 =  -169.45550495464624 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  18 =  -537.6210206131425 with epsilon =  0.6324176956343934\n",
      "episodeReward for episode  19 =  -4260.140664340053 with epsilon =  0.6324176956343934\n",
      "Iteration 9/10\n",
      "Learning Rate: 0.0889, Discount Factor: 0.8300000000000001, Batch Size: 32, Epsilon Decay: 0.9986661224489797, Min Epsilon: 0.08, Replay Memory Capacity: 15000, Update Target Interval: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 13:01:58.755449: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_386/kernel/Assign' id:28992 op device:{requested: '', assigned: ''} def:{{{node dense_386/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_386/kernel, dense_386/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:01:59.457924: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_387/BiasAdd' id:29026 op device:{requested: '', assigned: ''} def:{{{node dense_387/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_387/MatMul, dense_387/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:01:59.829378: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_391/BiasAdd' id:29165 op device:{requested: '', assigned: ''} def:{{{node dense_391/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_391/MatMul, dense_391/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:02:00.182527: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_119/mul' id:29069 op device:{requested: '', assigned: ''} def:{{{node loss_119/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_119/mul/x, loss_119/dense_387_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:02:00.490628: W tensorflow/c/c_api.cc:305] Operation '{name:'training_34/Adam/dense_385/kernel/m/Assign' id:29377 op device:{requested: '', assigned: ''} def:{{{node training_34/Adam/dense_385/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_34/Adam/dense_385/kernel/m, training_34/Adam/dense_385/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -10013.653290754964 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  1 =  -16590.490038767708 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  2 =  -14926.887559367762 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  3 =  -5827.571058346457 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  4 =  -15470.63816551307 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  5 =  -14130.777815566236 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  6 =  -11229.045162661878 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  7 =  -6345.151604329504 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  8 =  -17084.07744051958 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  9 =  -16420.547748390025 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  10 =  -6094.717579722551 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  11 =  -15063.0398123281 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  12 =  -7895.630237880561 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  13 =  -13311.519503578564 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  14 =  -15570.008926199502 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  15 =  -13095.078264043532 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  16 =  -15107.011595061016 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  17 =  -10177.750696144143 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  18 =  -16568.123662838938 with epsilon =  0.07999560476172372\n",
      "episodeReward for episode  19 =  -11350.931723979817 with epsilon =  0.07999560476172372\n",
      "Iteration 10/10\n",
      "Learning Rate: 0.06468181818181819, Discount Factor: 0.81, Batch Size: 256, Epsilon Decay: 0.9977495918367347, Min Epsilon: 0.07, Replay Memory Capacity: 35000, Update Target Interval: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 13:09:12.643152: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_395/kernel/Assign' id:29609 op device:{requested: '', assigned: ''} def:{{{node dense_395/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_395/kernel, dense_395/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:09:13.401804: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_395/BiasAdd' id:29619 op device:{requested: '', assigned: ''} def:{{{node dense_395/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_395/MatMul, dense_395/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:09:14.225513: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_399/BiasAdd' id:29758 op device:{requested: '', assigned: ''} def:{{{node dense_399/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_399/MatMul, dense_399/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:09:14.594109: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_121/mul' id:29662 op device:{requested: '', assigned: ''} def:{{{node loss_121/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_121/mul/x, loss_121/dense_395_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:09:14.925660: W tensorflow/c/c_api.cc:305] Operation '{name:'training_36/Adam/dense_392/bias/m/Assign' id:29963 op device:{requested: '', assigned: ''} def:{{{node training_36/Adam/dense_392/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_36/Adam/dense_392/bias/m, training_36/Adam/dense_392/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -14095.818721034251 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  1 =  -12530.95366068895 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  2 =  -12422.472176278085 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  3 =  1552.908288655966 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  4 =  -20065.59094608696 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  5 =  -19813.42890714481 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  6 =  -17744.48269362927 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  7 =  -16687.4298258788 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  8 =  -9046.863023573374 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  9 =  -14580.664519736572 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  10 =  -10016.099201432891 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  11 =  -9044.751764179673 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  12 =  -10283.65533410404 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  13 =  -10277.55719756807 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  14 =  -20238.571863904064 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  15 =  -10414.56095602948 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  16 =  -20263.3004115652 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  17 =  -17530.13164570064 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  18 =  -11095.941100605332 with epsilon =  0.0699823199441582\n",
      "episodeReward for episode  19 =  -10309.401863355923 with epsilon =  0.0699823199441582\n",
      "Best Hyperparameters found:\n",
      "{'learning_rate': 0.06064545454545455, 'discount_factor': 0.92, 'batch_size': 256, 'epsilon_decay': 0.9998881632653062, 'min_epsilon': 0.01, 'replay_memory_capacity': 20000, 'update_target_interval': 4}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter ranges\n",
    "learning_rate_options = np.linspace(0.0001, 0.1, 100)\n",
    "discount_factor_options = np.linspace(0.8, 0.99, 20)\n",
    "batch_size_options = [32, 64, 128, 256]\n",
    "epsilon_decay_options = np.linspace(0.995, 0.99999, 50)\n",
    "epsilon_options =  [0.99, 0.8, 0.5]\n",
    "min_epsilon_options = np.linspace(0.01, 0.1, 10)\n",
    "replay_memory_capacity_options = range(10000, 50001, 5000)\n",
    "update_target_interval_options = range(1, 6)\n",
    "\n",
    "\n",
    "num_random_search_iters = 10  # Define how many random configurations you want to try\n",
    "\n",
    "best_performance = float('-inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "for search_iter in range(num_random_search_iters):\n",
    "    # Randomly select hyperparameters for this iteration\n",
    "    learning_rate = np.random.choice(learning_rate_options)\n",
    "    discount_factor = np.random.choice(discount_factor_options)\n",
    "    batch_size = np.random.choice(batch_size_options)\n",
    "    epsilon_decay = np.random.choice(epsilon_decay_options)\n",
    "    min_epsilon = np.random.choice(min_epsilon_options)\n",
    "    replay_memory_capacity = int(np.random.choice(replay_memory_capacity_options))\n",
    "    update_target_interval = np.random.choice(update_target_interval_options)\n",
    "    epsilon = np.random.choice(epsilon_options)\n",
    "\n",
    "    print(f\"Iteration {search_iter + 1}/{num_random_search_iters}\")\n",
    "    print(f\"Learning Rate: {learning_rate}, Discount Factor: {discount_factor}, \"\n",
    "          f\"Batch Size: {batch_size}, Epsilon Decay: {epsilon_decay}, \"\n",
    "          f\"Min Epsilon: {min_epsilon}, Replay Memory Capacity: {replay_memory_capacity}, \"\n",
    "          f\"Update Target Interval: {update_target_interval}\")\n",
    "\n",
    "    # Initialize the environment and the DQN agent with the selected hyperparameters\n",
    "    env = RCMazeEnv()\n",
    "    state = env.reset()\n",
    "    env.init_pygame()\n",
    "    agent = DQNAgent(replayCapacity=replay_memory_capacity,\n",
    "                     input_shape=state.shape,\n",
    "                     output_shape=len(env.possible_actions),\n",
    "                     learning_rate=learning_rate,\n",
    "                     discount_factor=discount_factor)\n",
    "\n",
    " \n",
    "\n",
    "    # ... (Insert the rest of your existing DQN training loop here)\n",
    "    # Make sure to replace hard-coded hyperparameters in the training loop\n",
    "    # with the variables defined above, like `learning_rate`, `batch_size`, etc.\n",
    "    update_counter = 0\n",
    "    reward_history = []\n",
    "    epsilon_history = []\n",
    "\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "    for episode in range(20):\n",
    "        episode_reward = 0\n",
    "        step_counter = 0  # count the number of successful steps within the episode\n",
    "        \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        epsilon_history.append(epsilon)\n",
    "        \n",
    "        # early stopping\n",
    "        # if len(reward_history) > 10:\n",
    "        #     last_10_rewards = reward_history[-10:]\n",
    "        #     if all(reward > 0 for reward in last_10_rewards):\n",
    "        #         differences = [abs(last_10_rewards[i] - last_10_rewards[i-1]) for i in range(1, 10)]\n",
    "        #         if all(diff < 200 for diff in differences):\n",
    "        #             print('The difference between each of the last 10 positive rewards is less than 200, stopping training')\n",
    "        #             break\n",
    "        while not done:\n",
    "            env.render(delay=0, framerate=360)\n",
    "\n",
    "            if random.random() <= epsilon:\n",
    "                action = random.sample(POSSIBLE_ACTIONS, 1)[0]\n",
    "            else:\n",
    "                qValues = agent.policy_network_predict(state.reshape(1,-1))\n",
    "                action = np.argmax(qValues[0])\n",
    "\n",
    "            new_state, reward, done = env.step(action)\n",
    "\n",
    "            step_counter +=1\n",
    "\n",
    "            # store step in replay memory\n",
    "            step = (state, action, reward, new_state, done)\n",
    "            agent.addToReplayMemory(step)\n",
    "            state = new_state\n",
    "            episode_reward += reward\n",
    "            # When enough steps in replay memory -> train policy network\n",
    "            if len(agent.memory) >= (batch_size):\n",
    "                # Update epsilon after each episode, not each training step\n",
    "                if episode % 10 == 0 and epsilon > min_epsilon:\n",
    "                    epsilon *= epsilon_decay\n",
    "                # sample minibatch from replay memory\n",
    "                \n",
    "                miniBatch = agent.sampleFromReplayMemory(batch_size)\n",
    "                miniBatch_states = np.asarray(list(zip(*miniBatch))[0],dtype=float)\n",
    "                miniBatch_actions = np.asarray(list(zip(*miniBatch))[1], dtype = int)\n",
    "                miniBatch_rewards = np.asarray(list(zip(*miniBatch))[2], dtype = float)\n",
    "                miniBatch_next_state = np.asarray(list(zip(*miniBatch))[3],dtype=float)\n",
    "                miniBatch_done = np.asarray(list(zip(*miniBatch))[4],dtype=bool)\n",
    "                \n",
    "                # current state q values1tch_states)\n",
    "                y = agent.policy_network_predict(miniBatch_states)\n",
    "\n",
    "                next_state_q_values = agent.target_network_predict(miniBatch_next_state)\n",
    "                max_q_next_state = np.max(next_state_q_values,axis=1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    if miniBatch_done[i]:\n",
    "                        y[i,miniBatch_actions[i]] = miniBatch_rewards[i]\n",
    "                    else:\n",
    "                        y[i,miniBatch_actions[i]] = miniBatch_rewards[i] + discount_factor *  max_q_next_state[i]\n",
    "                        \n",
    "                agent.policy_model.fit(miniBatch_states, y, batch_size=batch_size, verbose = 0)\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            if update_counter == update_target_interval:\n",
    "                agent.update_target_network()\n",
    "                update_counter = 0\n",
    "            update_counter += 1\n",
    "        print('episodeReward for episode ', episode, '= ', episode_reward, 'with epsilon = ', epsilon)\n",
    "        reward_history.append(episode_reward)\n",
    "\n",
    "    # After training, evaluate the performance of the agent\n",
    "    current_performance = evaluate_agent_performance(agent, env)  # Define this function as per your evaluation metric\n",
    "\n",
    "    if current_performance > best_performance:\n",
    "        best_performance = current_performance\n",
    "        best_hyperparameters = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"discount_factor\": discount_factor,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epsilon_decay\": epsilon_decay,\n",
    "            \"min_epsilon\": min_epsilon,\n",
    "            \"replay_memory_capacity\": replay_memory_capacity,\n",
    "            \"update_target_interval\": update_target_interval\n",
    "        }\n",
    "\n",
    "    env.close_pygame()\n",
    "    env.close()\n",
    "\n",
    "# After all iterations, print out the best performing hyperparameters\n",
    "print(\"Best Hyperparameters found:\")\n",
    "print(best_hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing with best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.06064545454545455, 'discount_factor': 0.92, 'batch_size': 256, 'epsilon_decay': 0.9998881632653062, 'min_epsilon': 0.01, 'replay_memory_capacity': 20000, 'update_target_interval': 4}\n"
     ]
    }
   ],
   "source": [
    "# make new model with the best hyperparameters\n",
    "print(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 13:33:33.352902: W tensorflow/c/c_api.cc:305] Operation '{name:'total_124/Assign' id:30356 op device:{requested: '', assigned: ''} def:{{{node total_124/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_124, total_124/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/lucasdriessens/.local/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-01-17 13:33:34.409075: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_403/BiasAdd' id:30212 op device:{requested: '', assigned: ''} def:{{{node dense_403/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_403/MatMul, dense_403/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:33:34.960297: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_407/BiasAdd' id:30351 op device:{requested: '', assigned: ''} def:{{{node dense_407/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_407/MatMul, dense_407/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:33:35.330094: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_123/mul' id:30255 op device:{requested: '', assigned: ''} def:{{{node loss_123/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_123/mul/x, loss_123/dense_403_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-17 13:33:35.648749: W tensorflow/c/c_api.cc:305] Operation '{name:'training_38/Adam/dense_401/kernel/m/Assign' id:30563 op device:{requested: '', assigned: ''} def:{{{node training_38/Adam/dense_401/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_38/Adam/dense_401/kernel/m, training_38/Adam/dense_401/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeReward for episode  0 =  -562.6959914407755 with epsilon =  0.9521015887850328\n",
      "episodeReward for episode  1 =  -6779.342488460645 with epsilon =  0.7896252134978342\n",
      "episodeReward for episode  2 =  -2217.930192354638 with epsilon =  0.7059917753086713\n",
      "episodeReward for episode  3 =  560.3892368685965 with epsilon =  0.6849145854627794\n",
      "episodeReward for episode  4 =  -422.09146182632026 with epsilon =  0.6401746133137192\n",
      "episodeReward for episode  5 =  1037.4210763789133 with epsilon =  0.6300173917980675\n",
      "episodeReward for episode  6 =  -2155.0935322230775 with epsilon =  0.5574602471269431\n",
      "episodeReward for episode  7 =  -5392.503390570234 with epsilon =  0.486846722644997\n",
      "episodeReward for episode  8 =  716.8066582262354 with epsilon =  0.4774639480385564\n",
      "episodeReward for episode  9 =  1155.5678167940846 with epsilon =  0.4718896386124908\n",
      "episodeReward for episode  10 =  1081.3744662836082 with epsilon =  0.46638040828968547\n",
      "episodeReward for episode  11 =  1274.71635965106 with epsilon =  0.4616577962755911\n",
      "episodeReward for episode  12 =  -5212.279921279519 with epsilon =  0.39629474347087423\n",
      "episodeReward for episode  13 =  -2501.6386365899502 with epsilon =  0.3547572231575977\n",
      "episodeReward for episode  14 =  167.95530687172925 with epsilon =  0.3457091956503241\n",
      "episodeReward for episode  15 =  655.7884075606477 with epsilon =  0.3384781774085488\n",
      "episodeReward for episode  16 =  -391.88638351195596 with epsilon =  0.32614033019013083\n",
      "episodeReward for episode  17 =  1192.209023038763 with epsilon =  0.32175640331321637\n",
      "episodeReward for episode  18 =  -5205.661398942351 with epsilon =  0.28103092909807664\n",
      "episodeReward for episode  19 =  1410.7212600040275 with epsilon =  0.2746608100881427\n",
      "episodeReward for episode  20 =  1010.4708655014127 with epsilon =  0.2705146543947272\n",
      "episodeReward for episode  21 =  -1043.149315518725 with epsilon =  0.2570929549947356\n",
      "episodeReward for episode  22 =  -4021.271247118819 with epsilon =  0.22911852212579495\n",
      "episodeReward for episode  23 =  752.6723034733996 with epsilon =  0.22533199053391478\n",
      "episodeReward for episode  24 =  751.5475777668629 with epsilon =  0.21607675836424123\n",
      "episodeReward for episode  25 =  -1820.4649273045575 with epsilon =  0.20352680341048837\n",
      "episodeReward for episode  26 =  -255.21545528578986 with epsilon =  0.19523268750377532\n",
      "episodeReward for episode  27 =  -605.4856651943358 with epsilon =  0.18769595337296452\n",
      "episodeReward for episode  28 =  -1123.2642095453098 with epsilon =  0.17930344640850243\n",
      "episodeReward for episode  29 =  1225.1733378298077 with epsilon =  0.17628102729332118\n",
      "episodeReward for episode  30 =  1692.5941507452576 with epsilon =  0.17299969741436147\n",
      "episodeReward for episode  31 =  927.7419696273477 with epsilon =  0.17080793612602413\n",
      "episodeReward for episode  32 =  382.6712787731053 with epsilon =  0.16513497372347866\n",
      "episodeReward for episode  33 =  -1202.8531826607464 with epsilon =  0.15674876947567948\n",
      "episodeReward for episode  34 =  860.3380815231151 with epsilon =  0.15472827941734427\n",
      "episodeReward for episode  35 =  1142.2970353467244 with epsilon =  0.1521371248314584\n",
      "episodeReward for episode  36 =  797.4290746757928 with epsilon =  0.15004176978903064\n",
      "episodeReward for episode  37 =  1041.5357806951606 with epsilon =  0.14837300715516588\n",
      "episodeReward for episode  38 =  863.8296101617772 with epsilon =  0.14639497293132997\n",
      "episodeReward for episode  39 =  633.1944427624626 with epsilon =  0.1435414576547707\n",
      "episodeReward for episode  40 =  1191.2331122652422 with epsilon =  0.14259738369446984\n",
      "episodeReward for episode  41 =  984.3856938414735 with epsilon =  0.14121659134717912\n",
      "episodeReward for episode  42 =  683.2008541589743 with epsilon =  0.138960459719677\n",
      "episodeReward for episode  43 =  703.4862777803435 with epsilon =  0.13695464897685652\n",
      "episodeReward for episode  44 =  856.0208290926495 with epsilon =  0.13524979836644502\n",
      "episodeReward for episode  45 =  1289.2498050017557 with epsilon =  0.13440534807561746\n",
      "episodeReward for episode  46 =  1250.4687222643101 with epsilon =  0.13385030236366974\n",
      "episodeReward for episode  47 =  832.2363105773933 with epsilon =  0.13215453112512077\n",
      "episodeReward for episode  48 =  840.3979986293834 with epsilon =  0.13015958593632856\n",
      "episodeReward for episode  49 =  1148.2208364044286 with epsilon =  0.12904347609466707\n",
      "episodeReward for episode  50 =  1042.891873447036 with epsilon =  0.12806578130141996\n",
      "episodeReward for episode  51 =  375.303017888441 with epsilon =  0.12396481044062622\n",
      "episodeReward for episode  52 =  -128.42803279713928 with epsilon =  0.1194461767627141\n",
      "episodeReward for episode  53 =  -1349.3380712311575 with epsilon =  0.11325349259057654\n",
      "episodeReward for episode  54 =  -2870.9461221812467 with epsilon =  0.10221399264470367\n",
      "episodeReward for episode  55 =  -790.9454089210963 with epsilon =  0.09625576790871618\n",
      "episodeReward for episode  56 =  1060.1130741204709 with epsilon =  0.0935807279671654\n",
      "episodeReward for episode  57 =  325.6563605467586 with epsilon =  0.08981734315611388\n",
      "episodeReward for episode  58 =  -12374.093934059401 with epsilon =  0.0681373761692028\n",
      "episodeReward for episode  59 =  -1759.2015850411913 with epsilon =  0.062466093320274425\n",
      "episodeReward for episode  60 =  842.5932620063481 with epsilon =  0.06157820543608935\n",
      "episodeReward for episode  61 =  -832.4551311446271 with epsilon =  0.058313898693583645\n",
      "episodeReward for episode  62 =  775.7469830934097 with epsilon =  0.056928393396404854\n",
      "episodeReward for episode  63 =  -65.40163382629135 with epsilon =  0.054334299571964735\n",
      "episodeReward for episode  64 =  -5176.334923355773 with epsilon =  0.04627256160364144\n",
      "episodeReward for episode  65 =  314.151357282537 with epsilon =  0.04522370294151585\n",
      "episodeReward for episode  66 =  -1627.027528350854 with epsilon =  0.04064237722873436\n",
      "episodeReward for episode  67 =  -17656.879322385284 with epsilon =  0.029054416034705318\n",
      "episodeReward for episode  68 =  -11371.623803872948 with epsilon =  0.023023998150650565\n",
      "episodeReward for episode  69 =  -16536.302858186333 with epsilon =  0.016459392059831046\n",
      "episodeReward for episode  70 =  -15330.276101400774 with epsilon =  0.011766487523435384\n",
      "episodeReward for episode  71 =  -18660.968508787555 with epsilon =  0.01\n",
      "episodeReward for episode  72 =  -21910.866185812643 with epsilon =  0.01\n",
      "episodeReward for episode  73 =  -20638.528946527866 with epsilon =  0.01\n",
      "episodeReward for episode  74 =  -20437.97030571071 with epsilon =  0.01\n",
      "episodeReward for episode  75 =  -20589.704953512737 with epsilon =  0.01\n",
      "episodeReward for episode  76 =  -2900.2248730308343 with epsilon =  0.01\n",
      "episodeReward for episode  77 =  -310.9071033836368 with epsilon =  0.01\n",
      "episodeReward for episode  78 =  -20256.62501379853 with epsilon =  0.01\n",
      "episodeReward for episode  79 =  -20528.767668576016 with epsilon =  0.01\n",
      "episodeReward for episode  80 =  -20576.536782409366 with epsilon =  0.01\n",
      "episodeReward for episode  81 =  -20479.387483756036 with epsilon =  0.01\n",
      "episodeReward for episode  82 =  -20368.821531676324 with epsilon =  0.01\n",
      "episodeReward for episode  83 =  -20636.234908701175 with epsilon =  0.01\n",
      "episodeReward for episode  84 =  -12971.149852999606 with epsilon =  0.01\n",
      "episodeReward for episode  85 =  -4531.8844732613 with epsilon =  0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# current state q values1tch_states)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m y \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy_network_predict(miniBatch_states)\n\u001b[0;32m---> 82\u001b[0m next_state_q_values \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_network_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminiBatch_next_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m max_q_next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(next_state_q_values,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BATCH_SIZE):\n",
      "Cell \u001b[0;32mIn[86], line 72\u001b[0m, in \u001b[0;36mDQNAgent.target_network_predict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtarget_network_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqTarget \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqTarget\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/training_v1.py:1059\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1058\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m-> 1059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/training_arrays_v1.py:801\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[1;32m    798\u001b[0m x, _, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_standardize_user_data(\n\u001b[1;32m    799\u001b[0m     x, check_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, steps_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39msteps\n\u001b[1;32m    800\u001b[0m )\n\u001b[0;32m--> 801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/training_arrays_v1.py:421\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(\n\u001b[1;32m    417\u001b[0m     mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_index, batch_logs\n\u001b[1;32m    418\u001b[0m )\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    423\u001b[0m     batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/backend.py:4607\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4599\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4603\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\n\u001b[1;32m   4604\u001b[0m ):\n\u001b[1;32m   4605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4607\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches) :])\n\u001b[1;32m   4609\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4611\u001b[0m     fetched[: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[1;32m   4612\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   4613\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/client/session.py:1505\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1504\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1505\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1508\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1509\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = RCMazeEnv()\n",
    "state = env.reset()\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "# Model parameters\n",
    "REPLAY_MEMORY_CAPACITY = 2000000\n",
    "# MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "POSSIBLE_ACTIONS = env.possible_actions\n",
    "# reset the parameters\n",
    "DISCOUNT = 0.92\n",
    "BATCH_SIZE = 256  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_INTERVAL = 4\n",
    "EPSILON = 0.99 # Exploration percentage\n",
    "MIN_EPSILON = 0.01\n",
    "DECAY = 0.9998881632653062\n",
    "EPISODE_AMOUNT = 170\n",
    "LEARNING_RATE = 0.06064545454545455\n",
    "\n",
    "# state = state[0]\n",
    "# create DQN agent\n",
    "agent = DQNAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, \n",
    "                input_shape=state.shape, \n",
    "                output_shape=len(POSSIBLE_ACTIONS),\n",
    "                learning_rate=LEARNING_RATE, \n",
    "                discount_factor=DISCOUNT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fill the replay memory with the first batch of samples\n",
    "update_counter = 0\n",
    "reward_history = []\n",
    "epsilon_history = []\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "for episode in range(EPISODE_AMOUNT):\n",
    "    episode_reward = 0\n",
    "    step_counter = 0  # count the number of successful steps within the episode\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    epsilon_history.append(EPSILON)\n",
    "    while not done:\n",
    "        env.render(delay=0, framerate=360)\n",
    "\n",
    "        if random.random() <= EPSILON:\n",
    "            action = random.sample(POSSIBLE_ACTIONS, 1)[0]\n",
    "        else:\n",
    "            qValues = agent.policy_network_predict(state.reshape(1,-1))\n",
    "            action = np.argmax(qValues[0])\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        step_counter +=1\n",
    "\n",
    "        # store step in replay memory\n",
    "        step = (state, action, reward, new_state, done)\n",
    "        agent.addToReplayMemory(step)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        # When enough steps in replay memory -> train policy network\n",
    "        if len(agent.memory) >= (BATCH_SIZE):\n",
    "            EPSILON = DECAY * EPSILON\n",
    "            if EPSILON < MIN_EPSILON:\n",
    "                EPSILON = MIN_EPSILON\n",
    "            # sample minibatch from replay memory\n",
    "            \n",
    "            miniBatch = agent.sampleFromReplayMemory(BATCH_SIZE)\n",
    "            miniBatch_states = np.asarray(list(zip(*miniBatch))[0],dtype=float)\n",
    "            miniBatch_actions = np.asarray(list(zip(*miniBatch))[1], dtype = int)\n",
    "            miniBatch_rewards = np.asarray(list(zip(*miniBatch))[2], dtype = float)\n",
    "            miniBatch_next_state = np.asarray(list(zip(*miniBatch))[3],dtype=float)\n",
    "            miniBatch_done = np.asarray(list(zip(*miniBatch))[4],dtype=bool)\n",
    "            \n",
    "            # current state q values1tch_states)\n",
    "            y = agent.policy_network_predict(miniBatch_states)\n",
    "\n",
    "            next_state_q_values = agent.target_network_predict(miniBatch_next_state)\n",
    "            max_q_next_state = np.max(next_state_q_values,axis=1)\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                if miniBatch_done[i]:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i]\n",
    "                else:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i] + DISCOUNT *  max_q_next_state[i]\n",
    "                    \n",
    "            agent.policy_model.fit(miniBatch_states, y, batch_size=BATCH_SIZE, verbose = 0)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        if update_counter == UPDATE_TARGET_INTERVAL:\n",
    "            agent.update_target_network()\n",
    "            update_counter = 0\n",
    "        update_counter += 1\n",
    "    print('episodeReward for episode ', episode, '= ', episode_reward, 'with epsilon = ', EPSILON)\n",
    "    reward_history.append(episode_reward)\n",
    "    \n",
    "\n",
    "env.close_pygame()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results after training with best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the reward history\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward History for Double DQN after hyperparameter tuning')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "agent.policy_model.save('./models/DDQN_RCmaze_HPT.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing with best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out\n",
    "# load model\n",
    "env = RCMazeEnv()\n",
    "state = env.reset()\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "REPLAY_MEMORY_CAPACITY = 20000\n",
    "POSSIBLE_ACTIONS = env.possible_actions\n",
    "\n",
    "# create DQN agent\n",
    "test_agent = DQNAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, input_shape=state.shape, output_shape=len(POSSIBLE_ACTIONS))\n",
    "\n",
    "test_agent.policy_model = load_model('./models/DDQN_RCmaze_HPT.h5')\n",
    "\n",
    "\n",
    "done = False\n",
    "\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    env.render(delay=100, framerate=60)\n",
    "   \n",
    "    qValues = test_agent.policy_network_predict(np.array([state]))\n",
    "    action = np.argmax(qValues[0])\n",
    "    state, reward, done = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    env.render()\n",
    "    if done:\n",
    "        print('done in ', len(rewards), 'steps')\n",
    "        break\n",
    "env.close()\n",
    "print(sum(rewards))\n",
    "env.close_pygame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
