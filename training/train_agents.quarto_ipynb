{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---"
      ],
      "id": "29c83df3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -r ./requirements.txt"
      ],
      "id": "5a33779b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np   \n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import pygame\n",
        "\n",
        "# Import Tensorflow libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "#disable eager execution (optimization)\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()\n",
        "\n",
        "# ###### Tensorflow-GPU ########\n",
        "try:\n",
        "  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "  print(\"GPU found\")\n",
        "except:\n",
        "  print(\"No GPU found\")"
      ],
      "id": "3c99e1c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## environment\n"
      ],
      "id": "1bc230fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "BLACK = (0, 0, 0)\n",
        "WHITE = (255, 255, 255)\n",
        "RED = (255, 0, 0)  # Car color\n",
        "GREEN = (0, 255, 0)  # Goal\n",
        "\n",
        "\n",
        "\n",
        "class RCMazeEnv(gym.Env):\n",
        "    def __init__(self, maze_size_x=12, maze_size_y=12):\n",
        "        self.maze_size_x = maze_size_x\n",
        "        self.maze_size_y = maze_size_y\n",
        "        self.maze = self.generate_maze()\n",
        "        self.car_position = (1, 1)\n",
        "        self.possible_actions = range(3)\n",
        "        self.car_orientation = 'E'\n",
        "        self.sensor_readings = {'front': 0, 'left': 0, 'right': 0}\n",
        "        self.steps = 0\n",
        "        self.previous_steps = 0\n",
        "        self.previous_distance = 0\n",
        "        self.goal = (10, 10)\n",
        "        self.visited_positions = set()\n",
        "        self.reset()\n",
        "\n",
        "        # Pygame initialization\n",
        "        pygame.init()\n",
        "        self.window_width = 600\n",
        "        self.window_height = 600\n",
        "        self.screen = pygame.display.set_mode((self.window_width, self.window_height))\n",
        "        self.cell_width = self.window_width / maze_size_x\n",
        "        self.cell_height = self.window_height / maze_size_y\n",
        "        # pygame clock\n",
        "        self.clock = pygame.time.Clock()\n",
        "        pygame.display.set_caption(\"RC Maze Game\")\n",
        "\n",
        "        # Load assets\n",
        "        self.car_image = self.load_image(\n",
        "            \"../textures/car.png\",\n",
        "            int(self.cell_width),\n",
        "            int(self.cell_height),\n",
        "        )\n",
        "\n",
        "        self.wall_image = self.load_image(\n",
        "            \"../textures/wall_center.png\",\n",
        "            int(self.cell_width),\n",
        "            int(self.cell_height),\n",
        "        )\n",
        "\n",
        "        self.goal_image = self.load_image(\n",
        "            \"../textures/door_closed.png\",\n",
        "            int(self.cell_width),\n",
        "            int(self.cell_height),\n",
        "        )\n",
        "        self.floor_image = self.load_image(\n",
        "            \"../textures/floor_mud_e.png\",\n",
        "            int(self.cell_width),\n",
        "            int(self.cell_height),\n",
        "        )\n",
        "        self.top_of_wall = self.load_image(\n",
        "            \"../textures/gargoyle_top_1.png\",\n",
        "            int(self.cell_width),\n",
        "            int(self.cell_height),\n",
        "        )\n",
        "        self.top_of_wall = pygame.transform.rotate(self.top_of_wall, 180)\n",
        "\n",
        "    def generate_maze(self):\n",
        "        # For simplicity, create a static maze with walls\n",
        "        # '1' represents a wall, and '0' represents an open path\n",
        "        maze = np.zeros((self.maze_size_y, self.maze_size_x), dtype=int)\n",
        "        # Add walls to the maze (this can be customized)\n",
        "\n",
        "        layout = [\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "            [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
        "            [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "            [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
        "            [1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
        "            [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1],\n",
        "            [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
        "            [1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
        "            [1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
        "            [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        ]\n",
        "\n",
        "        maze = np.array(layout)\n",
        "\n",
        "        return maze\n",
        "\n",
        "    def reset(self):\n",
        "        self.car_position = (1, 1)\n",
        "        self.car_orientation = 'E'\n",
        "        self.update_sensor_readings()\n",
        "        self.previous_steps = self.steps\n",
        "        self.steps = 0\n",
        "        self.previous_distance = 0\n",
        "        self.visited_positions.clear()  # Clear the visited positions\n",
        "        self.visited_positions.add(self.car_position)\n",
        "        return self.get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        if action == 0:\n",
        "            self.move_forward()\n",
        "        elif action == 1:  # Turn left\n",
        "            self.turn_left()\n",
        "        elif action == 2:  # Turn right\n",
        "            self.turn_right()\n",
        "\n",
        "        self.update_sensor_readings()\n",
        "        self.visited_positions.add(self.car_position)\n",
        "        reward = self.compute_reward()\n",
        "        self.steps += 1\n",
        "        done = self.is_done()\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "    def move_forward(self):\n",
        "        x, y = self.car_position\n",
        "\n",
        "        # Check sensor reading in the direction of car's orientation\n",
        "        if self.sensor_readings['front'] <= 4:\n",
        "            # If the sensor reading is 4 or less, do not move forward\n",
        "            return\n",
        "\n",
        "        if self.car_orientation == 'N' and y > 0 and self.maze[y - 1][x] != 1:\n",
        "            self.car_position = (x, y - 1)\n",
        "        elif self.car_orientation == 'S' and y < self.maze_size_y - 1 and self.maze[y + 1][x] != 1:\n",
        "            self.car_position = (x, y + 1)\n",
        "        elif self.car_orientation == 'E' and x < self.maze_size_x - 1 and self.maze[y][x + 1] != 1:\n",
        "            self.car_position = (x + 1, y)\n",
        "        elif self.car_orientation == 'W' and x > 0 and self.maze[y][x - 1] != 1:\n",
        "            self.car_position = (x - 1, y)\n",
        "\n",
        "    def turn_left(self):\n",
        "        orientations = ['N', 'W', 'S', 'E']\n",
        "        idx = orientations.index(self.car_orientation)\n",
        "        self.car_orientation = orientations[(idx + 1) % 4]\n",
        "\n",
        "    def turn_right(self):\n",
        "        orientations = ['N', 'E', 'S', 'W']\n",
        "        idx = orientations.index(self.car_orientation)\n",
        "        self.car_orientation = orientations[(idx + 1) % 4]\n",
        "\n",
        "    def update_sensor_readings(self):\n",
        "        # Simple sensor implementation: counts steps to the nearest wall\n",
        "        self.sensor_readings['front'] = self.distance_to_wall('front')\n",
        "        self.sensor_readings['left'] = self.distance_to_wall('left')\n",
        "        self.sensor_readings['right'] = self.distance_to_wall('right')\n",
        "\n",
        "    def distance_to_wall(self, direction):\n",
        "        x, y = self.car_position\n",
        "        sensor_max_range = 255  # Maximum range of the ultrasonic sensor\n",
        "\n",
        "        def calculate_distance(dx, dy):\n",
        "            distance = 0\n",
        "            while 0 <= x + distance * dx < self.maze_size_x and \\\n",
        "                0 <= y + distance * dy < self.maze_size_y and \\\n",
        "                self.maze[y + distance * dy][x + distance * dx] != 1:\n",
        "                distance += 1\n",
        "                if distance > sensor_max_range:  # Limiting the sensor range\n",
        "                    break\n",
        "            return distance\n",
        "\n",
        "        if direction == 'front':\n",
        "            if self.car_orientation == 'N':\n",
        "                distance = calculate_distance(0, -1)\n",
        "            elif self.car_orientation == 'S':\n",
        "                distance = calculate_distance(0, 1)\n",
        "            elif self.car_orientation == 'E':\n",
        "                distance = calculate_distance(1, 0)\n",
        "            elif self.car_orientation == 'W':\n",
        "                distance = calculate_distance(-1, 0)\n",
        "\n",
        "        elif direction == 'left':\n",
        "            if self.car_orientation == 'N':\n",
        "                distance = calculate_distance(-1, 0)\n",
        "            elif self.car_orientation == 'S':\n",
        "                distance = calculate_distance(1, 0)\n",
        "            elif self.car_orientation == 'E':\n",
        "                distance = calculate_distance(0, -1)\n",
        "            elif self.car_orientation == 'W':\n",
        "                distance = calculate_distance(0, 1)\n",
        "\n",
        "        elif direction == 'right':\n",
        "            if self.car_orientation == 'N':\n",
        "                distance = calculate_distance(1, 0)\n",
        "            elif self.car_orientation == 'S':\n",
        "                distance = calculate_distance(-1, 0)\n",
        "            elif self.car_orientation == 'E':\n",
        "                distance = calculate_distance(0, 1)\n",
        "            elif self.car_orientation == 'W':\n",
        "                distance = calculate_distance(0, -1)\n",
        "\n",
        "        # Normalize the distance to a range of 0-1\n",
        "        normalized_distance = distance / sensor_max_range\n",
        "        normalized_distance = max(0, min(normalized_distance, 1))\n",
        "\n",
        "        return normalized_distance * 1000\n",
        "\n",
        "    def compute_reward(self):\n",
        "        \"\"\"\n",
        "        Compute the reward based on the current state of the environment\n",
        "\n",
        "\n",
        "        @return The reward to be added to the step function\n",
        "        \"\"\"\n",
        "        # Initialize reward\n",
        "        reward = 0\n",
        "\n",
        "        # Check for collision or out of bounds\n",
        "        # If the sensor is on the front left right or front side of the sensor is not on the board.\n",
        "        if any(\n",
        "            self.sensor_readings[direction] == 0\n",
        "            for direction in [\"front\", \"left\", \"right\"]\n",
        "        ):\n",
        "            reward -= 20\n",
        "\n",
        "        # Check if goal is reached\n",
        "        # The reward of the goal.\n",
        "        if self.car_position == self.goal:\n",
        "            reward += 500\n",
        "            # Additional penalty if it takes too many steps to reach the goal\n",
        "            # If the reward is more than 1000 steps then the reward is decremented by 200.\n",
        "            if self.steps > 1000:\n",
        "                reward -= 200\n",
        "            \n",
        "\n",
        "        # Calculate the Euclidean distance to the goal\n",
        "        distance_to_goal = (\n",
        "            (self.car_position[0] - self.goal[0]) ** 2\n",
        "            + (self.car_position[1] - self.goal[1]) ** 2\n",
        "        ) ** 0.5\n",
        "\n",
        "        # Define a maximum reward when the car is at the goal\n",
        "        max_reward_at_goal = 50\n",
        "\n",
        "        # Reward based on proximity to the goal\n",
        "        reward += max_reward_at_goal / (\n",
        "            distance_to_goal + 1\n",
        "        )  # Adding 1 to avoid division by zero\n",
        "\n",
        "        # # Reward or penalize based on movement towards or away from the goal\n",
        "        # Move the reward to the goal\n",
        "        if distance_to_goal < self.previous_distance:\n",
        "            reward += 50  # Positive reward for moving closer to the goal\n",
        "        elif distance_to_goal > self.previous_distance:\n",
        "            reward -= 25  # Negative reward for moving farther from the goal\n",
        "\n",
        "        # Apply a penalty to revisit the same position\n",
        "        if self.car_position in self.visited_positions:\n",
        "            # Apply a penalty for revisiting the same position\n",
        "            reward -= 10\n",
        "\n",
        "        # Penalize for each step taken to encourage efficiency\n",
        "        reward -= 5\n",
        "\n",
        "        # Update the previous_distance for the next step\n",
        "        self.previous_distance = distance_to_goal\n",
        "        return reward\n",
        "\n",
        "    def is_done(self):\n",
        "        # is done if it reaches the goal or goes out of bounds or takes more than 3000 steps\n",
        "        return self.car_position == self.goal or self.steps > 3000 or self.car_position[0] < 0 or self.car_position[1] < 0 or self.car_position[0] > 11 or self.car_position[1] > 11\n",
        "\n",
        "    def get_state(self):\n",
        "        car_position = [float(coord) for coord in self.car_position]\n",
        "        sensor_readings = [float(value) for value in self.sensor_readings.values()]\n",
        "\n",
        "        state = car_position + [self.car_orientation] + sensor_readings\n",
        "\n",
        "        # cast state to this ['1.0' '1.0' 'N' '1.0' '1.0' '10.0']\n",
        "        state = np.array(state, dtype=str)\n",
        "\n",
        "        # get the orientation and convert do label encoding\n",
        "        if state[2] == 'N':\n",
        "            state[2] = 0\n",
        "        elif state[2] == 'E':\n",
        "            state[2] = 1\n",
        "        elif state[2] == 'S':\n",
        "            state[2] = 2\n",
        "        elif state[2] == 'W':\n",
        "            state[2] = 3\n",
        "\n",
        "        state = np.array(state, dtype=float)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def load_image(self, image_path, width, height):\n",
        "        image = pygame.image.load(image_path)\n",
        "        image = pygame.transform.scale(image, (width, height))\n",
        "        return image\n",
        "\n",
        "    def draw_maze(self):\n",
        "        for y in range(self.maze_size_y):\n",
        "            for x in range(self.maze_size_x):\n",
        "                if self.maze[y][x] == 1:\n",
        "                    self.screen.blit(\n",
        "                        self.wall_image, (x * self.cell_width, y * self.cell_height)\n",
        "                    )\n",
        "\n",
        "                    self.screen.blit(\n",
        "                        self.top_of_wall, (x * self.cell_width, y * self.cell_height)\n",
        "                    )\n",
        "                    # add top of wall\n",
        "                if self.maze[y][x] == 0:\n",
        "                    self.screen.blit(\n",
        "                        self.floor_image, (x * self.cell_width, y * self.cell_height)\n",
        "                    )\n",
        "\n",
        "    def draw_car(self):\n",
        "        if self.car_orientation == \"N\":\n",
        "            car_image = pygame.transform.rotate(self.car_image, 180)\n",
        "        elif self.car_orientation == \"E\":\n",
        "            car_image = pygame.transform.rotate(self.car_image, 90)\n",
        "        elif self.car_orientation == \"S\":\n",
        "            car_image = self.car_image\n",
        "        elif self.car_orientation == \"W\":\n",
        "            car_image = pygame.transform.rotate(self.car_image, 270)\n",
        "\n",
        "        self.screen.blit(\n",
        "            car_image,\n",
        "            (\n",
        "                self.car_position[0] * self.cell_width,\n",
        "                self.car_position[1] * self.cell_height,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def draw_goal(self):\n",
        "        self.screen.blit(\n",
        "            self.goal_image,\n",
        "            (self.goal[0] * self.cell_width, self.goal[1] * self.cell_height),\n",
        "        )\n",
        "\n",
        "    def render(self,render_mode='human', framerate=60, delay=0):\n",
        "        if render_mode == 'human':\n",
        "            self.draw_maze()\n",
        "            self.draw_car()\n",
        "            self.draw_goal()\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(framerate)  \n",
        "        elif render_mode == 'rgb_array':\n",
        "            rendered_maze = np.array(self.maze, dtype=str)\n",
        "            x, y = self.car_position\n",
        "            rendered_maze[y][x] = 'C'\n",
        "            # print array\n",
        "            print(rendered_maze, '\\n')\n",
        "\n",
        "    def close_pygame(self):\n",
        "        # Close the Pygame window\n",
        "        pygame.quit()"
      ],
      "id": "bb0cb7a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DDQN\n"
      ],
      "id": "c06a23bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "class DQNAgent:\n",
        "    def __init__(self, replayCapacity, input_shape, output_shape, learning_rate=0.001, discount_factor=0.90):\n",
        "        self.capacity = replayCapacity\n",
        "        self.memory = collections.deque(maxlen=self.capacity)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.policy_model = self.buildNetwork()\n",
        "        self.target_model = self.buildNetwork()\n",
        "        self.target_model.set_weights(self.policy_model.get_weights())\n",
        "\n",
        "    def addToReplayMemory(self, step):\n",
        "        self.step = step\n",
        "        self.memory.append(self.step)\n",
        "\n",
        "    def sampleFromReplayMemory(self, batchSize):\n",
        "        self.batchSize = batchSize\n",
        "        if self.batchSize > len(self.memory):\n",
        "            self.populated = False\n",
        "            return self.populated\n",
        "        else:\n",
        "            return random.sample(self.memory, self.batchSize)\n",
        "\n",
        "    def buildNetwork(self):\n",
        "        \"\"\"\n",
        "        Builds and compiles the neural network. This is a helper to allow subclasses to override the build method in order to add more features such as learning rate and loss.\n",
        "\n",
        "\n",
        "        @return A Sequential object that can be used to train the\n",
        "        \"\"\"\n",
        "        model = Sequential(\n",
        "            [\n",
        "                Input(shape=self.input_shape),\n",
        "                Dense(32, activation=\"relu\"),\n",
        "                Dense(64, activation=\"relu\"),\n",
        "                Dense(32, activation=\"relu\"),\n",
        "                Dense(self.output_shape, activation=\"linear\"),\n",
        "            ]\n",
        "        )\n",
        "        model.compile(\n",
        "            loss=mse,\n",
        "            optimizer=Adam(learning_rate=self.learning_rate),\n",
        "            metrics=[\"mean_squared_error\"],  # Use the string identifier\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def policy_network_fit(self, batch, batch_size):\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "\n",
        "        # Predict Q-values for starting state using the policy network\n",
        "        q_values = self.policy_model.predict(states)\n",
        "\n",
        "        # Predict Q-values for next state using the policy network\n",
        "        q_values_next_state_policy = self.policy_model.predict(next_states)\n",
        "\n",
        "        # Select the best action for the next state using the policy network\n",
        "        best_actions = np.argmax(q_values_next_state_policy, axis=1)\n",
        "\n",
        "        # Predict Q-values for next state using the target network\n",
        "        q_values_next_state_target = self.target_model.predict(next_states)\n",
        "\n",
        "        # Update Q-values for actions taken\n",
        "        for i in range(batch_size):\n",
        "            if dones[i]:\n",
        "                q_values[i, actions[i]] = rewards[i]\n",
        "            else:\n",
        "                # Double DQN update rule\n",
        "                q_values[i, actions[i]] = rewards[i] + self.discount_factor * q_values_next_state_target[i, best_actions[i]]\n",
        "\n",
        "        # Train the policy network\n",
        "        History = self.policy_model.fit(states, q_values, batch_size=batch_size, verbose=0)\n",
        "        return History\n",
        "\n",
        "    def policy_network_predict(self, state):\n",
        "        self.state = state\n",
        "        self.qPolicy = self.policy_model.predict(self.state)\n",
        "        return self.qPolicy\n",
        "\n",
        "    def target_network_predict(self, state):\n",
        "        self.state = state\n",
        "        self.qTarget = self.target_model.predict(self.state)\n",
        "        return self.qTarget\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_model.set_weights(self.policy_model.get_weights())"
      ],
      "id": "a18be3f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### training\n"
      ],
      "id": "f4be1f94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "env = RCMazeEnv()\n",
        "state = env.reset()\n",
        "\n",
        "\n",
        "# Model parameters\n",
        "REPLAY_MEMORY_CAPACITY = 2000000\n",
        "# MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
        "POSSIBLE_ACTIONS = env.possible_actions\n",
        "\n",
        "# state = state[0]\n",
        "# create DQN agent\n",
        "agent = DQNAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, \n",
        "                input_shape=state.shape, \n",
        "                output_shape=len(POSSIBLE_ACTIONS),\n",
        "                learning_rate=0.001, \n",
        "                discount_factor=0.90)\n",
        "\n",
        "# reset the parameters\n",
        "DISCOUNT = 0.90\n",
        "BATCH_SIZE = 128  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_INTERVAL = 2\n",
        "EPSILON = 0.99 # Exploration percentage\n",
        "MIN_EPSILON = 0.01\n",
        "DECAY = 0.99973\n",
        "EPISODE_AMOUNT = 175\n",
        "\n",
        "# Fill the replay memory with the first batch of samples\n",
        "update_counter = 0\n",
        "reward_history = []\n",
        "epsilon_history = []\n",
        "loss_history = []\n",
        "epsilon_history = []\n",
        "mse_history = []\n",
        "\n",
        "# step history for each episode\n",
        "episode_path_history = []\n",
        "step_history = []\n",
        "action_history = []\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "for episode in range(EPISODE_AMOUNT):\n",
        "    episode_reward = 0\n",
        "    step_counter = 0  # count the number of successful steps within the episode\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    epsilon_history.append(EPSILON)\n",
        "\n",
        "    while not done:\n",
        "        env.render(delay=0, framerate=720)\n",
        "\n",
        "        if random.random() <= EPSILON:\n",
        "            action = random.sample(POSSIBLE_ACTIONS, 1)[0]\n",
        "        else:\n",
        "            qValues = agent.policy_network_predict(state.reshape(1,-1))\n",
        "            action = np.argmax(qValues[0])\n",
        "            \n",
        "        # add the action to the action history\n",
        "        action_history.append((episode, action))\n",
        "\n",
        "        new_state, reward, done = env.step(action)\n",
        "\n",
        "        step_counter +=1\n",
        "\n",
        "        # store step in replay memory\n",
        "        step = (state, action, reward, new_state, done)\n",
        "        agent.addToReplayMemory(step)\n",
        "        # add the current car position to the episode path history with the episode number\n",
        "        episode_path_history.append((episode, env.car_position))\n",
        "        \n",
        "        \n",
        "        state = new_state\n",
        "        episode_reward += reward\n",
        "        # When enough steps in replay memory -> train policy network\n",
        "        if len(agent.memory) >= (BATCH_SIZE):\n",
        "            EPSILON = DECAY * EPSILON\n",
        "            if EPSILON < MIN_EPSILON:\n",
        "                EPSILON = MIN_EPSILON\n",
        "            # sample minibatch from replay memory\n",
        "\n",
        "            miniBatch = agent.sampleFromReplayMemory(BATCH_SIZE)\n",
        "            miniBatch_states = np.asarray(list(zip(*miniBatch))[0],dtype=float)\n",
        "            miniBatch_actions = np.asarray(list(zip(*miniBatch))[1], dtype = int)\n",
        "            miniBatch_rewards = np.asarray(list(zip(*miniBatch))[2], dtype = float)\n",
        "            miniBatch_next_state = np.asarray(list(zip(*miniBatch))[3],dtype=float)\n",
        "            miniBatch_done = np.asarray(list(zip(*miniBatch))[4],dtype=bool)\n",
        "\n",
        "            # current state q values1tch_states)\n",
        "            y = agent.policy_network_predict(miniBatch_states)\n",
        "\n",
        "            next_state_q_values = agent.target_network_predict(miniBatch_next_state)\n",
        "            max_q_next_state = np.max(next_state_q_values,axis=1)\n",
        "\n",
        "            for i in range(BATCH_SIZE):\n",
        "                if miniBatch_done[i]:\n",
        "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i]\n",
        "                else:\n",
        "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i] + DISCOUNT *  max_q_next_state[i]\n",
        "\n",
        "            # fit model\n",
        "            history = agent.policy_network_fit(miniBatch, BATCH_SIZE)\n",
        "\n",
        "            loss_history.append(history.history['loss'])\n",
        "            mse_history.append(history.history[\"mean_squared_error\"])\n",
        "\n",
        "        else:\n",
        "            continue\n",
        "        if update_counter == UPDATE_TARGET_INTERVAL:\n",
        "            agent.update_target_network()\n",
        "            update_counter = 0\n",
        "        update_counter += 1\n",
        "    print('episodeReward for episode ', episode, '= ', episode_reward, 'with epsilon = ', EPSILON, 'and steps = ', step_counter)\n",
        "    step_history.append(step_counter)\n",
        "    reward_history.append(episode_reward)\n",
        "\n",
        "\n",
        "env.close_pygame()\n",
        "env.close()"
      ],
      "id": "d0194f3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "env.close_pygame()"
      ],
      "id": "bdaf3c05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### results\n"
      ],
      "id": "da0a48d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot the reward history\n",
        "plt.plot(reward_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title(\"Reward History for DDQN\")\n",
        "plt.savefig('../images/training_images/reward_history_DDQN.png')\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "c6f355aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_reward_distribution(rewards, title=\"Reward Distribution\"):\n",
        "    plt.hist(rewards, bins=30, alpha=0.7, color=\"blue\")\n",
        "    plt.xlabel(\"Reward\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(\"../images/training_images/reward_distribution_DDQN.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming `rewards` is a list of total rewards per episode.\n",
        "# plot_reward_distribution(rewards)\n",
        "plot_reward_distribution(reward_history, title=\"Reward Distribution for DDQN\")"
      ],
      "id": "8b00caf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot epsilon over time\n",
        "plt.plot(epsilon_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon')\n",
        "plt.title(\"Epsilon History for DDQN\")\n",
        "plt.savefig(\"../images/training_images/epsilon_history_DDQN.png\")\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "7715108e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# evalution\n",
        "\n",
        "# plot mse\n",
        "plt.plot(mse_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title(\"Mean Squared Error over time for DDQN\")\n",
        "plt.savefig(\"../images/training_images/mse_history_DDQN.png\")\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "3dc05b6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assuming mse_history is a flat list with the correct number of elements\n",
        "EPISODE_AMOUNT = len(mse_history)  # Make sure this reflects the actual number of episodes\n",
        "desired_samples = 170  # The number of points you want to plot\n",
        "\n",
        "# Calculate the step size\n",
        "step = max(EPISODE_AMOUNT // desired_samples, 1)  # Avoid division by zero\n",
        "sampled_mse_history = mse_history[::step]\n",
        "\n",
        "# Ensure sampled_episodes has the same number of elements as sampled_mse_history\n",
        "sampled_episodes = list(range(0, EPISODE_AMOUNT, step))[:len(sampled_mse_history)]\n",
        "\n",
        "plt.plot(sampled_episodes, sampled_mse_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title(\"Mean Squared Error over time (Sampled) for DDQN\")\n",
        "plt.savefig(\"../images/training_images/mse_history_sampled_DDQN.png\")\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "c693b9b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "weights, biases = agent.policy_model.layers[0].get_weights()\n",
        "plt.hist(weights.reshape(-1), bins=50)\n",
        "plt.title(\"Distribution of Weights - First Layer for DDQN\")\n",
        "plt.savefig(\"../images/training_images/weights_distribution_first_layer_DDQN.png\")\n",
        "plt.show()\n",
        "# save the image\n",
        "\n",
        "# Assuming the model has 3 layers\n",
        "weights, biases = agent.policy_model.layers[1].get_weights()\n",
        "plt.hist(weights.reshape(-1), bins=50)\n",
        "plt.title(\"Distribution of Weights - Second Layer for DDQN\")\n",
        "plt.savefig(\"../images/training_images/weights_distribution_second_layer_DDQN.png\")\n",
        "plt.show()\n",
        "# save the image\n",
        "\n",
        "\n",
        "# Assuming the model has 3 layers\n",
        "weights, biases = agent.policy_model.layers[2].get_weights()\n",
        "plt.hist(weights.reshape(-1), bins=50)\n",
        "plt.title(\"Distribution of Weights - Third Layer for DDQN\")\n",
        "plt.savefig(\"../images/training_images/weights_distribution_third_layer_DDQN.png\")\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "7980fdb1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# save model\n",
        "agent.policy_model.save('../models/DDQN_RCmaze_v3.h5')"
      ],
      "id": "74a245b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate moving average\n",
        "window_size = 10\n",
        "moving_avg = np.convolve(step_history, np.ones(window_size) / window_size, mode=\"valid\")\n",
        "\n",
        "low_point = np.argmin(moving_avg)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(step_history, label=\"Steps per Episode\", color=\"lightgray\")\n",
        "plt.plot(\n",
        "    range(window_size - 1, len(step_history)),\n",
        "    moving_avg,\n",
        "    label=\"Moving Average\",\n",
        "    color=\"blue\",\n",
        ")\n",
        "# Highlight the lowest point\n",
        "plt.scatter(low_point, moving_avg[low_point], color=\"red\", label=\"Lowest Point\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Steps\")\n",
        "plt.ylim(10, 150)\n",
        "plt.title(\"Average Steps per Episode with Moving Average for DDQN\")\n",
        "plt.legend()\n",
        "plt.savefig(\"../images/training_images/steps_per_episode_with_moving_avg_DDQN.png\")\n",
        "plt.show()"
      ],
      "id": "1798ed66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assuming maze_size_x and maze_size_y are the dimensions of your maze\n",
        "# convert to dictionary\n",
        "episode_path_dict = {}\n",
        "for episode, position in episode_path_history:\n",
        "    if episode in episode_path_dict:\n",
        "        episode_path_dict[episode].append(position)\n",
        "    else:\n",
        "        episode_path_dict[episode] = [position]\n",
        "\n",
        "maze = env.maze\n",
        "\n",
        "visit_counts = np.zeros((maze.shape[0], maze.shape[1]), dtype=int)\n",
        "\n",
        "visit_counts[env.maze == 1] = -1\n",
        "print(visit_counts)\n",
        "\n",
        "\n",
        "for episode, path in episode_path_dict.items():\n",
        "    for position in path:\n",
        "        visit_counts[position] += 1\n",
        "\n",
        "from math import nan\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap, LogNorm\n",
        "\n",
        "\n",
        "def plot_visit_heatmap(visit_counts, title=\"Visit Heatmap\"):\n",
        "    # Transpose visit_counts to match the expected orientation\n",
        "    visit_counts_transposed = visit_counts.T  # Transpose the matrix\n",
        "\n",
        "    # Filter out the wall cells by setting their count to NaN for visualization\n",
        "    filtered_counts = np.where(\n",
        "        visit_counts_transposed == -1, np.nan, visit_counts_transposed\n",
        "    )\n",
        "\n",
        "    # Define a continuous colormap (you can choose any colormap you like)\n",
        "    cmap = plt.cm.plasma\n",
        "    cmap.set_bad(\"white\")  # Use gray for NaN (walls)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    # Use LogNorm for logarithmic normalization; set vmin to a small value > 0 to handle cells with 0 visits\n",
        "    plt.imshow(\n",
        "        filtered_counts,\n",
        "        cmap=cmap,\n",
        "        norm=LogNorm(vmin=0.1, vmax=np.nanmax(filtered_counts)),\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    # add the nr of visits to the cells\n",
        "    for i in range(visit_counts_transposed.shape[0]):\n",
        "        for j in range(visit_counts_transposed.shape[1]):\n",
        "            if visit_counts_transposed[i, j] != -1 or visit_counts_transposed[i, j] != nan:\n",
        "                plt.text(j, i, visit_counts_transposed[i, j], ha=\"center\", va=\"center\")\n",
        "\n",
        "    plt.colorbar(label=\"Number of Visits\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(\"../images/training_images/visit_heatmap_DDQN.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "\n",
        "plot_visit_heatmap(visit_counts, title=\"Visit Heatmap for DDQN\")"
      ],
      "id": "ff9df810",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_episode_lengths(episode_lengths, title=\"Episode Length Over Time\"):\n",
        "    plt.plot(episode_lengths, label=\"Episode Length\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Length\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    # y limit to avoid showing outliers\n",
        "    plt.ylim(0, 100)\n",
        "    plt.savefig(\"../images/training_images/episode_length_DDQN.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "print(step_history)\n",
        "\n",
        "plot_episode_lengths(step_history, title=\"Episode Length Over Time for DDQN\")"
      ],
      "id": "558f565f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### test\n"
      ],
      "id": "2e7312c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# try it out\n",
        "# load model\n",
        "env = RCMazeEnv()\n",
        "state = env.reset()\n",
        "\n",
        "REPLAY_MEMORY_CAPACITY = 2000000\n",
        "POSSIBLE_ACTIONS = env.possible_actions\n",
        "\n",
        "# create DQN agent\n",
        "test_agent = DQNAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, input_shape=state.shape, output_shape=len(POSSIBLE_ACTIONS))\n",
        "\n",
        "test_agent.policy_model = load_model('../models/DDQN_RCmaze_v3.h5')\n",
        "\n",
        "\n",
        "done = False\n",
        "\n",
        "rewards = []\n",
        "solution_path = []\n",
        "test_steps = 0\n",
        "\n",
        "while not done:\n",
        "    env.render(delay=100, framerate=10)\n",
        "    qValues = test_agent.policy_network_predict(np.array([state]))\n",
        "    action = np.argmax(qValues[0])\n",
        "    state, reward, done = env.step(action)\n",
        "    rewards.append(reward)\n",
        "    test_steps += 1\n",
        "    print(env.car_position)\n",
        "    solution_path.append(env.car_position)\n",
        "    env.render()\n",
        "    if done:\n",
        "        print('done in ', len(rewards), 'steps')\n",
        "        break\n",
        "env.close()\n",
        "print(sum(rewards))\n",
        "print('steps: ', test_steps)\n",
        "env.close_pygame()"
      ],
      "id": "7345dfdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_agent.policy_model.summary()"
      ],
      "id": "90d1b836",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# test_agent.policy_model.save('./main_web_app/models/DDQN_RCmaze_v2.h5')# "
      ],
      "id": "c68758f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_maze_solution(maze, path, title=\"Maze Solution\"):\n",
        "    plt.imshow(maze, cmap=\"binary\")\n",
        "    x, y = zip(*path)  # Assuming path is a list of (x, y) tuples\n",
        "    plt.plot(\n",
        "        x, y, marker=\"o\", color=\"r\", markersize=5\n",
        "    ) \n",
        "    plt.xticks([])  # Remove x-axis ticks\n",
        "    plt.yticks([])  # Remove y-axis ticks\n",
        "    plt.title(title)\n",
        "    # show nr of steps\n",
        "    plt.text(1, 1, f\"Steps: {len(path)}\", color=\"red\", fontsize=12)\n",
        "    plt.savefig(\"../images/training_images/maze_solution_DDQN.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# get the path\n",
        "path = solution_path\n",
        "print(path)\n",
        "plot_maze_solution(env.maze, path, title=\"Maze Solution for DDQN\")"
      ],
      "id": "2fc4c39d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO\n"
      ],
      "id": "0eb32e18"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError as mse\n",
        "\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, env, action_dim):\n",
        "        self.env = env\n",
        "        self.clip_epsilon = 0.2\n",
        "        self.optimizer = Adam(learning_rate=0.001)\n",
        "    \n",
        "        self.action_dim = action_dim\n",
        "        self.policy_network = self.build_policy_network()\n",
        "        self.value_network = self.build_value_network()\n",
        "\n",
        "    def build_policy_network(self):\n",
        "    # Create a policy network appropriate for your custom environment\n",
        "        policy_network = Sequential([\n",
        "            Input(shape=(6,)),  # Adjust the input shape to (None, 6)\n",
        "            Dense(32, activation='tanh'),\n",
        "            Dense(64, activation='tanh'),\n",
        "            Dense(32, activation='tanh'),\n",
        "            Dense(self.action_dim, activation='softmax')\n",
        "        ])\n",
        "        return policy_network\n",
        "\n",
        "    def build_value_network(self):\n",
        "        # Create a value network appropriate for your custom environment\n",
        "        value_network = Sequential([\n",
        "            Input(shape=(6,)),  # Adjust the input shape to (None, 6)\n",
        "            Dense(32, activation='tanh'),\n",
        "            Dense(64, activation='tanh'),\n",
        "            Dense(32, activation='tanh'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "        return value_network\n",
        "\n",
        "    def process_observation(self, observation):\n",
        "        \"\"\"Flatten observation array and check its validity.\"\"\"\n",
        "        flattened_observation = np.hstack(observation)\n",
        "        if flattened_observation.shape[0] != 6:\n",
        "            raise ValueError(\"Invalid observation shape.\")\n",
        "        return flattened_observation\n",
        "\n",
        "    def compute_discounted_rewards(self, rewards, gamma=0.99):\n",
        "        \"\"\"Compute discounted rewards for the episode.\"\"\"\n",
        "        discounted_rewards = np.zeros_like(rewards)\n",
        "        R = 0\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            R = rewards[i] + gamma * R\n",
        "            discounted_rewards[i] = R\n",
        "        return discounted_rewards\n",
        "\n",
        "    def compute_advantages(self, discounted_rewards, observations):\n",
        "        \"\"\"Compute advantages based on discounted rewards and observations.\"\"\"\n",
        "        baseline = np.mean(discounted_rewards)\n",
        "        advantages = discounted_rewards - baseline\n",
        "        return advantages\n",
        "\n",
        "    def compute_loss(self, observations, actions, advantages, old_probabilities):\n",
        "        \"\"\"Compute the PPO loss.\"\"\"\n",
        "        new_probabilities = self.policy_network(observations)\n",
        "        action_masks = tf.one_hot(actions, self.action_dim, dtype=tf.float32)\n",
        "        new_action_probabilities = tf.reduce_sum(action_masks * new_probabilities, axis=1)\n",
        "        old_action_probabilities = tf.reduce_sum(action_masks * old_probabilities, axis=1)\n",
        "\n",
        "        ratio = new_action_probabilities / (old_action_probabilities + 1e-10)\n",
        "        clipped_ratio = tf.clip_by_value(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
        "        surrogate_objective = tf.minimum(ratio * advantages, clipped_ratio * advantages)\n",
        "        loss = -tf.reduce_mean(surrogate_objective)\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    # def train_step(self, observations, actions, advantages, old_probabilities):\n",
        "    #     \"\"\"Train the network with one step of samples.\"\"\"\n",
        "    #     with tf.GradientTape() as tape:\n",
        "    #         loss = self.compute_loss(observations, actions, advantages, old_probabilities)\n",
        "    #     grads = tape.gradient(loss, self.policy_network.trainable_variables)\n",
        "    #     self.optimizer.apply_gradients(zip(grads, self.policy_network.trainable_variables))\n",
        "    #     return loss.numpy()\n",
        "\n",
        "    def train_step(self, observations, actions, advantages, old_probabilities):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.compute_loss(observations, actions, advantages, old_probabilities)\n",
        "        grads = tape.gradient(loss, self.policy_network.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.policy_network.trainable_variables))\n",
        "        return K.eval(loss)  # Evaluate the tensor explicitly\n",
        "\n",
        "\n",
        "    def train_episode(self,episode, max_steps):\n",
        "        \"\"\"Run one episode of training, collecting necessary data and updating the network.\"\"\"\n",
        "        observations, actions, rewards, probabilities = [], [], [], []\n",
        "        total_reward = 0\n",
        "        observation = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        path_history = []\n",
        "        while not done:\n",
        "\n",
        "            flattened_observation = self.process_observation(observation)\n",
        "            action_probabilities = self.policy_network.predict(np.expand_dims(flattened_observation, axis=0))\n",
        "            # action_probabilities = self.policy_network(np.expand_dims(flattened_observation, axis=0), training=False)\n",
        "\n",
        "            action = np.random.choice(self.action_dim, p=action_probabilities.ravel())\n",
        "            \n",
        "            # render the environment\n",
        "            # self.env.render(framerate=720, delay=0)\n",
        "            observation, reward, done = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            observations.append(flattened_observation)\n",
        "            path_history.append((episode, self.env.car_position))\n",
        "            if done:\n",
        "                reward += 10000\n",
        "                total_reward += 10000\n",
        "\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            probabilities.append(action_probabilities.ravel())\n",
        "\n",
        "        discounted_rewards = self.compute_discounted_rewards(rewards)\n",
        "        advantages = self.compute_advantages(discounted_rewards, observations)\n",
        "\n",
        "        loss = self.train_step(np.vstack(observations), np.array(actions), advantages, np.vstack(probabilities))\n",
        "        return total_reward, loss, path_history\n",
        "\n",
        "    def plot_and_save_results(self, reward_history, loss_history,episode_path_history):\n",
        "        # Reward history plot\n",
        "        plt.plot(reward_history)\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.title(\"Reward History for PPO\")\n",
        "        plt.savefig('../images/training_images/reward_history_PPO.png')\n",
        "        plt.show()\n",
        "\n",
        "        # check -\n",
        "        self.plot_visit_heatmap(episode_path_history, title=\"Visit Heatmap for PPO\")\n",
        "\n",
        "        # print reward distribution\n",
        "        self.plot_reward_distribution(reward_history, title=\"Reward Distribution for PPO\")\n",
        "\n",
        "        try:\n",
        "            self.plot_move_avg_reward(reward_history)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            plt.figure()\n",
        "            plt.plot(loss_history)\n",
        "            plt.title(\"Loss over Episodes for PPO\")\n",
        "            plt.xlabel(\"Episode\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.savefig(\"../images/training_images/loss_history.png\")\n",
        "            plt.show()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def plot_move_avg_reward(self, reward_history, window_size=10):\n",
        "                # moving average\n",
        "        window_size = 10\n",
        "        moving_avg = np.convolve(reward_history, np.ones(window_size) / window_size, mode=\"valid\")\n",
        "\n",
        "        low_point = np.argmin(moving_avg)\n",
        "\n",
        "        # Plotting\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(reward_history, label=\"Reward per Episode\", color=\"lightgray\")\n",
        "        plt.plot( range(window_size - 1, len(reward_history)), moving_avg, label=\"Moving Average\", color=\"blue\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.title(\"Average Reward per Episode with Moving Average for PPO\")\n",
        "        plt.legend()\n",
        "        plt.savefig(\"../images/training_images/reward_per_episode_with_moving_avg_PPO.png\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_reward_distribution(self, rewards, title=\"Reward Distribution\"):\n",
        "        plt.hist(rewards, bins=30, alpha=0.7, color=\"blue\")\n",
        "        plt.xlabel(\"Reward\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.title(title)\n",
        "        plt.savefig(\"../images/training_images/reward_distribution_PPO.png\")\n",
        "        plt.show()\n",
        "        \n",
        "\n",
        "    def plot_mse_history(self, mse_history):\n",
        "        plt.plot(mse_history)\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('loss')\n",
        "        plt.title(\"loss over time for PPO\")\n",
        "        plt.savefig(\"../images/training_images/mse_history_PPO.png\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_visit_heatmap(self, episode_path_history, title=\"Visit Heatmap for PPO\"):\n",
        "\n",
        "        from math import nan\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "        from matplotlib.colors import ListedColormap, LogNorm\n",
        "\n",
        "        # convert to dictionary\n",
        "        episode_path_dict = {}\n",
        "        for episode, position in episode_path_history:\n",
        "            if episode in episode_path_dict:\n",
        "                episode_path_dict[episode].append(position)\n",
        "            else:\n",
        "                episode_path_dict[episode] = [position]\n",
        "\n",
        "        maze = self.env.maze\n",
        "\n",
        "        visit_counts = np.zeros((maze.shape[0], maze.shape[1]), dtype=int)\n",
        "\n",
        "\n",
        "\n",
        "        for episode, path in episode_path_dict.items():\n",
        "            for position in path:\n",
        "                visit_counts[position] += 1\n",
        "\n",
        "        # visit_counts[env.maze == 1] = -1\n",
        "        print(visit_counts)\n",
        "\n",
        "        # Transpose visit_counts to match the expected orientation\n",
        "        visit_counts_transposed = visit_counts.T  # Transpose the matrix\n",
        "\n",
        "        # Filter out the wall cells by setting their count to NaN for visualization\n",
        "        # filtered_counts = np.where(\n",
        "        #     visit_counts_transposed == -1, np.nan, visit_counts_transposed\n",
        "        # )\n",
        "        filtered_counts = visit_counts_transposed\n",
        "        # set the walls to -1\n",
        "        filtered_counts[env.maze == 1] = -1\n",
        "        \n",
        "\n",
        "        # Define a continuous colormap (you can choose any colormap you like)\n",
        "        cmap = plt.cm.plasma\n",
        "        cmap.set_bad(\"white\")  # Use gray for NaN (walls)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        # Use LogNorm for logarithmic normalization; set vmin to a small value > 0 to handle cells with 0 visits\n",
        "        plt.imshow(\n",
        "            filtered_counts,\n",
        "            cmap=cmap,\n",
        "            norm=LogNorm(vmin=0.1, vmax=np.nanmax(filtered_counts)),\n",
        "            interpolation=\"nearest\",\n",
        "        )\n",
        "        # add the nr of visits to the cells\n",
        "        for i in range(visit_counts_transposed.shape[0]):\n",
        "            for j in range(visit_counts_transposed.shape[1]):\n",
        "                if visit_counts_transposed[i, j] != -1 or visit_counts_transposed[i, j] != nan:\n",
        "                    plt.text(j, i, visit_counts_transposed[i, j], ha=\"center\", va=\"center\")\n",
        "\n",
        "        plt.colorbar(label=\"Number of Visits\")\n",
        "        plt.title(title)\n",
        "        plt.savefig(\"../images/training_images/visit_heatmap_PPO.png\")\n",
        "        plt.show()\n",
        "            # save the image\n",
        "\n",
        "\n",
        "    def train(self, num_episodes, max_steps_per_episode):\n",
        "        \"\"\"Main training loop.\"\"\"\n",
        "        reward_history = []\n",
        "        loss_history = []\n",
        "        episode_path_history = []\n",
        "        for episode in range(num_episodes):\n",
        "            self.env.reset()\n",
        "            total_reward, loss, path_history = self.train_episode(episode ,max_steps_per_episode)\n",
        "            reward_history.append(total_reward)\n",
        "            loss_history.append(loss)\n",
        "            print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}, Loss: {loss}\")\n",
        "            for path in path_history:\n",
        "                episode_path_history.append(path)\n",
        "        \n",
        "        print('reward_history:',reward_history)\n",
        "        print('loss_history:',loss_history)\n",
        "        print('episode_path_history:',episode_path_history)\n",
        "        self.plot_and_save_results(reward_history, loss_history, episode_path_history)"
      ],
      "id": "1b9c1e67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def state_to_tuple( state):\n",
        "      \n",
        "      #((0, 0), 'N', {'front': 1, 'left': 0, 'right': 0})\n",
        "      # if like this convert to ((0, 0), 'N', (1, 0, 0))\n",
        "      if not isinstance(state[2], dict):\n",
        "         # print(state)\n",
        "         # print(state[2])\n",
        "         #take state[2] and make it from this (1, 0, 0) to this {'front': 1, 'left': 0, 'right': 0}\n",
        "         newState = {'front': state[2][0], 'left': state[2][1], 'right': state[2][2]}\n",
        "         # print(newState)\n",
        "         #create a new state with the [2] being the new dictionary\n",
        "         state = (state[0], state[1], newState)\n",
        "         \n",
        "      # Convert the state dictionary to a hashable tuple\n",
        "      # Adjust this based on the specific format of your state\n",
        "      position, orientation, sensor_readings = state\n",
        "      sensor_readings_tuple = tuple(sensor_readings.values())\n",
        "      return (position, orientation, sensor_readings_tuple)"
      ],
      "id": "f5cedd9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tf.experimental.numpy.experimental_enable_numpy_behavior()"
      ],
      "id": "354ecba3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "env = RCMazeEnv()  # Create your custom environment\n",
        "\n",
        "observation_dim = env.reset().shape[0]  # Adjust this based on your custom environment's state space\n",
        "action_dim = 3  # Adjust this based on your custom environment's action space\n",
        "ppo_agent = PPOAgent(env, action_dim=action_dim)\n",
        "\n",
        "ppo_agent.train(num_episodes=175, max_steps_per_episode=3000)"
      ],
      "id": "d0aba361",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# q-agent\n"
      ],
      "id": "1d8c494b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# class QAgent:\n",
        "#     def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1, possible_actions=3, min_epsilon=0.01, epsilon_decay=0.99):\n",
        "#         self.q_table = {}\n",
        "#         self.alpha = alpha\n",
        "#         self.gamma = gamma\n",
        "#         self.epsilon = epsilon\n",
        "#         self.possible_actions = possible_actions\n",
        "#         self.min_epsilon = min_epsilon\n",
        "#         self.epsilon_decay = epsilon_decay\n",
        "\n",
        "#     def get_q_value(self, state, action):\n",
        "#         \"\"\"Retrieve Q value for a given state-action pair, defaults to 0.\"\"\"\n",
        "#         state = tuple(state)  # Convert state to tuple for hashing\n",
        "#         return self.q_table.get((state, action), 0)\n",
        "\n",
        "#     # def choose_action(self, state, test=False):\n",
        "#     #     \"\"\"Choose an action based on epsilon-greedy policy.\"\"\"\n",
        "#     #     state = tuple(state)  # Convert state to tuple for consistent hashing\n",
        "#     #     if random.random() < self.epsilon:\n",
        "#     #         return random.randint(0, self.possible_actions - 1)\n",
        "#     #     else:\n",
        "#     #         q_values = [self.get_q_value(state, action) for action in range(self.possible_actions)]\n",
        "#     #         max_q = max(q_values)\n",
        "#     #         return random.choice([action for action, q in enumerate(q_values) if q == max_q])\n",
        "\n",
        "#     def choose_action(self, state, test=False):\n",
        "#         \"\"\"Choose an action based on epsilon-greedy policy or optimal policy if testing.\"\"\"\n",
        "#         state = tuple(state)  # Convert state to tuple for consistent hashing\n",
        "#         if test:\n",
        "#             return np.argmax([self.get_q_value(state, action) for action in range(self.possible_actions)])\n",
        "#         if not test and random.random() < self.epsilon:\n",
        "#             return random.randint(0, self.possible_actions - 1)\n",
        "#         else:\n",
        "#             q_values = [self.get_q_value(state, action) for action in range(self.possible_actions)]\n",
        "#             max_q = max(q_values)\n",
        "#             return random.choice([action for action, q in enumerate(q_values) if q == max_q])\n",
        "\n",
        "\n",
        "#     def update_q_value(self, state, action, reward, next_state):\n",
        "#         \"\"\"Update Q value for a state-action pair using the learning rule.\"\"\"\n",
        "#         state = tuple(state)  # Convert state to tuple for hashing\n",
        "#         next_state = tuple(next_state)\n",
        "#         max_q_next = max(self.get_q_value(next_state, a) for a in range(self.possible_actions))\n",
        "#         current_q = self.get_q_value(state, action)\n",
        "#         new_q = current_q + self.alpha * (reward + self.gamma * max_q_next - current_q)\n",
        "#         self.q_table[(state, action)] = new_q\n",
        "\n",
        "#     def train(self, environment, num_episodes):\n",
        "#         reward_history = []\n",
        "#         episode_path_history = []\n",
        "#         step_history = []\n",
        "#         epselon_history = []\n",
        "#         for _ in range(num_episodes):\n",
        "#             state = environment.reset()\n",
        "#             state = tuple(state)\n",
        "#             done = False\n",
        "#             total_reward = 0\n",
        "#             step_counter = 0\n",
        "#             while not done:\n",
        "#                 action = self.choose_action(state)\n",
        "#                 next_state, reward, done = environment.step(action)\n",
        "#                 self.update_q_value(state, action, reward, next_state)\n",
        "#                 total_reward += reward\n",
        "#                 state = next_state\n",
        "#                 step_counter += 1\n",
        "#                 episode_path_history.append((_, environment.car_position))\n",
        "#             # Add the total reward for this episode to the history\n",
        "#             reward_history.append(total_reward)\n",
        "#             step_history.append(step_counter)\n",
        "#             epselon_history.append(self.epsilon)\n",
        "#             # Decay epsilon, but not below the minimum value\n",
        "#             self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "#             # Print episode summary\n",
        "#             print(\"Episode finished after {} timesteps\".format(environment.steps))\n",
        "#             print(\n",
        "#                 \"Total reward: {}, Epsilon: {:.3f}\".format(total_reward, self.epsilon)\n",
        "#             )\n",
        "\n",
        "#         return reward_history, episode_path_history, step_history, epselon_history\n",
        "\n",
        "#     def test(self, environment):\n",
        "#         state = environment.reset()\n",
        "#         state = tuple(state)  # Convert initial state to a tuple for consistency\n",
        "#         done = False\n",
        "#         total_reward = 0\n",
        "#         solution_path = []\n",
        "#         while not done:\n",
        "#             environment.render()\n",
        "#             action = self.choose_action(state, test=True)\n",
        "#             next_state, reward, done = environment.step(action)\n",
        "#             next_state = tuple(next_state)  # Convert next_state to a tuple\n",
        "#             state = next_state  # Move to the next state\n",
        "#             total_reward += reward  # Accumulate total reward\n",
        "#             solution_path.append(environment.car_position)\n",
        "\n",
        "#         print(f\"Test Total Reward: {total_reward}\")\n",
        "#         return total_reward, solution_path\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QAgent:\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1, possible_actions=3, min_epsilon=0.01, epsilon_decay=0.99):\n",
        "        self.q_table = {}\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.possible_actions = possible_actions\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        \"\"\"Retrieve Q value for a given state-action pair, defaults to a small random value.\"\"\"\n",
        "        state = tuple(state)  # Convert state to tuple for hashing\n",
        "        return self.q_table.get((state, action), random.uniform(-0.01, 0))\n",
        "\n",
        "    def choose_action(self, state, test=False):\n",
        "        \"\"\"Choose an action based on epsilon-greedy policy or optimal policy if testing.\"\"\"\n",
        "        state = tuple(state)  # Convert state to tuple for consistent hashing\n",
        "        if test:\n",
        "            return np.argmax([self.get_q_value(state, action) for action in range(self.possible_actions)])\n",
        "        if not test and random.random() < self.epsilon:\n",
        "            return random.randint(0, self.possible_actions - 1)\n",
        "        else:\n",
        "            q_values = [self.get_q_value(state, action) for action in range(self.possible_actions)]\n",
        "            max_q = max(q_values)\n",
        "            # In case there are several max Q-values, randomly choose one of them\n",
        "            return random.choice([action for action, q in enumerate(q_values) if q == max_q])\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        \"\"\"Update Q value for a state-action pair using the learning rule.\"\"\"\n",
        "        state = tuple(state)\n",
        "        next_state = tuple(next_state)\n",
        "        max_q_next = max(self.get_q_value(next_state, a) for a in range(self.possible_actions))\n",
        "        current_q = self.get_q_value(state, action)\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * max_q_next - current_q)\n",
        "        self.q_table[(state, action)] = new_q\n",
        "\n",
        "    def train(self, environment, num_episodes):\n",
        "        reward_history = []\n",
        "        episode_path_history = []\n",
        "        step_history = []\n",
        "        epsilon_history = []\n",
        "        for episode in range(num_episodes):\n",
        "            state = environment.reset()\n",
        "            state = tuple(state)\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            step_counter = 0\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done = environment.step(action)\n",
        "                self.update_q_value(state, action, reward, next_state)\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "                step_counter += 1\n",
        "                episode_path_history.append((episode, environment.car_position))\n",
        "            reward_history.append(total_reward)\n",
        "            step_history.append(step_counter)\n",
        "            epsilon_history.append(self.epsilon)\n",
        "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            print(f\"Episode {episode} finished after {step_counter} timesteps, Total reward: {total_reward}, Epsilon: {self.epsilon:.3f}\")\n",
        "\n",
        "        return reward_history, episode_path_history, step_history, epsilon_history\n",
        "\n",
        "    def test(self, environment):\n",
        "        state = environment.reset()\n",
        "        state = tuple(state)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        solution_path = []\n",
        "        while not done:\n",
        "            environment.render()\n",
        "            action = self.choose_action(state, test=True)\n",
        "            next_state, reward, done = environment.step(action)\n",
        "            next_state = tuple(next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            solution_path.append(environment.car_position)\n",
        "\n",
        "        print(f\"Test Total Reward: {total_reward}\")\n",
        "        return total_reward, solution_path"
      ],
      "id": "3c786cae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## training\n"
      ],
      "id": "a0370b48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "EPSILON = 0.99  # High initial exploration\n",
        "ALPHA = 0.05    # Increase learning rate for faster adaptation\n",
        "GAMMA = 0.9     # Focus more on future rewards\n",
        "DECAY_RATE = 0.9998  # Keep as is, standard rate\n",
        "MINEPSILON = 0.05  # Allow for more exploitation as learning progresses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "env = RCMazeEnv()\n",
        "agent = QAgent(\n",
        "    alpha=ALPHA,\n",
        "    gamma=GAMMA,\n",
        "    epsilon=EPSILON,\n",
        "    min_epsilon=MINEPSILON,\n",
        "    epsilon_decay=DECAY_RATE,\n",
        ")\n",
        "# env.init_pygame()\n",
        "reward_history , episode_path_history, step_history, epselon_history = agent.train(env, 5000)\n",
        "env.close_pygame()"
      ],
      "id": "6ee1d8cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## results\n"
      ],
      "id": "d7ac0317"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot the reward history\n",
        "plt.plot(reward_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title(\"Reward History for Q-Learning\")\n",
        "plt.savefig('../images/training_images/reward_history_Qlearning.png')\n",
        "plt.show()\n",
        "# save the image\n",
        "\n",
        "def plot_reward_distribution(rewards, title=\"Reward Distribution\"):\n",
        "    plt.hist(rewards, bins=30, alpha=0.7, color=\"blue\")\n",
        "    plt.xlabel(\"Reward\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(\"../images/training_images/reward_distribution_Q-agent.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "\n",
        "plot_reward_distribution(reward_history, title=\"Reward Distribution for Q-Learning\")"
      ],
      "id": "5dcaf2e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_q_table_3d(agent, action_index):\n",
        "    x = []\n",
        "    y = []\n",
        "    z = []\n",
        "\n",
        "    # Loop through the Q-table and collect x, y, and z only for the given action index\n",
        "    for (state, action), value in agent.q_table.items():\n",
        "        if action == action_index:\n",
        "            x.append(state[0])\n",
        "            y.append(state[1])\n",
        "            z.append(value)\n",
        "\n",
        "    # Convert lists to numpy arrays for plotting\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    z = np.array(z)\n",
        "\n",
        "    # Create a 3D plot\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    scatter = ax.scatter(x, y, z, c=z, cmap='viridis', marker='o')\n",
        "\n",
        "    # Label the axes and set a title\n",
        "    ax.set_xlabel('State Dimension 1')\n",
        "    ax.set_ylabel('State Dimension 2')\n",
        "    ax.set_zlabel('Q-Value')\n",
        "    ax.set_title(f'Q-Values for Action {action_index}')\n",
        "\n",
        "    # Adding a color bar to indicate the scale of Q-values\n",
        "    color_bar = fig.colorbar(scatter, ax=ax, extend='both')\n",
        "    color_bar.set_label('Q-Value')\n",
        "    plt.savefig(f\"../images/training_images/q_table_3d_Q-agent_{action_index}.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_q_table_3d(agent, action_index=0)\n",
        "plot_q_table_3d(agent, action_index=1)\n",
        "plot_q_table_3d(agent, action_index=2)"
      ],
      "id": "186928ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from math import nan\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap, LogNorm\n",
        "\n",
        "# convert to dictionary\n",
        "episode_path_dict = {}\n",
        "for episode, position in episode_path_history:\n",
        "    if episode in episode_path_dict:\n",
        "        episode_path_dict[episode].append(position)\n",
        "    else:\n",
        "        episode_path_dict[episode] = [position]\n",
        "\n",
        "maze = env.maze\n",
        "\n",
        "visit_counts = np.zeros((maze.shape[0], maze.shape[1]), dtype=int)\n",
        "\n",
        "\n",
        "\n",
        "for episode, path in episode_path_dict.items():\n",
        "    for position in path:\n",
        "        visit_counts[position] += 1\n",
        "\n",
        "# visit_counts[env.maze == 1] = -1\n",
        "print(visit_counts)\n",
        "\n",
        "# Transpose visit_counts to match the expected orientation\n",
        "visit_counts_transposed = visit_counts.T  # Transpose the matrix\n",
        "\n",
        "# Filter out the wall cells by setting their count to NaN for visualization\n",
        "# filtered_counts = np.where(\n",
        "#     visit_counts_transposed == -1, np.nan, visit_counts_transposed\n",
        "# )\n",
        "filtered_counts = visit_counts_transposed\n",
        "# set the walls to -1\n",
        "filtered_counts[env.maze == 1] = -1\n",
        "\n",
        "\n",
        "# Define a continuous colormap (you can choose any colormap you like)\n",
        "cmap = plt.cm.plasma\n",
        "cmap.set_bad(\"white\")  # Use gray for NaN (walls)\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Use LogNorm for logarithmic normalization; set vmin to a small value > 0 to handle cells with 0 visits\n",
        "plt.imshow(\n",
        "    filtered_counts,\n",
        "    cmap=cmap,\n",
        "    norm=LogNorm(vmin=0.1, vmax=np.nanmax(filtered_counts)),\n",
        "    interpolation=\"nearest\",\n",
        ")\n",
        "# add the nr of visits to the cells\n",
        "for i in range(visit_counts_transposed.shape[0]):\n",
        "    for j in range(visit_counts_transposed.shape[1]):\n",
        "        if visit_counts_transposed[i, j] != -1 or visit_counts_transposed[i, j] != nan:\n",
        "            plt.text(j, i, visit_counts_transposed[i, j], ha=\"center\", va=\"center\")\n",
        "\n",
        "plt.colorbar(label=\"Number of Visits\")\n",
        "plt.title(\"Visit Heatmap for q-agent\")\n",
        "plt.savefig(\"../images/training_images/visit_heatmap_q-agent.png\")\n",
        "plt.show()\n",
        "    # save the image"
      ],
      "id": "f1846799",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## test\n"
      ],
      "id": "dc97f091"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# test\n",
        "env = RCMazeEnv()\n",
        "\n",
        "\n",
        "# Example of running the environment\n",
        "reward, path = agent.test(env)\n",
        "\n",
        "\n",
        "# env.close_pygame()\n",
        "\n",
        "# plot the solution\n",
        "\n",
        "def plot_maze_solution(maze, path, title=\"Maze Solution\"):\n",
        "    plt.imshow(maze, cmap=\"binary\")\n",
        "    x, y = zip(*path)  # Assuming path is a list of (x, y) tuples\n",
        "    plt.plot(\n",
        "        x, y, marker=\"o\", color=\"r\", markersize=5\n",
        "    )  # Plot the path as red circles\n",
        "    plt.text(1, 1, f\"Steps: {len(path)}\", color=\"red\", fontsize=12)\n",
        "    plt.xticks([])  # Remove x-axis ticks\n",
        "    plt.yticks([])  # Remove y-axis ticks\n",
        "    plt.title(title)\n",
        "    plt.savefig(\"../images/training_images/maze_solution_Q-agent.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "plot_maze_solution(env.maze, path, title=\"Maze Solution for Q-agent\")"
      ],
      "id": "621a5b52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dqn\n"
      ],
      "id": "e599425e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import random\n",
        "# import collections\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Input\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "# mse = MeanSquaredError()\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, replayCapacity, input_shape, output_shape, learning_rate=0.001, discount_factor=0.90):\n",
        "        self.capacity = replayCapacity\n",
        "        self.memory = collections.deque(maxlen=self.capacity)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.model = self.buildNetwork()\n",
        "\n",
        "    def addToReplayMemory(self, step):\n",
        "        self.memory.append(step)\n",
        "\n",
        "    def sampleFromReplayMemory(self, batchSize):\n",
        "        return random.sample(self.memory, batchSize) if len(self.memory) >= batchSize else []\n",
        "\n",
        "    def buildNetwork(self):\n",
        "        model = Sequential([\n",
        "            Input(shape=self.input_shape),\n",
        "            Dense(32, activation=\"relu\"),\n",
        "            Dense(64, activation=\"relu\"),\n",
        "            Dense(32, activation=\"relu\"),\n",
        "            Dense(self.output_shape, activation=\"linear\"),\n",
        "        ])\n",
        "        model.compile(loss=mse, optimizer=Adam(learning_rate=self.learning_rate), metrics=[\"mean_squared_error\"])\n",
        "        return model\n",
        "\n",
        "    def policy_network_fit(self, batch, batch_size):\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "        q_values = self.model.predict(states)\n",
        "        q_values_next = self.model.predict(next_states)\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            if dones[i]:\n",
        "                q_values[i, actions[i]] = rewards[i]\n",
        "            else:\n",
        "                q_values[i, actions[i]] = rewards[i] + self.discount_factor * np.max(q_values_next[i])\n",
        "\n",
        "        return self.model.fit(states, q_values, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    def policy_network_predict(self, state):\n",
        "        return self.model.predict(np.array([state]))\n",
        "\n",
        "# Assume env and RCMazeEnv are defined somewhere\n",
        "env = RCMazeEnv()\n",
        "state = env.reset()\n",
        "\n",
        "REPLAY_MEMORY_CAPACITY = 2000000\n",
        "POSSIBLE_ACTIONS = env.possible_actions\n",
        "\n",
        "agent = DQNAgent(replayCapacity=REPLAY_MEMORY_CAPACITY, \n",
        "                 input_shape=state.shape, \n",
        "                 output_shape=len(POSSIBLE_ACTIONS),\n",
        "                 learning_rate=0.001, \n",
        "                 discount_factor=0.90)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPSILON = 0.99\n",
        "MIN_EPSILON = 0.01\n",
        "DECAY = 0.99993\n",
        "EPISODE_AMOUNT = 175\n",
        "reward_history = []\n",
        "epsilon_history = []\n",
        "mse_history = []\n",
        "episode_path_history = []\n",
        "step_history = []\n",
        "\n",
        "for episode in range(EPISODE_AMOUNT):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "    step_counter = 0\n",
        "    while not done:\n",
        "        if random.random() <= EPSILON:\n",
        "            action = random.choice(POSSIBLE_ACTIONS)\n",
        "        else:\n",
        "            q_values = agent.policy_network_predict(state)\n",
        "            action = np.argmax(q_values[0])\n",
        "        \n",
        "        new_state, reward, done = env.step(action)\n",
        "        step_counter += 1\n",
        "        agent.addToReplayMemory((state, action, reward, new_state, done))\n",
        "        state = new_state\n",
        "        episode_reward += reward\n",
        "        episode_path_history.append((episode, env.car_position))\n",
        "\n",
        "        if len(agent.memory) >= BATCH_SIZE:\n",
        "            batch = agent.sampleFromReplayMemory(BATCH_SIZE)\n",
        "            history = agent.policy_network_fit(batch, BATCH_SIZE)\n",
        "            mse_history.append(history.history['mean_squared_error'])\n",
        "            EPSILON = max(EPSILON * DECAY, MIN_EPSILON)\n",
        "    \n",
        "    reward_history.append(episode_reward)\n",
        "    epsilon_history.append(EPSILON)\n",
        "    step_history.append(step_counter)\n",
        "    print(f'Episode {episode}: Reward = {episode_reward}, Epsilon = {EPSILON}')\n"
      ],
      "id": "a40bb972",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# test out\n",
        "env = RCMazeEnv()\n",
        "state = env.reset()\n",
        "\n",
        "done = False\n",
        "\n",
        "rewards = []\n",
        "solution_path = []\n",
        "test_steps = 0\n",
        "\n",
        "while not done:\n",
        "    env.render(delay=100, framerate=10)\n",
        "    qValues = agent.policy_network_predict(state)\n",
        "    action = np.argmax(qValues[0])\n",
        "    state, reward, done = env.step(action)\n",
        "    rewards.append(reward)\n",
        "    test_steps += 1\n",
        "    print(env.car_position)\n",
        "    solution_path.append(env.car_position)\n",
        "    env.render()\n",
        "    if done:\n",
        "        print('done in ', len(rewards), 'steps')\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(sum(rewards))\n",
        "print('steps: ', test_steps)\n",
        "env.close_pygame()\n",
        "\n",
        "# plot the path\n",
        "def plot_maze_solution(maze, path, title=\"Maze Solution\"):\n",
        "    plt.imshow(maze, cmap=\"binary\")\n",
        "    x, y = zip(*path)  # Assuming path is a list of (x, y) tuples\n",
        "    plt.plot(\n",
        "        x, y, marker=\"o\", color=\"r\", markersize=5\n",
        "    ) \n",
        "    plt.xticks([])  # Remove x-axis ticks\n",
        "    plt.yticks([])  # Remove y-axis ticks\n",
        "    plt.title(title)\n",
        "    # show nr of steps\n",
        "    plt.text(1, 1, f\"Steps: {len(path)}\", color=\"red\", fontsize=12)\n",
        "    plt.savefig(\"../images/training_images/maze_solution_DQN.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "# Example usage\n",
        "plot_maze_solution(env.maze, solution_path, title=\"Maze Solution for DQN\")"
      ],
      "id": "bc1990b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### results\n"
      ],
      "id": "453c415c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot the reward history\n",
        "plt.plot(reward_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title(\"Reward History for DQN\")\n",
        "plt.savefig('../images/training_images/reward_history_DQN.png')\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "4973b954",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_reward_distribution(rewards, title=\"Reward Distribution\"):\n",
        "    plt.hist(rewards, bins=30, alpha=0.7, color=\"blue\")\n",
        "    plt.xlabel(\"Reward\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(\"../images/training_images/reward_distribution_DQN.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming `rewards` is a list of total rewards per episode.\n",
        "# plot_reward_distribution(rewards)\n",
        "plot_reward_distribution(reward_history, title=\"Reward Distribution for DQN\")"
      ],
      "id": "64404f35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot epsilon over time\n",
        "plt.plot(epsilon_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon')\n",
        "plt.title(\"Epsilon History for DQN\")\n",
        "plt.savefig(\"../images/training_images/epsilon_history_DQN.png\")\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "48c01620",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# evalution\n",
        "\n",
        "# plot mse\n",
        "plt.plot(mse_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title(\"Mean Squared Error over time for DQN\")\n",
        "plt.savefig(\"../images/training_images/mse_history_DQN.png\")\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "b088fd3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assuming mse_history is a flat list with the correct number of elements\n",
        "EPISODE_AMOUNT = len(mse_history)  # Make sure this reflects the actual number of episodes\n",
        "desired_samples = 170  # The number of points you want to plot\n",
        "\n",
        "# Calculate the step size\n",
        "step = max(EPISODE_AMOUNT // desired_samples, 1)  # Avoid division by zero\n",
        "sampled_mse_history = mse_history[::step]\n",
        "\n",
        "# Ensure sampled_episodes has the same number of elements as sampled_mse_history\n",
        "sampled_episodes = list(range(0, EPISODE_AMOUNT, step))[:len(sampled_mse_history)]\n",
        "\n",
        "plt.plot(sampled_episodes, sampled_mse_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title(\"Mean Squared Error over time (Sampled) for DQN\")\n",
        "plt.savefig(\"../images/training_images/mse_history_sampled_DQN.png\")\n",
        "plt.show()\n",
        "# save the image"
      ],
      "id": "441dca59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate moving average\n",
        "window_size = 10\n",
        "moving_avg = np.convolve(step_history, np.ones(window_size) / window_size, mode=\"valid\")\n",
        "\n",
        "low_point = np.argmin(moving_avg)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(step_history, label=\"Steps per Episode\", color=\"lightgray\")\n",
        "plt.plot(\n",
        "    range(window_size - 1, len(step_history)),\n",
        "    moving_avg,\n",
        "    label=\"Moving Average\",\n",
        "    color=\"blue\",\n",
        ")\n",
        "# Highlight the lowest point\n",
        "# plt.scatter(low_point, moving_avg[low_point], color=\"red\", label=\"Lowest Point\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Steps\")\n",
        "plt.ylim(10, 150)\n",
        "plt.title(\"Average Steps per Episode with Moving Average for DQN\")\n",
        "plt.legend()\n",
        "plt.savefig(\"../images/training_images/steps_per_episode_with_moving_avg_DQN.png\")\n",
        "plt.show()"
      ],
      "id": "e23f3da2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from math import nan\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap, LogNorm\n",
        "\n",
        "# convert to dictionary\n",
        "episode_path_dict = {}\n",
        "for episode, position in episode_path_history:\n",
        "    if episode in episode_path_dict:\n",
        "        episode_path_dict[episode].append(position)\n",
        "    else:\n",
        "        episode_path_dict[episode] = [position]\n",
        "\n",
        "maze = env.maze\n",
        "\n",
        "visit_counts = np.zeros((maze.shape[0], maze.shape[1]), dtype=int)\n",
        "\n",
        "\n",
        "\n",
        "for episode, path in episode_path_dict.items():\n",
        "    for position in path:\n",
        "        visit_counts[position] += 1\n",
        "\n",
        "# visit_counts[env.maze == 1] = -1\n",
        "print(visit_counts)\n",
        "\n",
        "# Transpose visit_counts to match the expected orientation\n",
        "visit_counts_transposed = visit_counts.T  # Transpose the matrix\n",
        "\n",
        "# Filter out the wall cells by setting their count to NaN for visualization\n",
        "# filtered_counts = np.where(\n",
        "#     visit_counts_transposed == -1, np.nan, visit_counts_transposed\n",
        "# )\n",
        "filtered_counts = visit_counts_transposed\n",
        "# set the walls to -1\n",
        "filtered_counts[env.maze == 1] = -1\n",
        "\n",
        "\n",
        "# Define a continuous colormap (you can choose any colormap you like)\n",
        "cmap = plt.cm.plasma\n",
        "cmap.set_bad(\"white\")  # Use gray for NaN (walls)\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Use LogNorm for logarithmic normalization; set vmin to a small value > 0 to handle cells with 0 visits\n",
        "plt.imshow(\n",
        "    filtered_counts,\n",
        "    cmap=cmap,\n",
        "    norm=LogNorm(vmin=0.1, vmax=np.nanmax(filtered_counts)),\n",
        "    interpolation=\"nearest\",\n",
        ")\n",
        "# add the nr of visits to the cells\n",
        "for i in range(visit_counts_transposed.shape[0]):\n",
        "    for j in range(visit_counts_transposed.shape[1]):\n",
        "        if visit_counts_transposed[i, j] != -1 or visit_counts_transposed[i, j] != nan:\n",
        "            plt.text(j, i, visit_counts_transposed[i, j], ha=\"center\", va=\"center\")\n",
        "\n",
        "plt.colorbar(label=\"Number of Visits\")\n",
        "plt.title(\"Visit Heatmap for DQN\")\n",
        "plt.savefig(\"../images/training_images/visit_heatmap_DQN.png\")\n",
        "plt.show()\n",
        "    # save the image"
      ],
      "id": "164316b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_episode_lengths(episode_lengths, title=\"Episode Length Over Time\"):\n",
        "    plt.plot(episode_lengths, label=\"Episode Length\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Length\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    # y limit to avoid showing outliers\n",
        "    plt.ylim(0, 100)\n",
        "    plt.savefig(\"../images/training_images/episode_length_DDQN.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "print(step_history)\n",
        "\n",
        "plot_episode_lengths(step_history, title=\"Episode Length Over Time for DQN\")"
      ],
      "id": "5cb3102c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# a2c\n"
      ],
      "id": "76629d2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class ActorCritic(tf.keras.Model):\n",
        "    def __init__(self, num_actions, state_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = self.create_actor_network(num_actions, state_dim)\n",
        "        self.critic = self.create_critic_network(state_dim)\n",
        "\n",
        "    def create_actor_network(self, num_actions, state_dim):\n",
        "        actor = tf.keras.Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(state_dim,)),\n",
        "            Dense(num_actions, activation='softmax')\n",
        "        ])\n",
        "        return actor\n",
        "\n",
        "    def create_critic_network(self, state_dim):\n",
        "        critic = tf.keras.Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(state_dim,)),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        return critic\n",
        "\n",
        "    def call(self, state):\n",
        "        policy_logits = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        return policy_logits, value\n",
        "\n",
        "    def act(self, state):\n",
        "        # convert state from (6,) to (None, 6)\n",
        "        state = tf.expand_dims(state, 0)\n",
        "        policy_logits = self.actor(state)\n",
        "        action_probs = tf.nn.softmax(policy_logits)\n",
        "        action = tf.random.categorical(action_probs, 1)\n",
        "        return action\n",
        "\n",
        "    def value(self, state):\n",
        "        return self.critic(state)"
      ],
      "id": "b280f49d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## train\n"
      ],
      "id": "eaf72f73"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_actions = 3\n",
        "state_dim = 6  # 2D position + orientation + sensor readings\n",
        "agent = ActorCritic(num_actions, state_dim)\n",
        "\n",
        "# Compile the model\n",
        "agent.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
        "\n",
        "# Create a buffer to store experiences\n",
        "buffer = []\n",
        "reward_history = []\n",
        "loss_history = []\n",
        "episode_path_history = []\n",
        "\n",
        "\n",
        "env = RCMazeEnv()\n",
        "# # Train the agent\n",
        "# for episode in range(176):\n",
        "#     state = env.reset()\n",
        "#     done = False\n",
        "#     rewards = 0\n",
        "#     while not done:\n",
        "#         # env.render(framerate=720, delay=0)\n",
        "#         action = agent.act(state)\n",
        "#         next_state, reward, done = env.step(action)\n",
        "#         buffer.append((state, action, reward, next_state, done))\n",
        "#         state = next_state\n",
        "#         rewards += reward\n",
        "#         episode_path_history.append((episode, env.car_position))\n",
        "#     print(f\"Episode {episode+1}, Reward: {rewards}\")\n",
        "#     reward_history.append(rewards)\n",
        "\n",
        "#     print(\"Training the agent...\")\n",
        "#     print(\"Buffer length:\", len(buffer))\n",
        "#     if len(buffer) > 0:\n",
        "#         for x in range(10):\n",
        "#             try:\n",
        "#                 states, actions, rewards, next_states, dones = zip(*buffer)\n",
        "#                 states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "#                 actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "#                 rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "#                 next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
        "#                 dones = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
        "\n",
        "#                 with tf.GradientTape() as tape:\n",
        "#                     policy_logits, values = agent(states)\n",
        "#                     # policy_loss = tf.reduce_mean(\n",
        "#                     #     tf.nn.sparse_softmax_cross_entropy_with_logits(labels=actions, logits=policy_logits)\n",
        "#                     # )\n",
        "#                     labels = actions\n",
        "#                     labels = tf.reshape(labels, (-1,))\n",
        "#                     policy_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, policy_logits)\n",
        "\n",
        "#                     value_loss = tf.reduce_mean(tf.square(values - rewards))\n",
        "#                     loss = policy_loss + value_loss\n",
        "#                     loss_history.append(loss)\n",
        "#                     print(\"Loss:\", loss.numpy())\n",
        "\n",
        "#                 gradients = tape.gradient(loss, agent.trainable_variables)\n",
        "#                 agent.optimizer.apply_gradients(zip(gradients, agent.trainable_variables))\n",
        "\n",
        "#                 buffer = []  # Reset the buffer\n",
        "#             except Exception as e:\n",
        "#                 # print(e)\n",
        "#                 print(\"Error in training the agent\")\n",
        "\n",
        "# env.close_pygame()\n",
        "\n",
        "for episode in range(176):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "    buffer_episode = []  # Create a buffer for this episode\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        buffer_episode.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        rewards += reward\n",
        "        episode_path_history.append((episode, env.car_position))\n",
        "    print(f\"Episode {episode+1}, Reward: {rewards}\")\n",
        "    reward_history.append(rewards)\n",
        "\n",
        "    # Add the episode buffer to the main buffer\n",
        "    buffer.extend(buffer_episode)\n",
        "\n",
        "    print(\"Training the agent...\")\n",
        "    print(\"Buffer length:\", len(buffer))\n",
        "    if len(buffer) > 0:\n",
        "        for x in range(10):\n",
        "            batch_size = 256\n",
        "            indices = np.random.choice(len(buffer), batch_size, replace=False)\n",
        "            batch = [buffer[i] for i in indices]\n",
        "\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "            # states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "            # actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "            # rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "            # next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
        "            # dones = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
        "\n",
        "            states = tf.stack([tf.convert_to_tensor(state, dtype=tf.float32) for state in states])\n",
        "            actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "            next_states = tf.stack([tf.convert_to_tensor(state, dtype=tf.float32) for state in next_states])\n",
        "            dones = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                policy_logits, values = agent(states)\n",
        "                labels = actions\n",
        "                labels = tf.reshape(labels, (-1,))\n",
        "                policy_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, policy_logits)\n",
        "\n",
        "                value_loss = tf.reduce_mean(tf.square(values - rewards))\n",
        "                loss = policy_loss + value_loss\n",
        "                loss_history.append(loss)\n",
        "                # print(\"Loss:\", loss.numpy())\n",
        "\n",
        "            gradients = tape.gradient(loss, agent.trainable_variables)\n",
        "            agent.optimizer.apply_gradients(zip(gradients, agent.trainable_variables))\n",
        "\n",
        "            buffer = [x for i, x in enumerate(buffer) if i not in indices]  # Remove the used samples from the buffer\n",
        "            # print(\"Buffer length after training:\", len(buffer))\n",
        "\n",
        "env.close_pygame()"
      ],
      "id": "5004e593",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## test\n"
      ],
      "id": "06736b4e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# test the agent\n",
        "\n",
        "env = RCMazeEnv()\n",
        "state = env.reset()\n",
        "done = False\n",
        "rewards = 0\n",
        "path = []\n",
        "while not done:\n",
        "    env.render(framerate=60, delay=0)\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, done = env.step(action)\n",
        "    path.append(env.car_position)\n",
        "    rewards += reward\n",
        "    state = next_state\n",
        "print(\"Test reward:\", rewards)\n",
        "env.close_pygame()"
      ],
      "id": "5ba38ce3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot path\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_path(path, maze):\n",
        "    plt.imshow(maze, cmap=\"binary\")\n",
        "    x, y = zip(*path)\n",
        "    plt.plot(x, y, marker=\"o\", color=\"r\", markersize=5)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "plot_path(path, env.maze)"
      ],
      "id": "2030d403",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## results\n"
      ],
      "id": "06877642"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot the reward history\n",
        "plt.plot(reward_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title(\"Reward History for AC\")\n",
        "plt.savefig('../images/training_images/reward_history_AC.png')\n",
        "plt.show()\n",
        "# save the image\n",
        "def plot_reward_distribution(rewards, title=\"Reward Distribution\"):\n",
        "    plt.hist(rewards, bins=30, alpha=0.7, color=\"blue\")\n",
        "    plt.xlabel(\"Reward\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(\"../images/training_images/reward_distribution_AC.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming `rewards` is a list of total rewards per episode.\n",
        "# plot_reward_distribution(rewards)\n",
        "plot_reward_distribution(reward_history, title=\"Reward Distribution for AC\")\n",
        "# plot epsilon over time\n",
        "# plt.plot(epsilon_history)\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Epsilon')\n",
        "# plt.title(\"Epsilon History for AC\")\n",
        "# plt.savefig(\"../images/training_images/epsilon_history_AC.png\")\n",
        "# plt.show()\n",
        "# # save the image\n",
        "# evalution\n",
        "\n",
        "# plot mse\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title(\"Mean Squared Error over time for AC\")\n",
        "plt.savefig(\"../images/training_images/mse_history_AC.png\")\n",
        "plt.show()\n",
        "# save the image\n",
        "# save model\n",
        "# agent.save('../models/AC_RCmaze_v3.h5')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "0072e8a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot moving average\n",
        "window_size = 10\n",
        "moving_avg = np.convolve(reward_history, np.ones(window_size) / window_size, mode=\"valid\")\n",
        "\n",
        "low_point = np.argmin(moving_avg)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(reward_history, label=\"Reward per Episode\", color=\"lightgray\")\n",
        "plt.plot(\n",
        "    range(window_size - 1, len(reward_history)),\n",
        "    moving_avg,\n",
        "    label=\"Moving Average\",\n",
        "    color=\"blue\",\n",
        ")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Average Reward per Episode with Moving Average for AC\")\n",
        "plt.legend()\n",
        "plt.savefig(\"../images/training_images/reward_per_episode_with_moving_avg_AC.png\")\n",
        "plt.show()"
      ],
      "id": "c46785f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# convert to dictionary\n",
        "episode_path_dict = {}\n",
        "for episode, position in episode_path_history:\n",
        "    if episode in episode_path_dict:\n",
        "        episode_path_dict[episode].append(position)\n",
        "    else:\n",
        "        episode_path_dict[episode] = [position]\n",
        "\n",
        "maze = env.maze\n",
        "\n",
        "visit_counts = np.zeros((maze.shape[0], maze.shape[1]), dtype=int)\n",
        "\n",
        "visit_counts[env.maze == 1] = -1\n",
        "print(visit_counts)\n",
        "\n",
        "\n",
        "for episode, path in episode_path_dict.items():\n",
        "    for position in path:\n",
        "        visit_counts[position] += 1\n",
        "\n",
        "from math import nan\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap, LogNorm\n",
        "\n",
        "\n",
        "def plot_visit_heatmap(visit_counts, title=\"Visit Heatmap\"):\n",
        "    # Transpose visit_counts to match the expected orientation\n",
        "    visit_counts_transposed = visit_counts.T  # Transpose the matrix\n",
        "\n",
        "    # Filter out the wall cells by setting their count to NaN for visualization\n",
        "    filtered_counts = np.where(\n",
        "        visit_counts_transposed == -1, np.nan, visit_counts_transposed\n",
        "    )\n",
        "\n",
        "    # Define a continuous colormap (you can choose any colormap you like)\n",
        "    cmap = plt.cm.plasma\n",
        "    cmap.set_bad(\"white\")  # Use gray for NaN (walls)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    # Use LogNorm for logarithmic normalization; set vmin to a small value > 0 to handle cells with 0 visits\n",
        "    plt.imshow(\n",
        "        filtered_counts,\n",
        "        cmap=cmap,\n",
        "        norm=LogNorm(vmin=0.1, vmax=np.nanmax(filtered_counts)),\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    # add the nr of visits to the cells\n",
        "    for i in range(visit_counts_transposed.shape[0]):\n",
        "        for j in range(visit_counts_transposed.shape[1]):\n",
        "            if visit_counts_transposed[i, j] != -1 or visit_counts_transposed[i, j] != nan:\n",
        "                plt.text(j, i, visit_counts_transposed[i, j], ha=\"center\", va=\"center\")\n",
        "\n",
        "    plt.colorbar(label=\"Number of Visits\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(\"../images/training_images/visit_heatmap_AC.png\")\n",
        "    plt.show()\n",
        "    # save the image\n",
        "\n",
        "\n",
        "plot_visit_heatmap(visit_counts, title=\"Visit Heatmap for AC\")"
      ],
      "id": "19badfac",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/lucasdriessens/miniconda3/envs/tf_latest/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}