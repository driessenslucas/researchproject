{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import pygame\n",
    "\n",
    "class RCMazeEnv(gym.Env):\n",
    "    def __init__(self, maze_size_x=10, maze_size_y=10):\n",
    "        self.maze_size_x = maze_size_x\n",
    "        self.maze_size_y = maze_size_y\n",
    "        self.maze = self.generate_maze()\n",
    "        self.car_position = (0, 0)\n",
    "        self.possible_actions = range(3)\n",
    "        self.car_orientation = 'N'\n",
    "        self.sensor_readings = {'front': 0, 'left': 0, 'right': 0}\n",
    "        self.steps = 0\n",
    "        self.previous_distance = 0\n",
    "        self.reset()\n",
    "\n",
    "    def generate_maze(self):\n",
    "        # For simplicity, create a static maze with walls\n",
    "        # '1' represents a wall, and '0' represents an open path\n",
    "        maze = np.zeros((self.maze_size_y, self.maze_size_x), dtype=int)\n",
    "        # Add walls to the maze (this can be customized)\n",
    "        maze[1::2, :] = 1\n",
    "        maze[:, 1::2] = 0\n",
    "        # Add goal\n",
    "        maze[-1, -1] = 0\n",
    "        \n",
    "        return maze\n",
    "\n",
    "    def reset(self):\n",
    "        self.car_position = (0, 0)\n",
    "        self.car_orientation = 'N'\n",
    "        self.update_sensor_readings()\n",
    "        self.steps = 0\n",
    "        self.previous_distance = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.move_forward()\n",
    "        elif action == 1:\n",
    "            self.turn_left()\n",
    "        elif action == 2:\n",
    "            self.turn_right()\n",
    "        self.update_sensor_readings()\n",
    "        reward = self.compute_reward()\n",
    "        self.steps += 1\n",
    "        done = self.is_done()\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    # def move_forward(self):\n",
    "    #     x, y = self.car_position\n",
    "    #     if self.car_orientation == 'N':\n",
    "    #         self.car_position = (x, max(y - 1, 0))\n",
    "    #     elif self.car_orientation == 'S':\n",
    "    #         self.car_position = (x, min(y + 1, self.maze_size_y - 1))\n",
    "    #     elif self.car_orientation == 'E':\n",
    "    #         self.car_position = (min(x + 1, self.maze_size_x - 1), y)\n",
    "    #     elif self.car_orientation == 'W':\n",
    "    #         self.car_position = (max(x - 1, 0), y)\n",
    "    \n",
    "    def move_forward(self):\n",
    "        x, y = self.car_position\n",
    "        if self.car_orientation == 'N' and y > 0 and self.maze[y - 1][x] != 1:\n",
    "            self.car_position = (x, y - 1)\n",
    "        elif self.car_orientation == 'S' and y < self.maze_size_y - 1 and self.maze[y + 1][x] != 1:\n",
    "            self.car_position = (x, y + 1)\n",
    "        elif self.car_orientation == 'E' and x < self.maze_size_x - 1 and self.maze[y][x + 1] != 1:\n",
    "            self.car_position = (x + 1, y)\n",
    "        elif self.car_orientation == 'W' and x > 0 and self.maze[y][x - 1] != 1:\n",
    "            self.car_position = (x - 1, y)\n",
    "        \n",
    "\n",
    "    def turn_left(self):\n",
    "        orientations = ['N', 'W', 'S', 'E']\n",
    "        idx = orientations.index(self.car_orientation)\n",
    "        self.car_orientation = orientations[(idx + 1) % 4]\n",
    "\n",
    "    def turn_right(self):\n",
    "        orientations = ['N', 'E', 'S', 'W']\n",
    "        idx = orientations.index(self.car_orientation)\n",
    "        self.car_orientation = orientations[(idx + 1) % 4]\n",
    "\n",
    "    def update_sensor_readings(self):\n",
    "        # Simple sensor implementation: counts steps to the nearest wall\n",
    "        self.sensor_readings['front'] = self.distance_to_wall('front')\n",
    "        self.sensor_readings['left'] = self.distance_to_wall('left')\n",
    "        self.sensor_readings['right'] = self.distance_to_wall('right')\n",
    "\n",
    "    def distance_to_wall(self, direction):\n",
    "        x, y = self.car_position\n",
    "        distance = 0\n",
    "        if direction == 'front':\n",
    "            if self.car_orientation == 'N':\n",
    "                while y - distance >= 0 and self.maze[y - distance][x] != 1:\n",
    "                    distance += 1\n",
    "            # Similar logic for other orientations...\n",
    "        # Implement for left and right...\n",
    "        return distance\n",
    "\n",
    "    def compute_reward(self):\n",
    "        reward = 0\n",
    "\n",
    "        # Penalty for hitting walls or going out of bounds\n",
    "        if self.sensor_readings['front'] == 0 or self.sensor_readings['left'] == 0 or self.sensor_readings['right'] == 0:\n",
    "            reward -= 50\n",
    "\n",
    "        # Reward for reaching the goal\n",
    "        if self.car_position == (self.maze_size_x - 1, self.maze_size_y - 1):\n",
    "            reward += 100\n",
    "            return reward  # Return immediately as this is the terminal state\n",
    "\n",
    "        # Calculate reward based on reduced distance to goal\n",
    "        x, y = self.car_position\n",
    "        goal_x, goal_y = (self.maze_size_x - 1, self.maze_size_y - 1)\n",
    "        distance = abs(x - goal_x) + abs(y - goal_y)\n",
    "        \n",
    "\n",
    "        # Assuming previous_distance is stored after each move\n",
    "        if distance < self.previous_distance:\n",
    "            reward += 10  # Positive reward for moving closer to the goal\n",
    "        elif distance > self.previous_distance:\n",
    "            reward -= 5   # Negative reward for moving farther from the goal\n",
    "\n",
    "        # Update previous_distance for the next step\n",
    "        self.previous_distance = distance\n",
    "\n",
    "        return reward\n",
    "\n",
    "        \n",
    "\n",
    "    def is_done(self):\n",
    "        # Define when the episode ends\n",
    "        # ends when the car reaches the goal or it takes more than 100 steps\n",
    "        return self.car_position == (self.maze_size_x - 1, self.maze_size_y - 1) or self.steps > 500\n",
    "        \n",
    "        \n",
    "    def get_state(self):\n",
    "        return (self.car_position, self.car_orientation, self.sensor_readings)\n",
    "\n",
    "    # def render(self):\n",
    "    #     rendered_maze = np.array(self.maze, dtype=str)\n",
    "    #     x, y = self.car_position\n",
    "    #     rendered_maze[y][x] = 'C'  # Representing the car\n",
    "        \n",
    "    #     #print array\n",
    "    #     print(rendered_maze, '\\n') \n",
    "\n",
    "    \n",
    "    def init_pygame(self):\n",
    "        # Initialize Pygame and set up the display\n",
    "        pygame.init()\n",
    "        self.cell_size = 40  # Size of each cell in pixels\n",
    "        self.width = self.maze_size_x * self.cell_size\n",
    "        self.height = self.maze_size_y * self.cell_size\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def render(self):\n",
    "        # Render the environment using Pygame\n",
    "        for y in range(self.maze_size_y):\n",
    "            for x in range(self.maze_size_x):\n",
    "                rect = pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "                if (x, y) == (self.maze_size_x - 1, self.maze_size_y - 1):  # Goal position\n",
    "                    color = (0, 255, 0)  # Green color for the goal\n",
    "                elif self.maze[y][x] == 0:\n",
    "                    color = (255, 255, 255)  # White color for empty space\n",
    "                else:\n",
    "                    color = (0, 0, 0)  # Black color for walls\n",
    "                pygame.draw.rect(self.screen, color, rect)\n",
    "\n",
    "        # Draw the car\n",
    "        car_x, car_y = self.car_position\n",
    "        car_rect = pygame.Rect(car_x * self.cell_size, car_y * self.cell_size, self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), car_rect)  # Red color for the car\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)  # Limit the frame rate to 60 FPS\n",
    "\n",
    "\n",
    "    def close_pygame(self):\n",
    "        # Close the Pygame window\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1, possible_actions=3, min_epsilon=0.01, epsilon_decay=0.99):\n",
    "        self.q_table = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.possible_actions = possible_actions\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "\n",
    "    def state_to_tuple(self, state):\n",
    "        \n",
    "        #((0, 0), 'N', {'front': 1, 'left': 0, 'right': 0})\n",
    "        # if like this convert to ((0, 0), 'N', (1, 0, 0))\n",
    "        if not isinstance(state[2], dict):\n",
    "            # print(state)\n",
    "            # print(state[2])\n",
    "            #take state[2] and make it from this (1, 0, 0) to this {'front': 1, 'left': 0, 'right': 0}\n",
    "            newState = {'front': state[2][0], 'left': state[2][1], 'right': state[2][2]}\n",
    "            # print(newState)\n",
    "            #create a new state with the [2] being the new dictionary\n",
    "            state = (state[0], state[1], newState)\n",
    "            \n",
    "        # Convert the state dictionary to a hashable tuple\n",
    "        # Adjust this based on the specific format of your state\n",
    "        position, orientation, sensor_readings = state\n",
    "        sensor_readings_tuple = tuple(sensor_readings.values())\n",
    "        return (position, orientation, sensor_readings_tuple)\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        state_tuple = self.state_to_tuple(state)\n",
    "        return self.q_table.get((state_tuple, action), 0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(range(self.possible_actions))\n",
    "        else:\n",
    "            state_tuple = self.state_to_tuple(state)\n",
    "            q_values = [self.get_q_value(state_tuple, action) for action in range(self.possible_actions)]\n",
    "            max_q = max(q_values)\n",
    "            actions_with_max_q = [action for action, q in enumerate(q_values) if q == max_q]\n",
    "            return random.choice(actions_with_max_q)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        state_tuple = self.state_to_tuple(state)\n",
    "        next_state_tuple = self.state_to_tuple(next_state)\n",
    "        max_q_next = max([self.get_q_value(next_state_tuple, next_action) for next_action in range(self.possible_actions)])\n",
    "        current_q = self.get_q_value(state_tuple, action)\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_q_next - current_q)\n",
    "        self.q_table[(state_tuple, action)] = new_q\n",
    "\n",
    "    def train(self, environment, num_episodes):\n",
    "        reward_history = []\n",
    "        for _ in range(num_episodes):\n",
    "            state = environment.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                environment.render()\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = environment.step(action)\n",
    "                self.update_q_value(state, action, reward, next_state)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            # Add the total reward for this episode to the history\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "            # Decay epsilon, but not below the minimum value\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            # Print episode summary\n",
    "            print(\"Episode finished after {} timesteps\".format(environment.steps))\n",
    "            print(\"Total reward: {}, Epsilon: {:.3f}\".format(total_reward, self.epsilon))\n",
    "\n",
    "        return reward_history\n",
    "            \n",
    "    def test(self, env):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "\n",
    "        print(f\"Test Total Reward: {total_reward}\")\n",
    "\n",
    "# Example usage:\n",
    "# env = RCMazeEnv()\n",
    "# agent = QAgent()\n",
    "# agent.train(env, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 501 timesteps\n",
      "Total reward: -24770, Epsilon: 0.100\n",
      "Episode finished after 501 timesteps\n",
      "Total reward: -25040, Epsilon: 0.100\n",
      "Episode finished after 501 timesteps\n",
      "Total reward: -24990, Epsilon: 0.100\n",
      "Episode finished after 501 timesteps\n",
      "Total reward: -25025, Epsilon: 0.100\n",
      "Episode finished after 501 timesteps\n",
      "Total reward: -25025, Epsilon: 0.100\n",
      "Episode finished after 501 timesteps\n",
      "Total reward: -25010, Epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "EPSILON = 0.9\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.6\n",
    "DECAY = 0.999\n",
    "MINEPSILON = 0.1\n",
    "DECAY_RATE = 0.001\n",
    "\n",
    "\n",
    "env = RCMazeEnv()\n",
    "agent = QAgent(alpha=ALPHA, gamma=GAMMA, epsilon=EPSILON, min_epsilon=MINEPSILON, epsilon_decay=DECAY_RATE)\n",
    "env.init_pygame()\n",
    "agent.train(env, 150)\n",
    "env.close_pygame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasdriessens/miniforge3/envs/tf/lib/python3.8/site-packages/pygame/pkgdata.py:27: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/lucasdriessens/miniforge3/envs/tf/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/Users/lucasdriessens/miniforge3/envs/tf/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.logging')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/Users/lucasdriessens/miniforge3/envs/tf/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Total Reward: -25020.18041549959\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "env = RCMazeEnv()\n",
    "\n",
    "env.init_pygame()\n",
    "\n",
    "# Example of running the environment\n",
    "agent.test(env)\n",
    "\n",
    "\n",
    "env.close_pygame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
